2025年9月27日 下午 10:46|1小时 14分钟 15秒

关键词:
times memory、memory model、memory size、fast memory、much memory、memory bandwidth、memory higher、memory reads、faster memory、global memory、memory access、memory movement、slow memory、few memory、memory reason、memory bottleneck、SM memory、vector memory

文字记录:
说话人 1 00:00
So hopefully everyone's having a good time with Assignment 1. It's due tonight. Let us know if you need an extension. Assignment due to is coming out soon. We're putting on the finishing touches onto some of the Triton stuff. Hopefully, you'll enjoy it. You'll get to implement Flash Attention 2 or parts of flash attention 2, which I think will be nice.

说话人 1 00:21
So today we're gonna talk about GPUs are the thing that makes our language models go. So they're pretty critical to get right. And if you haven't really studied, you know, the hardware that makes, you know, your models run, they can seem pretty mysterious. So my goal today is to try to make CUDA and GPUs less magic. And one of the things that I wanna demestify, you don't have to understand the plot.

说话人 1 00:49
There's a lot on the slide. I know you know, why do GPUs get slow? And they get slow in very mysterious ways. You know, I will try to talk through this plot near towards the end of lecture. As you increase the size of your matrix multiplies, you might expect, you know, either gets slower or faster or whatever. You got these very unpredictable looking wave like patterns. And you're like, why is my GPU fast at certain multiples of certain numbers and slow at others, right? It's very mysterious. We'll try to understand that. The other thing is we would like to understand how to make fast algorithms. I think almost all of you have heard of flash attention. It's the thing that makes, you know, much longer context possible by very cleverly computing the attention operation inside a transformer. And so maybe you would like to, you know, come up with new algorithms or new implementations like flash attention, right? Like what primitives and what components do we need to understand in order to be able to do that, right? So those are kind of the two learning goals of today. The first one is, you know, by the end of the lecture, you should feel kind of comfortable with GPUs. You should kind of understand how they work. And the second one is you should feel comfortable accelerating certain parts of your algorithms. You make a new architecture, you should hopefully feel like you can try to accelerate that with CUDA.

说话人 1 02:07
And because hardware is not necessarily the domain in which I work, you know, there's special resources that I have to give a lot of credit to, especially Horace, his blog, where he's got a lot of fun GPU facts that you can learn about. For example, why our matrix multiplies that are filled with zeros faster than ones that are not filled with zeros? You can learn by going to his blog. There's also other resources that I've drawn from like the Kuda Mode group and the nice TPU book from Google. If this topic interests you, you know, I'd encourage you to go and look at those resources to learn more because this is in some ways like a shallow, but hopefully, you know, complete coverage of the hardware.

说话人 1 02:42
So today we're only going to focus on, you know, non parallel parts of the hardware stack. So we're going to study the GPU like a single accelerator in depth, how they work in some important parts. I'm also gonna talk very briefly about TPUs, because in some ways they're very similar conceptually to a GPU. And so my discussion here is gonna carry over.

说话人 1 03:02
And then once we understand kind of the hardware and execution model of the GPU, then we're going to try to understand what makes GPUs go fast on certain workloads, what makes them slow. We're going to understand the performance. And in the last part, this is kind of going to be almost like a hands on piece. I'm going to try to walk through flash attention, right? I'm gonna take all of the lessons that we've Learned and try to walk you through flash attention saying, see, here's how it all comes together, right? So that's the last part of today's. So, you know, many of you have, you know, taken an NLP course. And these days in an NLP course, I think you teach some amount of scaling laws. And so you've probably seen this, right? And so this is just setting the context.

说话人 1 03:44
We know that having more compute is helpful for training large language models. This is a pre training scaling chart, but you could replace this with an inference scaling chart if you would like. It's generally agreed upon that the more compute you have, the more processing you can do on your data. You can ingest more data, you can train larger models. All of those lead to improve performance, right? So you might think, of course, you know, deep learning is really important, but what's really driven performance is, you know, faster hardware, better utilization, improve paralyzation, right? So that's kind of setting the stage of why hardware is important to understand. And of course, you know, once you think about compute scaling, you ask, okay, how do we get compute scaling? How do we get our models to train faster? So kind of in the early days, you know, of semiconductor scaling, if you were thinking about, okay, our CPUs, how do they get faster? They went, they, you know, would scale under something called dennered scaling, right? With Moore's Law, you would sort of double the amount of transistors on a chip every year. And if you have this doubling, what you end up is dennered scaling, where smaller and smaller transistors can be driven at faster and faster clock speeds with lower and lower power, which in turn give you more performance, right? And then in the 1980s to 2, this sort of tapped out. You can kind of see in this chart here by. Tennessee and Patterson, that single thread performance, that's the blue dots here, that basically started to paper out.

说话人 1 05:05
Of course, a number of transistors didn't really, you know, start falling off. You did have, you know, chips with higher and higher transistor densities, but that wasn't helpful. It wasn't giving you higher are throughput on single threads. And so this means that we can just do computation faster.

说话人 1 05:20
In absolute terms, you know what we have to make up for it with is parallel scaling, right? So the story of scaling for deep learning and neural networks is going from single thread scaling, which is us doing your computation faster in absolute terms, to parallel scaling where you have a lot of workloads that are all computed at once. And this is one of my favorite, you know, sort of compute scaling charts by Bill Dowley and his keynote where, you know, he's showing the super exponential increase in the number of sort of integer operations per second going from, you know, the earliest K 20s to the H100, right? And it's kind of like this really remarkable exponential or super exponential curve. And so, you know, we have to really understand how to take advantage of this curve in order to really get the most out of our language model, right? So that's kind of going to be our goal.

说话人 1 06:17
And so I've already hinted at this kind of important difference, right? CPU is something that I think everyone's familiar with once you start doing programming, right? This execution model, if you have a program, it goes through and in a single thread, it execute step by step what's happening. And in order to support that kind of an execution model, what do you need? Well, you need big control units. You just, you need to generally run these things very quickly because you have a lot of branching and you have a lot of conditional control logic, right? So the CPU, this is an abstracted diagram, is going to dedicate, you know, a lot of its chip towards, you know, large control branch prediction. And it's going to run these, you know, very quickly because it doesn't have that many threads. Know there's CPUs with lots and lots of cores now, but compared to a GPU, it's almost nothing. And so in contrast, the GPU has really tons and tons of compute units, als, right? So there's the little green boxes and there's much smaller amounts of the chip dedicated to control. So there's a little bit of control logic sort of orchestrating tons and tons of compute units, you know, operating in parallel.

说话人 1 07:22
And I think mentally, so this is kind of the picture of what is being emphasized in a CPU versus GPU. But if you kind of look at what the design goals are, they design for very different sort of goals. So you can think about CPUs as optimizing for latency. I want to finish my tasks as quickly as possible. So if I have tasks T1 through T4 here on the right side, you know, in a CPU, I'm gonna try to finish each task as quickly as possible. And so if you want any one of these tasks to be finished quickly, t one's going to complete really quickly. In GPU, you're optimizing for high throughput. Like I don't care about latency. I just want all of my tasks that I have in aggregate to complete as quickly as possible. And to support that, you know, maybe you have lots of threads and these threads can go to sleep and wake up very quickly. And in the end, you know, you finish all of your workload, T1 through T4, you know, before the CPU one does, even though individually all of these have sort of higher latency, right? So they have different sort of design principles and design goals.

说话人 1 08:23
Okay. And so a GPU has a pretty different anatomy. And I don't know if you know, you all have ever looked at what a GPU sort of layout diagram looks like. I'll actually show you the chip figures in a moment here.

说话人 1 08:37
But the core idea, and this is important conceptual concepts behind a GPU, is that a GPU executes, you know, many SMS streaming multi processors. In a streaming multi processor, you can kind of think of as an atomic unit when you're programming and something like Triton, then they're gonna operate at the level of SM. And within each SM, they're going to, it contains many SPS, streaming processors. And a streaming processor is going to execute a whole bunch of threads in parallel. So one way to think about it is SM has a bunch of control logic. It can decide what to execute. It can do, for example, branching. SPs are going to operate to take the same instruction and apply it to many different pieces of data, right? And so you can do tons and tons of parallel computation under this model. And SM is sort of each granular unit of control. SP can do a lot of computation individually. And if you look at an A100, which is the previous generation GPU, at this point, you've got 128 SMS. You know, that's a lot more than the most cores for CPUs. And each of these SMS is going to have a very large number of SPS and specialized sort of matrix multiply units inside them. And so that's kind of the compute model. Was there a question? Sorry, but we're gonna to. Yeah, that's like important ug. So if this should be the same asset, so the question was, is this GP the same as that GP?

说话人 1 10:08
Yes, like this is a kind of cartoon version of this. You could kind of think of each row as being SM. It's got its own control units. Each green block might be sort of one of these green blocks here, like F P30 two sort of processing unit inside of it. And each SM can sort of, you know, operate various pieces that owns like the tensor course to do computation.

说话人 1 10:34
Okay, and there's gonna be two important things you think of GPUs, you know, computers, they compute. But actually computation is only one of the two important things we have to keep track of, right? Memory is arguably more important at this point, and it will continue to be more important in terms of the performance profiles of how we run our programs on the GPU. And so to understand memory, you kind of have to understand the physical layout of the GPU and the chip because in some sense, the, you know, when you're operating at such fast speeds, the physical proximity of the memory starts to matter quite a bit. And so I will show you kind of the physical proximity of how things are laid out and how that relates to how you should think about memory access and performance. So the closer a piece of memory is to each SM, the faster it's going to be. So there's going to be certain very fast kinds of memory like L1 and shared memory, and that's gonna live inside of VSM, right? And that's gonna be really fast, right? Things like registers, things like things you're reading and writing very frequently, you're gonna wanna put into the L1 and shared memory L2 cache. As you can kind of see, there's these green areas, which are SMS, and then there's these blue areas. This is on the GPU chip, right? These are L2UH memory that sort of right next to the SMS, right? So they're not inside the SMS, but they're physically still quite close, right? And these are still pretty fast. You know, they're still a factor of 10 slower, but they're still reasonably fast. And then outside of the chip itself, this is sort of a, you know, I think this is like a 30,90 card or something like this or maybe a Pcie 100. Oh, this is a PCIA 100. You know, you've got your GPU here and you've got actually DRAM sort of living next to the chip, right? So it has to actually go physically outside of the chip and connect and you can kind of see on this chip diagram here, these yellow connectors at the edges, these are HBM connectors. These are connecting to the DRAM chips that are outside of the actual GPU, and you can kind of see the speed that it takes to access these, right? The on SM memory is much faster, like 20 clock cycles to access something from there, whereas it's gonna take something like 200 to 300 clock cycles to access something from the L2 cache or global memory, right? And this factor of 10 is gonna hurt you real bad, right? So if you have a piece of computation that requires you to access global memory, right, it might mean that you actually run out of work to do on your SM. You've multiplied all the matrices, you've run out, now you just have to idle, right? So utilization won't be good. And this will be a really key theme. Thinking about memories, in some sense, the key to thinking about how GPUs work. And in Assignment 2, you're gonna, you know, actually be writing high performance code for a GPU. So you have to actually think about the execution model of how a GPU actually executes things. And this is somewhat complicated, but not insanely.

说话人 1 13:27
So there's sort of three granularities of things that you need to think about. There's blocks, there's warps, and there's threads. And that's the order in which kind of the granularity narrows down, right? Blocks are kind of these big groups of threads. And each block is going to be assigned to a SM. So think about this as each SM is kind of a worker. It's its own autonomous unit. And a block is going to be assigned to an SM to process, right? So this is each granular unit.

说话人 1 13:56
Now, then within these blocks are a whole bunch of threads. Each thread is a sort of a piece of task that needs to be done. And when these threats execute, they're going to execute in groups. And this is a thing called a warp, right? So you take a block, which is a collection of threads, and you're gonna take, you know, threads from that block and they're gonna execute in groups of 32 consecutively numbered threads each time. And that's sort of called, you know, warps.

说话人 1 14:20
And so you can kind of see at this diagram here what's happening. You've got a bunch of blocks. Each block is assigned to a different SM. And within each block, there's going to be many different warps. And each warp is going to consist of a whole bunch of threads. And all of these threads are going to execute the same instruction on different data, right? And so this is kind of the execution model right now.

说话人 1 14:40
It's going to, it seems probably mysterious, you know, what these blocks and warps and threads are. They will have important implications for our performance in how we design things like Kuda kernels later. So hopefully you can kind of remember this. I'll refresh your memory kind of as we go. Hopefully, that's clear. So that was. What's the kind of logical execution model of a GPU? And if you understand that, you kind of understand how GPUs execute things. There's also a logical sort of memory model of a GPU. So, you know, now I'm not showing you the physical hardware. This is just kind of how you think about the programming of a GPU. And so there's registers. So these are really fast, you know, storing single numbers, type storage. You've got local memory, you've got shared memory, and you've got global memory, right? And that increases in sort of the memory higher if you get slower and slower. And your code can sort of write to global memory, you can also write a constant memory, which is not something that's used too often. And so each thread can access, you know, its own register and shared memory, but information that goes across blocks need to be written to global memory. This is actually quite coordinate, right? So now it means that, you know, whenever you write a thread that executes something, ideally it's operating on sort of the same small amount of data. So you load that small amount of data into shared memory, all the threads are very happy accessing that shared memory. It terminates, it's done, right? That would be a great execution model. Instead, if you have a thread that needs to access data all over the place, you know, that's gonna have to access global memory, that's very slow. This thing will come back. You know, as we talk about different ways of operating on a GPU, hopefully that's clear. That's kind of the very, you know, high level 4 slide overview of a GPU. If you have kind of questions about how any of that works, feel free to ask me as I go on. Okay, so here's a side thread. Last year, I didn't cover this because I think resources on TPUs was a little thin. But the nice TPU book or internet website that I mentioned at the start of the lecture came out. And that has actually a lot of nice details. And I talk to a few Google people about the TPU and at a high level, it's very similar to a GPU. And so I wanna just talk for a moment about TPUs. You may never, you know, operate on a TPU, but I think it's important to understand that these alternative accelerators operate in many ways very similarly. So here's a diagram of what a TPU looks like. There's kind of a, so there's something called a tensor core. And mentally, you can think about a tensor core as being similar to SM or streaming multi processor. Each of these are kind of its own atomic units that can operate on data. There's a scalar unit, which is basically a control unit, and it can also do CPU, like arbitrary things. You've got a vector unit that can operate on vector. So if you've got a vector and you wanna operate entrywise on it, that's a good place to do it. And then it's got a very big specialized, you know, part of the chip dedicated to just doing matrix multiplies called the MXU. And then it's got very fast memory for vector memory and SMM. Both of these are very fast on chip or like on tensor core memory. And then there's high bandwidth memory that lives outside of the chip, right? So hopefully you see the similarities to an SM, right? There's slow memory outside, very fast memory inside, and there's specialized hardware to do matrix multiplication. Core structure is very much the same. The difference is, I'll talk about this in the parallelism lecture next week. You know, how the accelerators are networked together is a little bit different. And then also, you know, mention, I didn't notice, I didn't talk about warps, I didn't talk about any of that other stuff. Tensor cores are in some ways very simple because they're optimized to just do matrix multiplies, right? Like the Tensor Core, unlike the GPU, doesn't attempt to do anything but that. And so that's in some ways very simple, much simpler in architecture, but conceptually doing the same thing. Yes. Is it call tensor also in some ways optimized cater tensor. So this is just, yeah, so the question was, you know, is it called tensor because it can operate on arbitrary tensors? So it can operate on arbitrary tensors. I can do the indexing. The operations that MXC performs is a matrix multiply. And so it would always be like a batch matrix multiply operating on a tensor. So it's kind of both a yes and a no answer if that makes sense. So they operate on tensors, but the operations they always perform are matrix multiplies, not more complicated tensor operations that you can do. Cool. The reason why the GPU has been so successful is that, you know, it scales up really easily if you want more processing power, just add more SMS, right? You don't have to worry about driving the clock faster and getting more heat dissipation problems. Programming wise, Kuda is intimidating, but it's actually, you know, not as horrendous to program because of the, its programming model, like the way it works is within each SM, right? You have a thread and it executes the same instruction on a bunch of different pieces of data, right? That's conceptually sort of easy to reason about. You can think through what that means. And especially it's nice if you're operating over a matrix and you're doing sort of very simple operations is exactly this kind of SIM key. Now finally, each of these threads are very lightweight and they can be kind of stopped and started at any time. And so if you need to wait for another thread or if you need to sort of like evict something and like start another process. All these threads are very lightweight. So this just kind of means that there's not much state associated with the threads and they can kind of be stopped and started, which allows GPUs to get high utilization within sort of each SM. So GPUs, you know, obviously graphics processing units. And for much of its life, you know, in the early days, it was not used to do scientific computing. But, you know, people, because it was programmable, researchers figured out how to use, you know, early Nvidia GPUs to do fast matrix multiplies. This is one of the early papers on, you know, doing fast matrix multiplies with graphics hardware. And it shows, you know, how you can, you know, hack kind of things like the texture buffer and so on to get it to do matrix multiplies, right? And so, you know, even without specific support for map moles, you know, researchers figured out how to do it. But I think now, you know, especially in this day and age, Nvidia and others have realized matrix multiplies are special. Like if you're doing deep learning, right, most of your workload is matrix multiplies. And so matrix multiplies are in some sense blessed operation. So this is a chart showing the number of teraflops per second by different generations of Nvidia GPUs. And the orange line is your map mole flops, right? Like with your performance, you can get if you're doing map moles. The blue line is your non map mole flops, right? And you see kind of this big gap at V1 hundreds when they started putting in sort of tensor cores that were specialized hardware to do matrix multiplies, and you see this gigantic gap in the matrix multiply performance relative to the non map more performance, right? And so if you're gonna design any sort of a neural architecture, I was saying this, you know, in the architecture part as well, you have to have most of your workload B matrix multiplies because that's the thing that's, you know, orders a magnitude faster than any other operation that you're gonna be able to do on a GPU, right? So if you make like a non net mole based neural network, you're gonna be in a big trouble. And then kind of the last thing that I want you to kind of understand as just general facts, you know, Matthews is fast as one thing. But the other thing that's important to remember is kind of the relative scaling of the different components of the GPU. So this is a very nice chart that shows, you know, how quickly different components of the GPU or different components of the, let's call it like LM training stack are scaling. So the blue line is the connectivity from the GPU to the host, right? Like the server that it's attached to, right? So you can use Pcie, you can use NV Link, you can use all these fancy interconnects. They are growing, but they're going somewhat slowly, right? So this chart is like normalized scaling, you know, bandwidth relative to when, you know, the first generation of interconnects. The green line, this is the global memory speed, right? So you go from GDDR to hbmqe, and that's much faster, right? This is log scale to 100 x faster, but this is still kind of slow scaling, right? And the gray line here, right, this is compute scaling. This is the number of floating point operations. If you're, you know, considering the map more flops, this is how fast the compute has been scaling. And this is astounding. We fast like 1 to 100,000 times faster. And so kind of in the early days of the scaling, maybe your problems were flops based, right? Like you just didn't have enough flop to do your matrix multiplications. But now you know all the way to the right with the H1 hundreds, you know, these are astoundingly fast GPUs. Your bottlenecks are probably end going to end up being memory, right? Cuz memory is not growing as fast, right? And as we go into the future, you know, this is not really gonna change. DRAM is very hard to scale. You're gonna keep getting this bigger and bigger gap, right? So if you're ever designing, you know, hardware efficient algorithms, you're gonna have to think more and more about memory, right? And so we're gonna keep a lookout on that. I'm gonna keep emphasizing this. It's one of the important themes in GPS. So, you know, I've been kind of throwing lots of GPU facts at you, especially if you haven't, you know, seen this recently and maybe me kind of new. So just to recap, right, GPUs are these massively parallel processing systems. They have same instructions applied across many different threads. And they have these things called SMS, which are kind of like cores that, you know, there's many of them in the GPUs. Compute and matrix multiplies have scaled really fast and they have scaled faster than memory.

说话人 1 24:21
And that is an important part of the characteristics that you think about GPUs. But there is some fast memory, right? It's not like everything is slow, so there's nothing we can do. There is the memory hierarchy, right? So some kinds of memory are very fast, other kinds of memories are slow. And so if we exploit this hierarchy, maybe we can get things that are really pass, right? So that's kind of things to remember about the GPU. And if you remember these facts, you know, you're gonna be able to think pretty cleanly about the performance components that I'm going to talk about next. Any questions before I move on to the next part? Cool. So now you all are GPU. Experts. And what we would like to do is we would like to make machine learning workloads go very fast on AGPU. And so I'm gonna start with this chart. And one of our goals will be to understand what this chart exactly is. I think it'll be a good puzzle to get us motivated. And so here what we are doing is we are multiplying square matrices together, right? So the x axis is the size of my square matrix multiply. And you know the y axis here, this is the number of operations per second that I'm doing. So you can kind of think of this as hardware utilization on the y, right? And so as I get bigger and bigger matrices, I'm gonna get better and better hardware utilization because, you know, I have more work to do. So I don't, you know, that overwhelms the overhead of sort of, you know, launching jobs and things like this. But there's all these weird things that are happening, right? You see one, two, three different, four different lines, right? And each of these lines are kind of wavy in a way that's kind of, you know, looks very unpredictable, right? And so we would like to kind of understand what exactly is going on with these lines. And by the end of this section, my promise is that you will kind of understand exactly each one of these phenomenon. You'll be able to say, yeah, that plot looks totally normal. That is a natural thing for GPU to do. Okay, so the very first part, right, is if you look at that plot, you will notice that it looks a little bit like this, right? And if you've taken a system's hardware course, you know, you should remember this as kind of the roofline model basically says if we're looking at, you know, throughput or utilization, you know, what we're gonna find is, you know, there's two regimes. There's gonna be a regime that is sort of memory limited, right? That is on the left side of this curve, on the green over here. And then there's a part that is throughput limited on the right side. In some sense, you can kind of think of it as on the right side, we have, we are fully utilizing our compute units. All the matrix multiply units are multiplying all the time. And on the diagonal here, we just have some sort of memory bottleneck. And so our ability to do computation is limited by kind of the amount of sort of intensity that we have, the amount of flops for byte that we have, right? So we want to avoid being in this left side region where we're memory bound, and we would like to be on this right side where we're getting, in some sense, full utilization of all of our compute units, right? So that's in some sense the goal. And hopefully this refine model looks something like this, right? Like we've got sort of this diagonal part, and then we've got this flat part all the way at the top here. So that's one part of the mystery. And so this is, this turns out to be kind of complex, right? The simple way to say this is let's make sure that we're not accessing memory unnecessarily, right? We have as few memory accesses to slow global memory as possible. But it turns out that in order to do that, we need a large array of tricks. There's a lot of different things that you could do that would mess you up, that would make you very slow. And the first one's not a memory bottleneck. I'll just mention it doesn't come up too often. We'll get it out of the way, and then we'll talk about the remaining five items that in some sense are really core to thinking about GPU performance. Okay, so the first thing that I want to talk about is conditionals. So as I said before, GPUs, their execution model is something called Simkey, right? Single instruction, multi thread. And so every thread in a warp is going to execute the same instruction and it's going to do so on different data. And so what happens if I write a piece of code that looks like this? I have an if statement. And if, you know, the thread index is less than 4, do something. If the thread index is greater than or equal to 4, then do something else, right? I have this very simple conditional model. If I run this on the GPU, what's going to happen is that I am going to run the a instruction on four of my threads. I will actually pause my other 4 threads, which are supposed to be executing the L spark. And then these other fourth threads will come alive and they will execute X and my original force rezzo will go to sleep, and I will just alternate executing each of these instructions. Why is that? I can't execute a and X at the same time on these different threads, right? As I said, again, every thread has to execute the same instruction. So conditional statements within a single warp can be really damaging because they will force you to pause any of the threads that are not doing exactly the main sort of control flow execution. Okay, so that was the only non memory thing that I wanted to mention and it should be kind of obvious that you should probably not be putting conditionals into sort of your massively parallel compute unit. But once we've gotten that out of the way, sort of the other tricks that we need to consider are all kind of memory based. The first thing I want to sort of mention is lower precision. And this is a big trick. This is an important trick. You should do it all the time. There's kind of going back to the spot of Bill Daly, there's a slight. Of hand here, this looks really good because the numbers are going up and up. But if you look at, you know, what's driving GPU progress over all these years, you actually kind of see that it's number representations. You go from FP32 to FP16 to INT eight to and so on. You get many orders of magnitude gains from just having lower and lower precision in your GPU operations. And let me sort of clarify why that's so important, right? If you have fewer bits in all of the things that you're computing and your weights and so on, you have much fewer bits to move, right? So even if you're accessing these bits from global memory, they become much less of a concern. So let's just give a simple example and let's just think about kind of arithmetic intensity of a simple element wise operation, right? So I'm going to do any value. So that's x equals max 0 and X. And I'm going to do that on a vector of size n. Let's say naively, I'm gonna do this on float 32, right? So how many memory access to this do I have to read my X? I have to write the result of FX less than zero. And that's all in float 32. So that's kind of 8 bytes, right? And how many operations do I do? Well, I have to do X less than zero. So that's one comparison operation. And I do one flop, right? So I do, you know, eight bytes per single floating point operation. If I do this in float 16 now, well, you know, I haven't changed the flops intensity here, but I have the memory access. And so now I have 4 bytes per flop, right? In some sense, I've gotten double the memory bandwidth for free, assuming that I can get away with FOP16. And this is a key part of how a lot of things are designed. Part of the assignment is going to be you're going to, you know, try and play with various like mixed precision or low precision training and other kinds of things. And a key part here is that not all the parts of your network and your training algorithm should be put into low precision, right? So let me give you an example of matrix multiplies. So in matrix multiplies that are mixed precision, what you would do is you would have, you know, your inputs be 16 bit. So these are low precision and then you're gonna do your multiplication in full 32 bit, right? And that's useful because the intermediate computations as you're like accumulating partial sums, you would like that to be in high precision. And so you're accumulating this with FP32 accumulator and then, you know, your Tensor Core, we'll return FP32 result, which you can, you know, downcast if you would like back into 16 bit, right? And so we have our inputs in 16 bit, but things like the accumulation we might want to do in 32, right? So there's lots of different things. There's operations that can use 16 bit storage. There's operations that might need more precision. So you want to keep it in like either FP32 or FP16. You might want to have operations that need more range like X functions if you don't have sort of the dynamic range, that might blow up or zero out. And so you might want to put those in VF16. There's a lot of sort of careful engineering that has to happen in order to make sure that, you know, these models are actually stable when they're being trained with lower precision. But if you can do it, that's really great because you've basically doubled the throughput of your bottleneck going from 32 to 16 net, right? If you memory is your bottle. Okay. The other one, and I think this is kind of what a lot of people think of when they say like, I'm gonna write a Kuda kernel or something. Operator fusion is kind of both very intuitive and both a like a fun, natural one to think about. So one memory or sorry, one mental model of how a GPU works and how memory works is this kind of fun diagram of a factory from horse heat, right? So imagine you have a factory and your factory is your compute part, right? And so, you know, it takes in little box widgets and then outputs little triangle widgets. And if you grow your compute, but your bell conveyor, you know, that takes memory to compute is, you know, finite bandwidth, you know, you're not gonna be able to use your second factory, right? Like you're still capped by the speed at which you can transfer things from memory to compute. And so you've got this bottleneck. Now, of course, you already knew that, right? I've been sort of hammering in the memory bottleneck thing, but I think one insidious way in which you can incur a ton of overhead without really realizing it, it's kind of this left hand side computation pattern, right? So, you know, imagine the left side of this plot is where the memory is, the right side is your compute unit. And so to do computation, I start with a square and I move my squares from my memory to my compute. I do some operation, I turn them into triangles, right now I ship my triangles back to memory. And then, you know, okay, I realize I need a triangles again. So I ship them back into the compute unit. Now the triangles become circles and so on and so forth, right? I send my compute sort of back and forth and back and forth back to memory. And you might call this kind of a very naive approach. And if you were just doing operations naively on the GPU and just shipping the results straight back to global memory, this is what you'd end up with, right? And if you count the number of times a piece. Of data went back and forth, this is pretty terrible. You've incurred tons of memory overhead. Now you should be able to realize that if you look at the right side, well, this compute, well, there's no dependency. So I should be able to go square to triangle, the circle to rectangle, and ship the rectangle back, right? I can just keep everything in the compute unit the whole time, right? And that's the right hand side diagram. And this is the mental model of a fused kernel, right? You have a bunch of operations that are going to happen on a piece of data in sequence instead of writing it back into storage, what I'm going to do is I'm going to do all the computation as much as I can in one place. And then only when I have to ship it back to memory, right? So that's this idea of kernel fusion. Okay, there's some very simple examples of how if you write some naive code, you might, you know, get sort of a naive set of launches. So here's an example. I wrote a little, let's say, neural network module. You know, let's say I write a neural network module that takes in X and it produces sine squared x and cosine squared x, right? Simple code. Now if I run this, you know, the computation graph in Pi Torch is going to look something like this. And it's gonna, you know, launch a whole bunch of CUDA kernels. It's gonna launch, take in the X, and it'll launch a CUDA kernel to compute sine x, a launch one to compute cosine x, then sine squared of x and cosine squared of x and sine squared x plus cosine squared of x. Right? So there's a bunch of back and forth that has to happen in order to do this computation. It's exactly the left hand side figure that I showed you before. But if you were a little smarter, right, and you either wrote your own Kuda kernel or you use something like torch compile. Well, you can easily realize that those five operations don't really depend on very much, like they use only a little bit of memory. And so you can fuse them into a single operation that does everything on GPU on a single thread without sending things back to global memory, right? So really easy fusion operations like this can be done automatically by compilers. I just mentioned Torch Compile. If you aren't already doing this, you know, you should consider strongly thinking about using torch compile everywhere. We'll show you in the assignment torch compile as well. It's pretty nice. Okay, so I've gone through precision and fusion. If anyone has questions, let me know. Before I move on to recomputation and other kinds of tricks that we can do on the GPU. Okay, good. So another thing that we can do is called recomputation. And recomputation is this idea of sort of spending more compute to avoid having to do memory access, right? So remember that your, you know, original back propagation lecture, this one's actually from CS 221. What do we do? Well, we take our inputs at the very bottom, these are the yellow ones, and then we propagate activations upwards. Those are also the yellow values on the tree. And then we compute the Jacobians backwards. Those are the greeting values on the edges. And then to compute my gradients, I'm going to propagate you multiply. So the Jacobian and the activations that might propagate the gradients backward, right? Well, if you think about it, those yellow values after the forward pass have to be stored, right? And then they're stored and then they have to be taken from global memory where I stored them and put them into the compute unit, right? Mechanically, that's how it has to happen. But that might actually be a ton of sort of memory inputs and outputs happening. Instead. You might actually be able to avoid this. So let me give you an example of how recomputation can speed things up. Here's another sort of silly function that I might write. I'm just going to stack three sigmoids on top of each other, right? You can look at the left. That's the forward graph. That should be exactly, you know, your mental model of three sigmoids on top of each other. Now you know the computation and graph for this. I'm gonna compute the sigmoids and I'm gonna store S1 and S2, which are the activations of the sigmoids. And I have my outputs. And then, you know, that's my sort of forward pass. Now the backward pass in this is kind of terrible. When I do my backward graph, I need to go and take S1 and S2. And I need to take, you know, the gradients coming sort of backwards into this out box and then push it into this, you know, backwards computation and I'll get the gradient of x, right? So I need to have 3 memory reads, one memory, right, in order to compute the backwards pass. And then for the forward pass, I need to do one memory read of X and I need to do three memory writes for S1,S2 and out. Right? So hopefully that's clear. This is, you know, a decent amount of memory reason, right? I have to do eight of them. And I have very low arithmetic intensity because I have no matrix multiplies at all. So the idea of recomputation is to say, I don't want to store those activations at all, right? Like I'm not gonna put them into memory. I'm just gonna recompute them on the fly in my backward pass, right? So now in my new forward pass, I don't store S1 and S2. I take X as input. I compute my signal. Points and I get my output, right? So now that's one memory read for x 1 memory, right? For L. Right now in my backward pass, right, I don't have activations anymore. So what I'm gonna do is I'm gonna get both d out, which is, you know, the backward signal coming in from above, and then X, which is my input, right? So I'm gonna take two of those, which is 2 memory reads, and then sort of on the fly in my SM and my local memory, I'm going to compute each of these sigmoids and I'm gonna put them into the backward graph, right? I'm gonna recompute S1, s two and out on the fly inside sort of my local memory. And because I do that, there's no global memory reads happening here. And then I have one memory, right, which is DX, right? So now if you compare the two, I have 5/8 of the memory access for the exact same computation, right? The price that we paid is that I'm gonna have to recompute these three sigmoids. But if you were running sort of idle anyway, because you were memory capped, this is a great tradeoff, right? Like you would be very happy with this because now you've traded compute, which you have too much of for memory bandwidth, which you had too little off, right? So this is one great way of trading one thing you need for another thing that you have. And of course, this is different. It's the same trick as sort of gradient checkpointing and recomputing activations for memory savings. But this is being done for different reasons. This is for sort of execution speed, not just because you're running out of memory, right? So it's the same technique, but for different goals. Okay. And then this one, I think is actually kind of a really interesting one and not one that I knew until I started sort of really looking into how the hardware model of a GPU and DRAM works. So the slow memory, the global memory called DRAM in a GPU, that's actually very slow. And in order to make it faster, there are certain optimizations that are being done at the hardware level. And one of the optimizations that's done at a hardware level for DRAM is that when you go and read a piece of memory, you don't actually get just that value back, you actually get a whole chunk of the memory back. And this is called burst mode. So let's say I went on and re try to read the very first value of this big memory block, right? Instead of just the memory giving me back 0, it would actually give me back 0,1,2,3, right? It would give me back four values at once. It would be like, here you go. You know, I'm sure you'll need the 1,2, and 3,2 in the future. And so each address space is cut up into what's called birth sections. And then you're given the entire birth section rather than just what you looked for. And this might seem very mystifying, like why would the memory give you three extra, you know, bytes for free when you're just asking for one? There's sort of like a very interesting hardware reason, which is that when you're addressing into the memory, you know, in order to send the signal out from the memory that those bytes have to be moved to an amplifier. That's the slow step. And once you've done that, you can get many bytes for free. And so that's why sort of this burst section thing exists. It's kind of masking this more expensive step of actually moving where the data is stored to the sample file. But kind of regardless, this kind of means that we might be able to significantly accelerate sort of our memory access if the pattern of memory access is good, right? So if I want to read this entire, you know, block over here, if I access it in random order, right? Then I'm gonna have to, you know, basically query a number of times equal roughly to the length of my query, right? But if I sort of go and I check the very first value, then I'm going to get all this entire burst section at once. And then if I go and check No. 4, I'll get this burst section, the second burst section at once. And so I can, you know, basically get four times the throughput if I'm really clever about my memory accesses and only access just the bits I need from each burst section. So this is called memory coalescing. So if all the threads in a warp fall within the same burst, then basically the sort of smart hardware and programming model will basically group those queries instead of querying 0,1,2,3. It will group them and say, just give me zero and then I will be able to read out all the 0,1,2,3 at once from this kind of burst mode DRAM, right? So remember that, you know, a warp is 32 sort of numbered threads. And so memory accesses from a warp happen together. And so when these warps are reading in to these kind of burst sections, there's optimizations that can be done so that you're getting all four bytes at once rather than getting one of them at a time individually. And so that will forex the throughput that you have on your memory. So these are kind of very simple things, but they're actually very important. Like imagine I'm gonna do matrix multiplations, right? This is a core thing that you're gonna have to do a ton if you were to sort of implement, let's say, neural network really from scratch. And Kuda in this case, imagine I'm going to read my matrices in one of two ways. I can read it by traversing the rows. Right, so each thread is going to traverse the row, or I can sort of read it in sort of column order. So each thread is going to go down a column, right? Turns out that this left one where you're sort of going across different rows, so each thread is accessing a different, sorry, each thread is going through columns. This left model is going to be quite slow because the memory reads are not going to be coalesced. Whereas if you're going to this right side where each of the threads are going down, so they're incrementing in rows, then these memory reads will be coalesce. And so, you know, you can think about it for a moment why this is true when I first looked at this diagram, I was like, isn't it reversed? Actually not. This is the correct one. And the way to think about this, right, is, let's say on this right hand side diagram over here, I'm gonna have a thread that's trying to a series of threads that's trying to access, you know, left to right. So each thread is going to try to load, you know, the very first element. And then in the next time step, I'm going to load the element from the this column, the second column, and then the third column and the fourth column and so on. So if that happens, what happens at time step one, right? A time step 1, my first thread loads this point and then the second thread loads this point and then this point and that point, right? So those can't be coalesced at all. They're reading different burst sections. And so that means that I have to read this entire chunk of memory in order to perform any sort of an operation. Instead, if I was sort of going in the column direction, all the threads will be reading within the single burst section. And then, so only one memory read operation needs to be performed and you get all the memory at once, right? This is a very low level optimization, but this is very important, right? If your memory traversal order is all wrong, you will actually get much slower memory accesses then you really want. Okay, so then that brings us to kind of the very last and kind of big one, and this is the idea of piling. And piling is this idea that you would like to group together memory accesses in order to minimize the amount of global memory access that we have to do. And so to explain this one, I'm going to try to go through this example of a matrix multiply, and hopefully I'll be able to sort of explain to you why sort of a naive algorithm for doing matrix multiply is gonna be very problematic. And then afterwards, I'm gonna give you a tiled version of the same idea. And hopefully you'll be able to see why that's gonna reduce the number of global memory reads that you have to do. So let's start with this very simple matrix multiply algorithm. So, you know, I've got a matrix, you know, I got this matrix on the left side. I'm not my end matrix on the top. And in order to compute, you know, the matrix product, right, I'm gonna have to traverse over the rows of m and the columns of N, and then take the inner product and store that into this p matrix, right? The corresponding rows. And I've written out here each of the threads, O00110,1, one corresponding to where they're sort of storing their outputs and sort of the access order in which they access each of the individual elements. Now notice here that, you know, what's gonna happen is that the memory access here is not coalesce like the row matrices here. These are going to be accessed in a non coalesced order. And I have repeated memory accesses, right? So I've got M00 being accessed in the first thread, M00 accessed here, M0N10 being accessed in two different threads. You know, so these values are being kind of read over and over from global memory into many different threads. And so this is going to be potentially very slow.

说话人 1 48:43
So there's a question of can we avoid having too many global memory reads and writes? What I would ideally like to do, right? So let me explain kind of the ideal outcome first, and then I'll explain the algorithm. The ideal outcome is that I would like to spend one sort of, you know, chunk of time loading pieces from global memory to shared memory where things are fast. I want to do a ton of computation and shared memory, and then I want to kind of be done with that piece of data, right?

说话人 1 49:10
That's the ideal outcome. I've minimized my global memory accesses. So now how can I do this in this matrix multiply world?

说话人 1 49:17
So now what I'm going to do is I'm going to take my matrices, both the n matrix and the n matrix, and I'm going to cut them up right into tiles. So here I've cut this up into 2 by 2 tiles. So I've got a 2 by 2 m tile and a 2 by 2 n tile, right? So I've got basically smaller sub matrices within each of the matrix. And now imagine that my shared memory is big enough to be able to fit these sub matrices right within each of these SMS. So now this gives a very simple algorithm with which we can do computation.

说话人 1 49:46
So what I'm going to do is I'm going to first load, you know, let's say this m 0,0 tile on the top left over here. And I'm going to also load my n 00 tile into shared memory here. Right. So now I have. These partial sums that I can compute, I can take, you know, the row product of m zero M01 with N00, n one 0, and I can increment that into P00. I can do the same with all of the different sub matrices that I can fill out over here right now. Then once I'm completely done sort of processing these two tiles, then I can load a new tile over here and then I can repeat that computation with my m tile and my n 2.0 tile loaded into shared memory.

说话人 1 50:28
And then I can sort of increment my partial sums in P, right? So now I've really sort of consolidated and reduced the amount of global memory access I have to do, right? I load as much memory as I can at once into shared memory. I do all of my sort of sub matrix computations on that tile that I can and then I move on to the next one, right? And of course, the other nice thing is that because I'm loading an entire tile, you know, I can traverse these sub matrices and whatever order I want, like column major or row major. And so I can coalesce all of the memory accesses whenever I'm loading a tile from global to shared memory, right? So there's kind of wins all around here when we tile our accesses. So we can do a little bit of piling math.

说话人 1 51:12
So we've got, let's say, a matrix a matrix B, and a matrix C. So let's say the full matrix C, these are square matrices are of size n. And let's say I have a tile of size t, right? Oh, yes. Question, slide. Oh, load up. 0,0. So 3 uploading and 0,0 again.

说话人 1 51:33
Okay, so in that case, I just wrote it for completeness, but m 0,0, let's say it's just, you know, stored in shared memory. Let's just keep it cached. I won't load it again. That's definitely just there for completeness, not that you would actually like discard and reload the matrix again. That would be kind of insane.

说话人 1 51:50
Cool. Okay. And so we can kind of do very simple tiling math to think about, you know, what's happening.

说话人 1 51:57
So let's say I'm gonna do an end by end matrix multiply, right? So if I do a non piled matrix multiply, if I'm just going over rows and columns, then every input, every time I process it has to come from global memory. So each input is read sort of n times from global memory, right? So each of these is read sort of n times. If I do a tiled matrix multiply.

说话人 1 52:18
Well, you know, the global reads are operating over tiles. So I'm reading each input n over times from global memory, and I'm reading times within each tile, right? Of course, I'm doing matrix multiplies, so I can't reduce the total number of reads. I have to read all the matrix elements, but I can shift the reads into basically fast shared memory, right? So I do times memory, reads into shared memory and n over key times from global memory.

说话人 1 52:44
And that's great because if we have a big shared memory that can store big tiles, that's a factor of t reduction in the total amount of data that has to come from global memory, right? So tiling can be really powerful of an idea when you're operating over a matrices. You can move things into shared memory. Piling is quite complex. This is the source of many sort of confusing things about GPU and matrix multiply performance.

说话人 1 53:12
One thing that can happen, right, once we start piling things, you start asking things about discretization, right? So imagine I have a tile size of 128. That seems like a nice good round tile size. But then, you know, when I have a full matrix of 256 size, that's great. That's a 2 by 2 tile.

说话人 1 53:31
Things load nicely. Now let's say I have a 257 size tile on the column side. Now this is a bad time because I need to have 6 tiles in order to cover this matrix.

说话人 1 53:43
And the two tiles on the right are very sparse. There's just not much stuff in there, right? And the problem with this is that each tile is going to be assigned to SM, right? So each of these tiles is going to be a block and each thread is going to be operating within each tile. So those two tiles on the right, they're not gonna be doing very much at all, right?

说话人 1 54:03
Those SMS are gonna be basically be sitting idle. And if you were kind of compute cap, you would have wanted to more evenly distribute the load between SMS, right?

说话人 1 54:11
So you have to basically optimize your tile sizes to try to avoid these kinds of scenarios. But in reality, right, there's a lot of complex things that go into setting the tile sides, right? Remember, you have to call us your memory accesses. You have to think carefully about that. You have to not exceed your shared memory size, right?

说话人 1 54:30
So the house can't be too big and you have to divide the matrix dimension hopefully evenly or as close to evenly as possible. So you don't end up with this situation of sort of an underutilized SM at the very, yes. So if you have, say, smaller sizes, do something, if our good GPUs do something like prevention, where they can like fetch the next child team beforehand. And so like, would that happen the. Excellent. Yeah, so you're asking about whether or not you can like overlap memory reads and computation.

说话人 1 55:08
And yeah, that's naturally done in GPUs. Like they're always like trying to use the available bandwidth.

说话人 1 55:14
Like as long as shared memory is available, they can go and put things into it. The issue is that whenever you're, you know, effectively utilizing your SMS, you're basically maxed out on your shared memory, right? That's like the bottleneck resource. And so there is no place to prefetch in some sense. Cool.

说话人 1 55:33
Okay. And the other thing that is very, very, you know, we're getting into the weeds here, complex is the interaction between piling and sort of burst sections. So imagine I have a matrix layout that's kind of like this, where, you know, I have my nice burst sections and each burst section lines up nicely with a tile. So to read this tile, all I have to do is to, you know, get four different birth sections. And I've gotten this entire tile.

说话人 1 56:06
Now imagine what happens if I add sort of one element extra. And the way the matrix is laid out, you know, might sort of tile sort of my burst sections flow over. So now what's happening is when I load my tile, I'm going to load this first part. And that's really great. I get the entire first row as a burst section.

说话人 1 56:26
Now in the second row, this actually belongs to two different burst sections. And so I have to do two reads in order to get this second row and so on and so forth. So I've essentially doubled the number of memory accesses because I've added a single extra element at the very end there that's kind of bumped up the alignment of my burst section and my aligned layout. And so basically, if tiles or your matrix sizes aren't multiples of your burst section, you can easily end up with situations like this, where the rows don't line up with the burst section and you've doubled the amount of memory access that you have to do. And the way to get around this is you have to do padding to be able to kind of get nice rounds, matrix sizes, so that your burst sections line up with the size of your tiles, right?

说话人 1 57:11
So this is getting very into the weeds here. But if you really want to squeeze out all the performance from your matrix multiplies, these are the kinds of things you have to think about, right? And you will get bitten by this if you're not thinking about it. And of course, I guess like things like torch compile and all the CUDA optimizations for matrix multiplies, they're doing exactly the kinds of stuff that I just talked about, right? That's the way you get better performance.

说话人 1 57:37
And so, you know, all of this matrix complexity, you know, ends up in situations like this where, you know, the, I'm reading out Andres, this tweet here, but you know, the most dramatic optimization to nano GPT is to increase the vocab size from 5257 to 5304 which is the nearest multiple 64, which gives you much higher occupancy. Careful with your powers of two, right? So that's a 25% speed up from adding how many? It's like 50,57, or 47 dimensions to your vocab. Like that's kind of like, you know, how does that happen, right? And so that kind of brings us back to the mystery.

说话人 1 58:18
Like, you know, I was dragging you through all of the GPU details in the hopes that, you know, you'll have a full understanding of all the performance characteristics. But in some sense, the payoff is, you know, I now get to explain to you how this chart comes to be. And at the end, you won't find matrix multiply performance to be so mysterious or scary at the end here, right? So the very first part is very simple. Like we understand compute intensity, right?

说话人 1 58:43
This is exactly the roof line that I pointed out at the very beginning, right? So up until here, which is about 1536, right? There's just not enough matrix multiply work to do, right? Just loading the matrix and doing very basic Io, right? That you have to do is becoming a bottleneck below this point, right? So throughput is going to fall through to the ground.

说话人 1 59:06
Past this point, you just don't have enough memory bandwidth to support your compute units. Now on the right side here, in theory, right, if I draw the upper envelope, this is the kind of maximum achievable performance. So it's possible up here to saturate all of my compute units and get really great performance.

说话人 1 59:24
But if you kind of mess up your matrix sizing, you can end up in these kind of really weird places. And within each one of these, you can kind of end up in a weird trough. And so we're gonna kind of think a little bit about, you know, why do you have all these different places you can end up?

说话人 1 59:39
So the very first thing, this first line here, this is a tiling alignment issue. So if you look at kind of the multiples here, so I've now colored each of these lines based on kind of the divisibility of the matrix size. And this is the size by which it's divisible. So if it's divisible by 32. Then you're in good shape. You're in these purple dots up here.

说话人 1 01:00:02
If you're divisible by 16, you're actually still up here. There's two colors. And then if you're green, your cake equals 8. You're up here. If you're orange, your cake was 2. And if your cake equals 1, you're all the way down here. If you're not divisible by any number, don't take prime dimensions, you're not going to get very good throughput on your matrix multiplies.

说话人 1 01:00:24
And a big part of this is gonna be, you know, once you get to kind of cables two and cables one, you are basically forcing the situation where you can no longer read tiles in the sort of nicely aligned way with your burst reads. And that's gonna lead to some serious issues. So that's kind of a problem. But then, okay, so, so that's one part of the mystery. But I think another part of the mystery remains like, so within this orange line, you know, I think if you zoom into here, you see this giant drop, right from this point all the way down to this point where you're just kind of wondering what happened here?

说话人 1 01:00:59
How could I lose so much performance increasing my dimension by q? And so let's just look at these numbers. And it's just, I think this is a fun puzzle. So I'm just gonna walk you through the puzzle. This is gonna happen when you transition from 1792 to 1790.

说话人 1 01:01:16
I guess three or four size, let's say four here, just so that's a factor of two still. Well, why does that happen? Okay, well, let's say that we're using a tile size of 256 by 128, that's a pretty natural size. As a fun fact, you know, the matrix multiply units in these GPUs, they're naturally operating on matrices of roughly size 128. So 256 by 128 is a very nice tile size, right?

说话人 1 01:01:42
So that means how many tiles are there? Well, there's seven times 14 tiles, right? Because we're dividing the dimension of the matrix by the size of our tiles. That's a total of 98 different tiles. And if we increase this by one, well, you know, we're gonna have to round up each one of our coordinates.

说话人 1 01:01:58
And so we're gonna have a lot more tiles, 120, right? So we've increased the number of tiles by quite a bit. Well, you know, what's going to happen is not only did we significantly increase the tiles and some of them have lower utilization, which is bad, but actually even worse, an A100 has 108 SMS, right? And if I if you go all the way back to the kind of the GPU execution model, right?

说话人 1 01:02:21
SMS can execute in parallel and they're kind of the execution units. And so when you have 98 SMS, they all go and run, right? You can dispatch them all the SMS are running. You know, you've got great utilization. Once you go to 120 tiles, now you've got more tiles than SMS.

说话人 1 01:02:37
So 108 of those will execute. And then you will go back and you'll say, all right, I've got some more SMS, a very low utilization. You're gonna execute the remaining 12 and wait for those to complete, right? And that's gonna be really bad. So if you look at your utilization, you'll be a good utilization for a while, you'll drop off a cliff and then you'll sort of finish up your job, right?

说话人 1 01:02:53
So this is something called wave quantization. And so ideally your tile sizes are either much bigger than the number of SMS or, you know, they're not like this where you're just like barely over the SM and you've caused this quantity optimization sort of error additional. Cool. All right. I know this is low level details, but in many ways, you know, I've been saying through many classes that language models and deep learning is attention to detail and these kinds of attention to details, the things that allow people to scale up LMS to really large sizes and get great performance.

说话人 1 01:03:28
So it's worth knowing even if you're not a person that's gonna do systems engineering. So what were the tricks, right? Key ideas here. First one is you gotta reduce the amount of memory accesses, right? So there's lots of ways to do it. You can do coalescing, right?

说话人 1 01:03:43
So that you're not, you can sort of reuse reads that you're getting for free. You can do fusion so that you know you can fuse multiple operations together and avoid unnecessary reads and writes. You can move memory to shared memory. So, you know, even if you're gonna do reads, they're gonna be from much faster memory, and that's gonna be sort of piling tricks that you can do. And then finally, you can kind of trade memory for other resources that you do have, right?

说话人 1 01:04:07
So you can trade it for compute, which is going to be recomputation, or you can trade it for just numerical precision or stability, which is gonna be quantization, right? So there's lots of bags of tricks that you have in order to get sort of performance out, right? So there's lots of things you can do. You just have to be really mindful of kind of the role that memory plays in the performance of a GPU, right?

说话人 1 01:04:28
That's kind of the key thing to get the map the most out. Cool. Any questions on that before I sort of move to the final part with flash attention? Okay, good. All right, so now I'm gonna put it all together, right? Like, I'm gonna try to make it so that all the tricks that I taught you aren't these like random disconnected factors about GPUs. They're kind of part of the standard performance optimization toolkit and flash attention too. We'll hopefully teach you how that all comes together to build one of the.

说话人 1 01:05:00
You know, the foundations, I guess, of modern high performance transformers. So flash attention, you know, we know that it dramatically accelerates attention. And most of you probably know that's done through some CUDA kernel magic, but maybe you don't know all the details, right? So, you know, what the paper says is, okay, so there's one part that's happening, which is, you know, you do attention on a unoffbed optimize, you know, Pytorch transformer implementation. If you fuse the kernel and you do some things, you can get significant speed UPS. And from the paper, you know, they say we apply two established techniques, tiling and recomputation, to overcome the technical challenge of computing exact attention and subcredit quadratic HBM accesses, right?

说话人 1 01:05:41
So it's not subquadratic, you know, computation because you can't do that. You have to compute, you know, attention in general. But they're going to get subquadratic accesses to the high bandwidth or global memory, right? And so that's really the key. If your memory is the bottleneck, you know, you want to make that not quadratic so that at least you can pay for quadratic cost with your compute rather than with your memory.

说话人 1 01:06:03
So just for a really quick recap, you know, at this point, you've implemented attention many times in many classes, right? So it's going to be three different matrix multiplies. You've got a KQ and V with a Softmax in between.

说话人 1 01:06:16
So the matrix multiplies are pretty simple that can be, you know, done with tiling. I've showed you examples like that. And what's different about attention? Well, there's a Softmax thing. That's gonna be the real tricky bit.

说话人 1 01:06:28
And then once we can deal with the Softmax, all of the sort of matrix multiply things I was talking about will just come into play. So the matrix multiply, as I said before, is exactly what I taught you. So if you look at the figure 1 from the flash attention paper, this is really just a simple tiled matrix multiply, right? You see, you know, the K matrix, the Q matrix, you see it cut up into small blocks. You know, small blocks of it are being copied to SRAM, they're being multiplied, and then they're being, you know, accumulate this sent to the HBM where you do soft maxes and then you multiply with a v. So this is all just really simple in terms of the KQV matrix multiply.

说话人 1 01:07:09
But now we have to think about the Softmax, right? Like what's going on with the Softmax. So the key thing here is the Softmax. Sorry, I'm gonna roll back one step. So the issue with the Softmax, what's the problem with the Softmax? It's a global operation, right? The Softmax in an attention operates row by row.

说话人 1 01:07:26
You have to sum the entire row, right, to compute sort of the sum normalizing term of the Softmax. And that's very problematic. If I have tiles, right? Ideally, I want to do everything within the tiles, right? I don't ever want to have to write back to the big matrix.

说话人 1 01:07:40
And so I need a Softmax that can be computed online within each tile, right? I want to do as much computation within each tile as possible.

说话人 1 01:07:49
So the key thing here is to use what's called the online Softmax. And so what is that? If you have a stream of values, right? Normally the batch version of the Softmax, you take all of your X1 through X of NS, and you would exponentiate them, sum them, and you would divide them, right? That's what you would do in your normal Softmax.

说话人 1 01:08:08
And then you would, you know, maybe compute the maximum value and you subtract that in order to be able to make this numerically stable, right? So this is the standard numerically stable Softmax on the left side. So the online Softmax, I've taken this from mikeloff and gimmerstein in 2018. Well, you can sort of realize that you can pull out via sort of like a telescoping, some kind of an argument, basically, the current running sort of normalizer term and the current sort of top term of E to the Xi minus max of XK, right? So what you're gonna do is you're gonna maintain your current max that you've seen over X1 through X of J, which is my current iteration.

说话人 1 01:08:48
And then I'm also going to maintain sort of this correction term. If my max updated, this is going to basically correct my max. And then I'm going to add my sort of new term over here, right? So this d of J is going to track online the top term of this equation 2 over here. And then, you know, at the end, I can also then compute the normalizer and then sort of get the normalized y of I that I want, right? This d of V as itself, sort of the normalization term that I need.

说话人 1 01:09:18
So the key thing here is that this can be done online. I don't need the X1 through X event upfront. All I need is sort of the stream of X1 through XN. And that's really key because I can now compute the Softmax tile by tile, right? Within each tile, I can run this algorithm and that will let me compute kind of the partial Softmax for that tile.

说话人 1 01:09:38
And then I can sort of write back if I need to, all the components that I sort of, I'm keeping track of. And that's all that I kind of need in order to do this computation, right? So I never have to materialize the full n squared matrix in order to compute the soft map. And so that's basically it. But once you have that, you know you've put it all. Altogether, and you can get the forward pass of flash attention.

说话人 1 01:10:03
And if you go and look at the flash attention to paper, which is gonna be a thing that we're gonna ask you to implement. So you're gonna be following through kind of these steps here. You're going to see exactly this idea. So first you're going to have your KQ matrix multiply. And this is going to be tiled.

说话人 1 01:10:19
So these are little tiled chunks and they're gonna be multiplied. And how am I gonna compete the soft Maxwell? I'm gonna maintain sort of a running value of these sort of exponentiated sums, and then I'm gonna keep incrementally updating it and correcting for the maximum terms. And by doing that, I can compute all of the necessary quantities kind of tile by tile, sort of going from one tile to another, and then just multiply once again with tiles with V in the end. And that will give me sort of my full soft match as output, right?

说话人 1 01:10:50
Yes. So we won't be able to compute that out until we compute the like UK multiplication across all files, right? So we'll do have to double back on each value. So the question one is, you can't compute this until you are done with all the tiles.

说话人 1 01:11:07
And so you have to double back on all the tiles. So yourself. That's right. So you will have to, before you can output your Softmax, you will have to go through all the tiles. This is correct.

说话人 1 01:11:22
But by, let's say I do all the tiles once, right? Like I do all n square tiles. At that point, I have all the components that I need in order to directly output the soft maps at that point, I don't have to re do recomputation because I have the normalizer terms already, right by going through each of these kind of tiles at the end of going through all these tiles, I've built up, you know, L3 or L, L of N, which is the sum of all of the exponentiated terms. So I already have that in my sort of in my shared memory for this last tile.

说话人 1 01:11:52
And then that allows me to exponentiate and divide and then return all of the components. Okay, so the backward pass I'm not gonna cover. You can do recomputation tile by tile, which will allow you to avoid storing the soft map, right? Remember, you know, I always want to avoid storing anything that's of size n square.

说话人 1 01:12:14
And so here I've been sort of clever with the tiles so that I don't have to store any of the n squared components when I'm computing, for example, the Softmax. But in the backwards pass, if I store the activations, that's already something that's n squared size, right? So I don't want to store my n squared activations. I'm gonna have to recompute it on the fly tile by tile when I do the backwards pass, right? So that's a really key other trick that they do in order to make the backwards pass possible. But otherwise, it's fairly standard. It's really the same thing as computing ingredients just tile by tile and doing that computation. So, okay, that brings us to the end here.

说话人 1 01:12:52
Hopefully you've kind of seen how all of the pieces I talked about tiling and coalescing and recomputation come together to give you flash attention and all these really cool things that make your transformers go much faster. So to, you know, recap for the whole lecture, right, hardware is kind of the thing that has really powered all of the language models that we have today. And so if you really want to leverage your hardware, you have to understand the low level details. I think all of the systems advances really engaged with a lot of the concepts that I taught today. And the current GPU sort of scaling, you know, that plot is really the one you should remember, really incentivizes and encourages you to think about memory movement, right? Like the memory movement is the bottleneck in all of this. And so you don't want to just think about, oh, how do I reduce the number of flops? That's important too, really. You really have to think about, okay, how do I make my memory movements more efficient? And then finally, if you, you know, have to do a certain amount of computation, well, to optimize things, the way to do it is to optimize your data movement, right? To be able to avoid as much movement from the high bandwidth memory or the global memory as possible. You want to reduce that and have everything in the very fast shared memory and that leads to good performance on things like flash attention. Thanks, everyone.

