好的, 我将开始翻译.

2025年7月14日 下午 10:13|1小时 20分钟 13秒
关键词:
same thing、single thing、different kernel、same time、CPU time、compute time、time computations、last thing、performance things、more time、basic things、benchmark things、Triton time、few things、execution time、profiler thing、GPU times、last time
文字记录:
说话人 1 00:00
编写高性能的GPU代码. 第二次作业的部分内容将要求你们进行大量的性能分析. 你们还需要为 flash attention 编写自己的 Triton 核函数. 你们需要让所有这些模块都达到非常高的性能. 因此, 在这次讲座中, 我们将稍微深入地探讨一下, 并尝试为语言模型中的标准组件编写一些高性能代码.
说话人 1 00:28
所以, 本次讲座的计划是, 我们会先简单回顾一下 GPU 的相关知识, 只是为了确保大家再次掌握理解讲座后续内容所需的基本 GPU 组件. 然后, 我会向大家展示一些关于基准测试(benchmarking)和性能分析(profiling)的非常基础的知识, 这对于完成作业和日常编写高性能的 PyTorch 深度学习代码都很有帮助. 接着, 我们将开始实际编写一些核函数. 我们会用类似 C++ 的方式编写 CUDA 核函数, 然后在 Triton 中做同样的事情. 最后, 我们会采取一种简单但效果很好的方法, 即使用 PyTorch 自带的 JIT 编译器来为我们进行优化.
说话人 1 01:14
之后, 我们会对所有这些方法进行比较, 并进行性能分析和基准测试. 在此过程中, 我们会进行深入挖掘, 一直探究到 PTX 层面.
说话人 1 01:22
PTX 已经非常接近机器码了, 这能帮助我们理解当我们编写这些代码时, GPU 在底层到底做了什么. 最后, 如果时间允许, 我想我们能完成一个用 Triton 实现的快速 Softmax. 第一次作业已经结束了. 排行榜依然开放, 大家仍然可以提交和更新. 可能有些同学会使用延迟提交的机会, 所以请尽快完成第一次作业, 而第二次作业现在已经发布了.
说话人 1 01:53
正如我之前所说, 第二次作业中有很多系统层面的工作需要你们去完成. 其中有趣的部分涉及到 GPU 核函数, 你们现在就可以开始做了. 下周我们会讨论并行计算, 这将是作业的另外一半内容, 包括编写快速的并行代码, 如数据并行等等. 这些我们下周会讲到.
说话人 1 02:16
好的, 现在我们来回顾一下 GPU 的工作原理. 当我们使用像 A100 或 H100 这样的 GPU 时, 它内部会有大量的 SM (Streaming Multiprocessors, 流式多处理器). 每个 SM 内部又包含大量的计算单元, 比如用于 int32 或 FP32 计算的单元. 然后, 每个 SM 会启动大量的线程.
说话人 1 02:38
GPU 也有内存层级结构:我们有 DRAM 或称为全局内存, 它容量大但速度慢. 然后是速度快得多的缓存. 实际上, 大家可以看到这里有一个叫做“寄存器文件”的东西. 这是一种非常高速的内存, 每个线程都可以访问. 今天在编写高性能 GPU 代码时, 我们会大量使用这些寄存器. 所以, 执行模型的基本结构是, 我们会有一组线程块(thread blocks), 每个线程块会被调度到单个 SM 上执行. 这是我们需要考虑的原子单元, 尤其是在使用像 Triton 这样的工具编写代码时.
说话人 1 03:16
在每个线程块内部, 又会有大量的线程. 真正执行计算的是这些线程. 所以, 如果你要对一个向量进行操作, 比如处理向量中的元素, 你编写的代码会让每个线程进入并可能一次性处理该向量的几个元素. 所有线程协同工作, 最终完成对整个向量的处理. 那么, 为什么要有线程块这个概念呢？
说话人 1 03:40
为什么不直接在全局上下文中使用线程呢？原因在于, 线程块内的线程可以相互通信. 它们在 SM 内部共享一块相当快的内存. 所以, 当你需要执行像矩阵乘法这样的操作时, 你需要在线程之间传递信息, 而在一个线程块内, 这个过程是非常快的.
说话人 1 03:56
然而, 跨线程块或跨线程组的通信则会非常昂贵. 所以, 你需要的任何数据, 都应该尽量保持在同一个线程块或同一个集合内. 这样可以保持非常高的速度, 其速度堪比 L1 缓存.
说话人 1 04:12
这是一个非常理想的状态. 你可以利用共享内存来同步线程, 但你无法跨线程块进行同步. 你很难控制跨块会发生什么. 还记得我上周提到的“线程束”(warps)吗？线程束通常不是你会直接考虑的概念, 但对于性能而言, 它是一个重要组成部分. 当我们实际运行这些程序时, 线程会被分组, 每 32 个连续的线程组成一个线程束. 这个线程束会在一个 SM 上被一次性执行. 所以, 我们希望做的一件事就是确保所有的线程束都有大致相等的计算量.
说话人 1 04:51
虽然不总能做到, 但如果可以, 我们应该尽量去实现. 我们希望线程块的数量理想情况下能够整除 SM 的数量, 并确保每个线程束的工作量均等. 所以, 理想情况是我们拥有的线程块数量远多于 SM 的数量. 在编写高性能代码时, 我们会努力实现这一点.
说话人 1 05:11
好的, 最后一个概念, 也许是这里最重要的概念之一, 就是算术强度(arithmetic intensity). 我们希望保持高的算术强度, 也就是说, 浮点运算次数(flops)要多于内存移动的字节数.
说话人 1 05:24
这是因为, 如果你还记得上节课的性能扩展图, 我们的计算性能扩展速度远快于内存性能的扩展速度. 因此, 很多时候计算任务最终会变成内存受限(memory bound), 导致我们无法充分利用计算能力. 所以, 总的来说, 如果方法得当, 矩阵乘法是计算受限(compute bound)的. 而其他几乎所有操作都是内存受限的. 我们将尝试巧妙地减少内存受限的操作数量, 或者减轻内存受限的严重程度.
说话人 1 05:53
以上就是对 GPU 的简要回顾. 希望大家还记得这些内容, 对执行模型还有一个清晰的印象. 如果你们对这一切如何运作还有任何疑问或问题, 随时可以打断我并提问. 是的. 不好意思, 线程束(warp)的功能是什么？它本质上是一组被同时执行的线程.
说话人 1 06:19
线程束之所以存在, 是因为它们减少了所需的控制逻辑开销. 因为你同时执行所有这些线程, 你不需要为每个线程都配备一个控制单元, 只需要为每 32 个线程的块配备一个就行了. 所以你会看到, 例如, 计算单元的数量远多于线程束调度器的数量. 这样你就能在不担心控制逻辑的情况下, 完成更多的并行工作. 这也是 GPU 与 CPU 的一个权衡. CPU 将更多的芯片面积用于控制逻辑、分支预测等方面, 而 GPU 则更侧重于计算, 控制逻辑相对简单.
说话人 1 06:56
好的, 现在我们开始讲一些新的内容. 我认为, 如果要记住一个高层次的要点, 那就是:如果你想编写高性能代码, 你应该记住对你的代码进行基准测试和性能分析. 这听起来显而易见, 但我见过很多情况, 比如学生或其他人上来就说:“我认为瓶颈在这里. ”然后他们会花三个小时去优化它, 结果发现那根本不是瓶颈.
说话人 1 07:21
我确信这个过程很有趣, 但这是一种时间的错配. 如果你使用高性能或非常详细的性能分析器, 你就能精确地看到瓶颈在哪里, 以及机器到底在做什么. 一旦掌握了这些信息, 你就可以把精力投入到代码执行中最关键的部分. 这就是我想要传达的核心思想. 因为关于 GPU 执行的一些细节, 比如如何编写一个 Softmax 核函数, 这些东西未来可能会改变.
说话人 1 07:48
甚至你可能只想依赖 torch.compile 这样的自动 JIT 工具. 但是, 无论工具如何变化, 你应该进行性能分析这一事实是不会改变的. 所以我希望你们能内化这个理念:如果你想编写高性能代码, 就应该时刻进行性能分析. 说真的, 理论是有极限的. 我认为系统是这门课程中你可以很好地进行推理的部分.
说话人 1 08:11
但架构在某种程度上是很难推理的. 你可以思考屋顶线模型(roofline model)等等, 但是你的矩阵乘法到底有多快？这可能取决于库的版本和你的硬件, 比如哪些部分因为什么原因成为了瓶颈. 还有各种你并不完全了解的微码细节. 所以, 最终在开发这些东西时, 你必须进行端到端的基准测试.
说话人 1 08:34
好的, 我将举一个计算示例. 这是我们能运行的最简单的东西, 与你们在第一次作业中做的所有事情相比. 我将运行一个非常简单的 MLP. 它的维度是 128, 有 16 层, 有一定的批量大小, 并且会运行五步.
说话人 1 08:51
我将在这里为五个不同的步骤执行前向和后向传播. 为了让代码更清晰, 它大概是这样的:我定义一个 MLP 模型, 稍后会展示给你们看. 然后我定义一个随机高斯输入. 之后我会运行五步.
说话人 1 09:07
在最后一步, 我进行一次前向计算, 然后一次后向计算, 并返回结果, 这个结果仅仅是我的 MLP 输出的均值. 这里甚至没有损失函数, 就这么简单. 只是运行 MLP 前向传播, 然后在最后进行平均池化.
说话人 1 09:22
而这个 MLP 也是你能想象到的最简单的结构. 它只是一堆线性层堆叠在一起, 就是这部分代码. 然后, 我在层与层之间加了一个 GELU 激活函数. 所以这只是 GELU、线性层、GELU 这样循环往复. 所有东西都是很规整的方形矩阵.
说话人 1 09:39
希望这是一个你们都非常熟悉的简单 MLP. 然后我们回到……是的. 哦, 抱歉. 我想回到这里. 好的.
说话人 1 09:53
现在我有了这个想要运行的 MLP 代码. 接下来我要做两件事. 首先是进行基准测试. 我会做一些计时. 我
说话人 1 10:00
想知道这个函数运行需要多长时间. 然后我会进行性能分析, 也就是深入函数内部, 探究我的时间都花在了哪里. 我们先从基准测试开始. 基准测试就是测量执行这些操作的墙上时钟时间(wall-clock time). 我只关注这个 MLP 函数的端到端执行时间.
说话人 1 10:23
这里面其实有一些微妙之处. 你可能会坐在那里想, 为什么我要被教如何调用一个计时函数, 但你确实需要对如何测量时间持谨慎态度. 我认为, 如果你不注意, 在做第二次作业时就会遇到这些陷阱. 我们为什么要这么做呢？我们稍后要比较不同的实现. 我们要比较我们自己用 Triton 写的、用 C++ 手写的、PyTorch 自带的实现以及 torch.compile 的实现.
说话人 1 10:50
我们想知道, 花时间写那个 CUDA 核函数到底值不值. 我们还想了解, 当我增大矩阵乘法的规模时, 速度会变慢多少. 所以我们想对它们进行一些经验性的基准测试.
说话人 1 11:00
在整个讲座中, 我都会使用这个 `benchmark` 函数. 它会是一个包装函数, 我会逐步讲解它. `benchmark` 函数会做以下几件事:它接受一个我想要测试的函数, 也就是 `run`.
说话人 1 11:13
然后, 我会进行一些预热迭代, 之后再进行若干次试验. 你可能会好奇, 这个预热是做什么的？有一件非常重要的事情是, 当你第一次运行 PyTorch 代码, 比如说它向 GPU 分发了一个任务. 这个过程对你来说可能看起来非常快且透明, 但在后台, 当某个东西第一次被执行时, 机器码正在被编译. 那条代码指令可能正在被发送到 GPU. 为了初始化你的代码, 会发生各种各样的事情.
说话人 1 11:46
所以, 你总是需要进行一些预热迭代, 以确保你测量的不是启动速度, 而是稳态速度. 如果你要运行成千上万次迭代, 你关心的是那部分的速度, 而不是你的 CUDA 代码即时编译的速度有多快. 这就是为什么我们需要预热, 你也应该总进行一些预热.
说话人 1 12:11
另一件非常重要的事情是, 等我们讲到性能分析器时会再提到, 你需要调用一个叫做 `torch.cuda.synchronize` 的函数. 这是什么呢？GPU 和 CPU 基本上是你电脑里两个独立的计算单元. 它们基本上可以独立运行. 所以, 执行模型是这样的:我这里的 Python 代码
说话人 1 12:33
是运行在 CPU 上的. 当我运行某个东西时, 它会向 GPU 分发一堆 CUDA 核函数, 告诉 GPU:“请为我运行这些东西. ”GPU 会去执行那些任务, 而 CPU 实际上会继续运行, 它不会等待那些 CUDA 执行结束.
说话人 1 12:49
这对于编写高性能代码来说是很好的, 但如果你想做基准测试, 你应该能立刻看到问题所在. 如果你在进行基准测试, 而你的模型中 GPU 在一旁运行, CPU 却在做别的事情, 那么你实际上测量的并不是 GPU 的执行时间. 所以 `torch.cuda.synchronize` 的作用就是, 确保 GPU 和 CPU 处于相同的状态, 没有正在排队的任务在运行, 并且我们在代码执行的进度上处于同一点. 现在, GPU 和 CPU 的状态同步了, 我就可以真正开始计时了.
说话人 1 13:22
我会对某个操作计时若干次, 然后运行计算, 在这个例子里是 `sleep` 命令. 我会执行三次. 因为我试图让程序休眠 50 毫秒, 所以最终得到的时间就是这个值. 我用 `time.time` 计时了三次. 当然, 在这里, 我同样在 `run` 函数的末尾调用了 `torch.cuda.synchronize`, 以确保 GPU 和 CPU 的状态是一致的.
说话人 1 13:47
所以, 如果 CPU 运行得太快, 它会在这里等待 GPU 的执行实际完成, 反之亦然. 现在, 我已经完成了计时, 然后我会取平均值, 因为单次测量可能会因为 GPU 的热性能等因素而波动. 所以你需要进行多次重复测量, 取平均值并返回. 这就是我们的基准测试代码, 非常简单.
说话人 1 14:09
但请记住其中两个重要的部分:始终进行预热, 并且确保调用 `cuda.synchronize`. 如果你做到了这些, 事情就很简单. 如果忘了做, 你会得到非常离谱的数字.
说话人 1 14:20
比如, 你会发现你的大型矩阵乘法瞬间就完成了, 这绝对是不可能的. 好的, 现在我们可以对矩阵乘法进行一些基准测试了. 我会带大家看一些例子, 它们只是把我们已知的一些东西用数字呈现出来, 但我想过一遍, 确保我们达成共识.
说话人 1 14:37
我是在课程提供的 H100 GPU 上运行的. 我将对这些尺寸的矩阵进行乘法运算, 然后我会收集这些不同维度的矩阵乘法计时数据, 并逐步分析这个基准测试的结果. 我们看到的结果, 正如我们所预期的那样.
说话人 1 15:00
随着矩阵尺寸的增加, 我们的运行时间呈现超线性增长. 当然, 在像 1024 和 2048 这样较小的尺寸下, 我们发现时间几乎没有增长, 因为执行这些矩阵乘法存在常数级别的开销, 比如这些数据需要从 CPU 传输到 GPU, 还有启动核函数的开销等等. 所以, 情况并非是一直超线性增长到零. 但一旦矩阵变得足够大, 我们就能看到与矩阵乘法完全符合预期的那种性能扩展趋势. 好的, 希望这部分内容很直白.
说话人 1 15:35
现在我们来对我们的 MLP 进行基准测试. 我们要怎么做呢？我们会把 MLP 变大. 我们将使用 256 的维度, 4 个层, 批量大小为 256, 并执行两步.
说话人 1 15:46
那么, 完成这个任务需要多长时间呢？嗯, 这需要 6.2 秒. 现在我可以做一些基本的操作. 我可以将步数从 2 增加到 5, 然后对所有这些情况进行基准测试, 我会得到 2、3、4 和 5 步的结果. 与矩阵乘法的情况不同, 如果我增加步数, 也就是增加 MLP 的前向和后向传播次数,
说话人 1 16:13
我期望运行时间会如何变化？嗯, 我期望是线性增长, 对吧？而这基本上就是我们看到的结果. 每次 MLP 的执行大约需要 5 秒, 所以总运行时间大约是 n 乘以 5. 好的, 我看看是否能重置一下.
说话人 1 16:33
我想这个正在被监控. 哦不, 我不能. 好吧, 我稍微缩小一点. 抱歉. 好的, 我们现在也可以将层数从 2、3、4 增加到 5. 这会给我们带来什么结果呢？
说话人 1 16:45
结果是, 运行时间再一次随着层数的增加而线性增长. 这一次, 同样地, 每层大约需要 5 秒, 甚至比那还要少一点. 所以我们得到了大约 4 倍的层数, 并且线性增长的趋势再次出现, 这并不奇怪. 所以, 步数和层数都与运行时间有着明显的线性关系, 这正是我们最终在这里看到的结果.
说话人 1 17:13
关于批量大小的部分我就跳过了, 因为被追踪的东西太多, 变得有点难以处理了. 好的, 基准测试这部分就到此结束. 我们可以编写一个不错的函数, 它能做一些预热和 `cuda.synchronize`, 然后我们就可以测量任何我们想要测量的东西的运行时间. 这很好, 你们应该在自己的代码中经常这样做.
说话人 1 17:33
你可以测量你的新潮架构运行需要多长时间. 但是, 我认为如果你想解决一些问题, 基准测试是一个非常粗粒度的工具. 它告诉你你的代码很慢, 但它不会告诉你时间都花在了哪里. 所以, 我们接下来想做的是性能分析.
说话人 1 17:53
这将是一个我们想要进行的更细粒度的分析. 性能分析非常好, 因为它不仅能帮助你看到时间花在了哪里, 花在了哪些函数上, 而且, 当你观察你所调用的东西时, 你通常是与 PyTorch 的接口交互, 对吧？就是你调用的那些 PyTorch 的部分. 但在 PyTorch 的底层, 有整个 CUDA 的世界正在被调用. 当你运行一个性能分析器时, 你实际上可以看到一直到底层调用的所有细节, 看到底调用了什么. 这样你就能对程序在硬件上实际是如何执行的有一个更清晰的直观认识.
说话人 1 18:28
所以, 我们将逐步对几个简单的函数进行性能分析, 然后对发生的事情获得一些直观的理解. 有一个很好的地方是, 如果你想要基本的性能分析, PyTorch 有一个非常好用的内置性能分析器. 这可以让你在不离开 Python 和 PyTorch 的世界的情况下, 获得一些相当不错的输出结果. 我在这里对一些函数进行了性能分析, 你也可以看到它的输出结果. 所以, 我用了之前那个 `sleep` 的例子.
说话人 1 19:02
这里是 `sleep` 函数. 当我们对 `sleep` 函数进行性能分析时, 这个性能分析函数看起来是这样的.
说话人 1 19:09
我同样设置了预热, 调用了 `torch.cuda.synchronize`, 然后我调用性能分析器, 同时追踪 CPU 和 GPU 的时间. 然后, 我运行一些东西, 再次同步, 并打印出所有时间的平均表格. 好的, 我回退一下. 现在我将对 `sleep` 函数进行性能分析. 如果我们看一下发生了什么, 这里发生了什么呢？嗯, 百分之百的时间都花在了一个叫做 `cudaDeviceSynchronize` 的东西上, 因为没有任何 GPU 工作在进行. 这只是一个空操作, 对它进行性能分析有点傻.
说话人 1 19:45
现在, 我们来看一些不那么微不足道的东西. 让我们看一下这个将两个矩阵相加的基本操作. 我定义了一个 `add` 函数, 它接受 a 和 B 并将它们相加. 还有一个辅助函数, 它会实例化两个随机高斯矩阵, 然后调用 `operation` 参数中指定的任何操作. 这里是把两个 2048 大小的矩阵加在一起.
说话人 1 20:11
好的, 现在我将对此进行性能分析. 我会调用性能分析器, 然后会得到一个看起来像这边这个代码块的东西. 这就是我得到的结果. 我需要再次缩小, 因为这……好的. 好的. 后面能看清吗？
说话人 1 20:28
如果后面看得清, 谁能给我竖个大拇指. 好的. 看不清的话就竖个倒拇指. 好的. 所以, 当我们在 Python 中调用 `add` 函数时, 我们交互的基本上就是 `add` 函数 A+B, 对吧？我们只考虑这个.
说话人 1 20:43
但实际上, 在冰山之下, 发生的事情远不止这些. 这个操作被分发到 GPU. 首先, 有一个叫做 aten 的东西, 它是 PyTorch 的 C 语言接口. 所以这个包装器被调用, 它说:“好的, 我要加一些数字. ”这就是被调用的东西.
说话人 1 21:01
这是外层包装器. 然后它会分发到一个特定的核函数, 叫做 `vectorized_elementwise_kernel`, 后面跟着 `at::native::cuda_function_add`. 这才是真正进行加法运算的东西. 此外, 还有一个叫做 `cudaLaunchKernel` 的东西也花了一些时间.
说话人 1 21:19
这实际上是 CPU 接收命令并将其发送到 GPU 的过程, 也就是核函数启动. 这需要一些时间. 最后, `cudaDeviceSynchronize`, 我们等待 GPU 完成并把结果发回给我们. 这同样需要时间.
说话人 1 21:33
仅仅是设置一个同步屏障就会花费我们一些时间. 所以我们最终得到的总时间是, CPU 上 1.4 毫秒, CUDA 上 17 微秒. 所以, 这个操作在 GPU 上非常快, 在 CPU 上则较慢. 如果我们观察 CPU 花费的时间, 也就是 `self_cpu_time`, 我们会发现 C++ 接口或 C 语言接口实际上是花费我们大量 CPU 时间的地方, 并且将数据发送到 GPU 的任何操作都存在开销. 这就是 `add` 函数的情况, 我们看到了底层发生了什么.
说话人 1 22:13
这里的故事是一样的. 如果我想做一个矩阵乘法, 所以我正在做 A 乘以 B. 这是一个 A 和 B 的矩阵乘法.
说话人 1 22:19
我再次使用了 2048 大小的矩阵. 然后我进行性能分析. 这次我看到了 `aten::matmul`. 这表示这是执行矩阵乘法的更底层的接口. 它会分发给 Cutlass, 这是 NVIDIA 的高性能矩阵乘法 CUDA 库. 然后它会分发到一个非常特定的 Cutlass 核函数, 这个核函数会有一些 tile size. 这里的名字被截断了, 我稍后会给你们看一个更详细的版本. 这基本上指向了一组非常特定的参数, 比如 tile size 和块的数量等等. 所以这个东西是参数化的, 它才是真正执行矩阵乘法的.
说话人 1 23:00
我们再次在底部看到了同样的两项:核函数启动(kernel launch)和 CUDA 设备同步. 你同样可以看到 CPU 时间和 CUDA 时间的分配. 我们花在 CUDA 上的时间要多得多, 因为矩阵乘法确实比仅仅相加两个向量要耗时.
说话人 1 23:17
好的, 到目前为止有什么问题吗？我可以暂停一下. 我觉得我刚刚在讲解性能分析器时讲得非常快, 有点自说自话. 所以如果有人有问题, 我可以停一下. 如果没有, 我就继续了. 好的. 哦, 是的, 在这种情况下.
说话人 2 23:35
这个大于我们的CPU, 但是我们确实有一个屏障, 比如发送到CPU工具进行信用同步. 所以这样的话, 接收你的时间不应该总是至少是相同的量吗？
说话人 1 23:51
是的, 我不认为这个计算是至关重要的. 酷. 哦, 是的, 抱歉, 有两个问题. 有什么特别的吗？
说话人 2 23:59
为什么, 比如当我们从加法切换到手动机器时, 它会下降.
说话人 1 24:04
当我们从加法切换到 `matmul` 时, CPU 时间下降了, 有什么原因吗？坦白说, 我也不确定. 是的.
说话人 2 24:14
这背后的故事.
说话人 1 24:17
与提供它相比, 性能分析器中是否存在可能扭曲结果的开销, 与在现实世界中运行相比？是的, 性能分析器中存在开销, 像同步屏障就会造成这种情况. 我会给你们展示一个来自 NVIDIA 的更高级的性能分析器, 你可以添加像注释这样的东西, 这也会稍微扭曲计时, 但不会太多. 你看到的那些大规模的时间差异并不会真的被性能分析器扭曲. 所以, 如果你关注的是微观层面的计时, 是的, 可能会有影响. 但对于我们在课堂上关心的大部分事情来说, 不会.
说话人 2 24:50
是的, 只是为了确保事情正确. 那么, 对于加法的情况, 是CPU的想法吗？
说话人 1 24:59
不, 我. 伙计们, 对, 毫秒时间. 不, 没错. 所以这是时间的百分比, 正如你所看到的, `aten::add` 实际在海上某个地方执行的毫秒时间.
说话人 1 25:16
我认为这并不容易. 我的百分比是 CPU 的什么. 那将会卖掉. 是的, 对. 这是 CPU 活跃的百分比, 而不是利用率百分比. 如果是这样的话, 是的, 所以这不像是 CPU 的总浮点运算次数之类的. 这是 CPU 在做某件事的总时间百分比. 是的. 好的, 酷.
说话人 1 25:37
好的, 这是 `matmul` 的另一个例子. 这是一个不同的维度. 我正在乘一个 128 维的矩阵, 所以是 128x128, 尺寸小得多. 你会发现, 现在它实际上直接执行了一个不同的命令, 即 `gemm`, 这是一种矩阵乘法类型, 并且是 float32 格式. 从这个核函数的命名中, 你大概能看出这里实际发生了什么, 这是一个某种形式的分块矩阵乘法(tiled matrix multiply). 它没有通过 Cutlass, 而是直接执行了这个特定的命令. 所以对于小矩阵乘法, 你会看到它分发到了一个不同的核函数. 因此, 当我们在这个高层抽象上操作时, 你就能看到矩阵乘法的复杂性. 我们只是把矩阵乘法看作一个单一的东西, 我们调用 `A @ B` 就完事了.
说话人 1 26:35
但在底层, 根据你使用的维度和你拥有的硬件, 它实际上会分发到非常不同的矩阵乘法原语. 这实际上会表现出非常不同的性能特征. 所以一个有趣的小提示是, `torch.compile`, 我稍后会讲到, 它实际上有一个选项可以对你硬件上的矩阵乘法性能进行微基准测试. 然后它会为你的模型选择性能最高的矩阵乘法子程序, 我过去发现, 这能免费给你带来大约 10% 的速度提升. 通过优化这些东西就能在现实世界中获得免费的收益, 这非常酷. 好的, 那是另一个 `matmul` 的例子. 与纯粹的基准测试相比, 性能分析器的酷炫之处在于, 我们现在能看到哪些 CUDA 核函数被调用了. 我们能看到, 不同尺寸的矩阵会导致不同的 CUDA 核函数被调用. 我们还看到了, `cutlass_80_sm80_sgemm`, 这是一个 Cutlass 线性代数库. 它告诉了我们一些信息, 比如 tile size.
说话人 1 27:47
到目前为止, 这些操作在某种程度上都非常无聊, 比如矩阵乘法和加法. 它们基本上是一对一的. 你在 CPU 端有一个操作, 它被转换成一个 GPU 操作, 然后就被传送过去. 所以在所有这些操作中, 只有一个操作在 GPU 上做事情. 所以我想看一些更复杂的操作, 另外两个具有更复合行为的操作. 我现在想做的是, 我想看看这个叫做 `torch.cdist` 的操作. 这是用来计算两组矩阵中向量之间的成对欧几里得距离的. 所以这将是一个我想要的在 A 和 B 之间的大型距离矩阵计算. 这就是 `cdist`. 这显然是一个更复杂的操作. 如果你想计算欧几里得距离, 你需要计算点积, 你需要计算平方根. 我们将会看到, 一旦我们计算 `cdist`. 现在这里是 `cdist` 的性能分析输出. 我们看到这个 `torch` Python 命令在 C 语言接口中确实映射到了某个更底层的 `cdist`. 所以这是 `aten::cdist`, 然后映射到 `aten::euclidean_dist`. 然后这会分解成一大堆东西, 比如 `aten::matmul`、`aten::pow`, 还有 `sum`, 因为这些都是你为了实际计算所有向量之间的欧几里得距离所需要的基本操作. 当你对每一个这样的操作, 比如矩阵乘法、拼接和求幂, 都有一个对应的 CUDA 命令在这里被调用. 我们有我们熟悉的 `gemm`.
说话人 1 29:33
这是一个矩阵乘法. 它占用了我们 GPU 上 78% 的计算时间. 我们还有数组的复制和拼接, 这占用了 6% 的执行时间. 然后是这个向量化的逐元素核函数, 它负责求幂, 占用了 5% 的 GPU 时间, 还有 3% 用于求和. 所以现在我们得到了一个非常好的、底层的分解图, 显示了我的 GPU 时间都花在了哪里. 从这个图中, 我大概能知道我应该把优化的时间花在哪里. 比如, 我觉得我可以优化我的矩阵乘法, 那会很棒, 因为它占了 GPU 70% 以上的时间. 最后一个例子, 抱歉是最后两个例子, 我想谈的是 GELU 和 Softmax. 这些将是我们的……哦抱歉, 有个问题. 好的, 我大概几分钟后回答那个问题, 因为有一个更酷的性能分析器, 能展示一个更漂亮的图景. 所以, 我可以在这里比划, 但我觉得用图来展示会更好. 好的, 我现在要讲一下 GELU 和 Softmax. GELU 将是我们整个课程中反复使用的例子. 这是一种非线性激活函数. 如果你还记得, 它是高斯误差线性单元(Gaussian Error Linear Unit). 它是一个 `tanh` 函数和一个指数函数的乘积, 如果我没记错的话. 所以我们会进行各种操作. 我们会把 A 和 B 相加, 然后调用 GELU, 模拟我们 MLP 中可能有的线性加非线性的结构. 我们再次看到了基本相同的映射关系. 我们看到 `aten::add` 对应于 `A + B`. 然后我们有等效的 CUDA 实现. 再往下, 我们实际上有一个用 CUDA 实现的 GELU 函数, 就在这里. 它大约占了 33% 的计算量. 好的, 相当合理. 然后我们还有 Softmax, 我不会详细讲解所有这些, 因为看多了之后它们都开始变得一样了. 但真正值得指出并且我觉得很酷的一点是, 很多像 Softmax 和 GELU 这样的核心原语, 都有专门为它们编写的核函数. 所以 GPU 不是在执行基本原语, 而是有一个融合了所有这些操作的算子. 所以 CPU 和 GPU 之间没有来回的交互. 好的, 我之前提到过我会回答那个关于 CPU 在做什么的问题. 现在我们来思考一些更复杂的事情. 我一开始用于基准测试的那个 MLP 例子, 假设我想优化那个 MLP, 让它运行得非常快. 我们该怎么做呢？理想情况下, 我们会用一种很好的、细粒度的方式来对它进行性能分析. 如果我们使用 torch 的性能分析器, 我们大概会得到这样的结果. 如果你还记得那个 MLP, 它有一堆堆叠的线性层, 有前向和后向传播. 你大概能看到, 这里有后向传播在发生. 有矩阵乘法, 有线性层, 然后是后向传播的 `accumulate_grad` 操作. 这里是矩阵乘法的核函数, 然后这里只能显示 10 项, 所以我觉得到某个地方就被截断了. 但这挺好的. 它确实告诉你大部分时间都花在了 `matmul` 上. 但你可能会好奇, 剩下的时间都去哪了？为什么这里只有 31% 的时间？那 60% 去哪了？是 `aten::mm`, 但没有对应的核函数. 这就有点神秘了. 对于一个非常复杂的模块来说, 这不是一个很好的可视化. 所以, 为此, 我想我们必须拿出一个真正的、更专业的性能分析器. 你们也需要, 或者说我们会要求你们去看这个东西, 就是 NVIDIA 的 Insight Systems. 这是 NVIDIA 用来观察 GPU 行为和性能的一种详细方式. 我们将会精确地看到, 当我们运行这个 MLP 时, 到底发生了什么. 后面的同学, 你们能看到吗？我不知道, 这里这些微小的文字？竖个大拇指. 好的. 好的. 如果你们能看到, 那我就不放大了, 但即使从这里看也觉得很小. 好的, 基本上, 我们在这里看到几个不同的东西. 我们看到这边有 CUDA HW, 然后我们看到有 threads. 所以上面这半部分, 这个 CUDA 部分, 是 GPU 大概在做的事情. 然后在这个 threads 部分, 我们看到 CPU 大概在做的事情. 我也可以把代码调出来. 我想, 是的, 这里的代码. 当我进行性能分析时, 我加了几个注释. 好的, 这个我肯定需要放大了. 让我们……很好. 好的. 所以我用这组东西给代码加了注释, 它说, 哦, 看, nvtx, 这基本上是用标记来注释我的代码. 所以当性能分析器进来时, 它就会知道这部分代码属于一个叫做 `define_model` 的块. 举个例子, 这部分代码, `step.range_push` 和 `range_pop`, 从第 77 行到第 55 行的这个范围, 应该被注释为 `step_` 加上步骤号. 好的, 在调用我的性能分析器之前, 我在代码中加入了所有这些注释. 现在我们回到这里. 现在如果我们去看这个标有 `nvtx` 的行, 我们能看到 `define_model`, 这是我用来包装模型构建调用的东西. 然后我看到 `step 0`, `step 1`, `step 2`, `step 3`, `step 4`, `step 5`. 所以每个步骤现在都在这个性能分析器中被很好地注释出来了. 我们可以看到模型在运行过程中做的所有事情. 我从这边开始. 我们看到一件事, 就是这部分代码其实没做什么工作. 它只花了 14 秒. 所以实际上性能分析器的大部分时间都花在了开销上. 所以直到大概这里的部分, 都是一些像加载库之类的事情, 这花了很长时间. 显然, 仅仅是初始化所有东西就花了 7.5 秒. 然后, 至少在 GPU 上, 程序运行到 7.5 秒左右时, 它开始真正地构建模型. 你可以在内存占用图上看到, 这里就是内存开始被分配的地方. 在 GPU 内存上, 内存使用量开始增长. 好的, 现在模型构建完成了. 然后 `step 0` 是真正开始有动作的地方. 你之前问到 CPU 和 GPU 之间发生了什么. 这个执行模型是这样的:这里是 CPU 上的 `step 0`, 我从这里开始. 这是前向传播. 这是 `layer 0`. 让我们来理一下发生了什么. 正如我之前所说, 当你第一次遇到或者说第一次在 PyTorch 中调用一段代码时, 它并不会直接执行. 它实际上会做一些事情, 比如即时编译. 所以, 像 `runtime_triggered_module_loading` 这样的东西, 是为了初始化层和计算, 以及将各种代码片段移动到 GPU 而做的开销工作. 这花了很长时间. 然后, 在 `layer 0` 完成之后. 现在如果我看一下这里的任何一个切片, 让我们放大到选区, 我们会看到每个层都非常快. 这里发生了什么呢？当我高亮显示 CPU 端的 `layer 1` 时, 注意到 GPU 端的 `layer 1` 并不在同一个位置, 对吧？正如我之前所说, CPU 和 GPU 是两种不同的执行设备. 我从 `layer 0` 开始. 我完成了 `layer 0`. 我开始 `layer 1`. 现在, CPU 实际上只是在发送所有的 CUDA 命令, 所有的 CUDA 核函数. 它已经在这个时间点把所有的 CUDA 核函数都启动并发送到 GPU 了. 所以当 CPU 说它在做 `layer 1` 的时候, 它实际上是在把命令排队放入 GPU. 它说:“接下来运行这个. ”所以 CPU 的运行远超前于 GPU. 到 `layer 1` 开始在 GPU 上执行的时候, CPU 实际上已经到 `layer 9` 了. CPU 运行得非常快, 并且 CPU 基本上维护着一个队列, 它向 GPU 发送固定数量的 CUDA 核函数. 一旦你达到了那个队列深度, 它就会停止超前运行. 但在此之前, 它会尽可能地一直往前跑. 在这种情况下, 这确实变得, 我再缩小一点. 撤销缩放. 好了. 在这种情况下, 这有点极端, 因为如果我再缩小一次, 你会注意到, 在这些步骤中, 我运行得非常超前. `step 0` 在这里, `step 2` 在这里. 这是 `step 1`, 基本上没花什么时间. `step 2` 在这里. 所以 CPU 基本上比 GPU 领先了整整一个前向和后向传播的步骤. 一个你可能会做的有趣的事情是, 如果你正在为训练语言模型编写各种代码, 一件很平常的事可能是在迭代之间打印损失. 这看起来似乎不应该对 GPU 的工作产生任何影响, 对吧？你可能会想:“哦, 这只是一个打印语句, 能有多大影响？”但如果你仔细想一下, 这会对 GPU 的执行布局产生巨大影响. 因为为了打印这个语句, 这个打印语句是在 CPU 上发生的, 而 CPU 需要获取到损失值. 这意味着它需要等待 GPU 计算出那个损失. 所以让我们看看会发生什么. 在这里, 正如我所说, CPU 上的 `step 4` 远早于 GPU 上的对应步骤. 现在我们切换回来. 这是我进行性能分析的带有打印语句的版本. 现在我放大到这个选区. 现在看看 `step 1` 和 `step 2` 是如何基本同步的, 对吧？因为我必须等待损失被计算出来. 你看着这个可能会说:“哦, 但还是有点偏移, `step 2` 和 `step 1` 并没有完全对齐. ”那我们再放大看看, 好的, CPU 上的 `step 1` 发生了什么？嗯, 基本上 CPU 上 `step 1` 的终点也大概是优化器步骤开始的地方. 所以到前向传播完成时, 抱歉, 这个 `cudaStreamSynchronize` 会同步. CPU 上的这个 `cudaStreamSynchronize` 命令, 基本上是在说:“我只是在等 GPU, 因为我不能超前运行了. 我在等这个损失被计算出来并传回给我. ”所以这算是一个空操作, 它在说 CPU 等待. 后向传播步骤完成了. 现在我可以打印损失了. 我打印了损失. 好的, 现在 CPU 可以开始超前运行了. 它确实超前了, 开始发送 `step 2` 的东西. 然后, 一旦到这里, 它的命令就用完了. 它又在等损失. `cuda.synchronize` 等待. 后向传播步骤完成. 现在我可以打印损失了. 现在我又开始超前运行了. 所以在这种情况下, GPU 在两种情况下都基本保持了满载利用率. 但在极端情况下, 比如说你一直不停地打印大量东西, 你实际上会引入一个 CPU 瓶颈. 因为 CPU 必须一直等待 GPU, 它无法提前启动核函数. 所以这是你能通过性能分析器看到的非常酷的一点, 关于 CPU 和 GPU 的对比. 它们实际上是相互通信的不同设备, 而不是一个单一的统一体. 除非你开始看一些像这样更高级的性能分析器, 否则你是看不到这一点的. 关于这组事情有什么问题吗？酷. 好的. 我想展示的另一件事是, 我之前玩的那个性能分析器. 你也可以在 NSight Systems (NSYS) 中生成非常相似的视图, 你可以在其中选择某个范围的东西. 让我们做一次预热. 我说过我们应该这样做, 所以我们应该排除前几个步骤. 我们将从第 3 步开始, 然后测量这个范围内的几个步骤. 我们可以看到这些核函数, 这才是真正在做计算的. 你可以看到, 实际上有许多不同种类的矩阵乘法. 这是一个矩阵乘法核函数. 这是另一个不同的矩阵乘法核函数. 还有一个不同的, 比如向量化元素核函数. 所有这些都占用了不同的计算量. 我们可以选择这个, 然后说:“哦, 在事件视图中显示所有正在发生的事情. ”我还可以看到统计视图, 看到它花费的所有时间. 等等, 让我们看看. 我们想要的是平均时间. 抱歉, 是 CUDA 核函数执行摘要. 是的, 我们想要核函数的总持续时间. 所以我们可以看到哪些核函数在这些视图中总共花费了最多的时间. 所以这实际上是一个非常强大的工具, 它可以给你一个关于什么是慢、什么是快的聚合视图, 也可以给你看单个核函数是如何被启动的、何时被启动的, 以及对应的 CPU 命令来自哪里. 我猜最后一个旁注是, 这也是为什么我们用 Python 编程, 而 Python 并不是一种高性能语言, 但却没关系的原因之一. 因为 CPU 从来都不是瓶颈, CPU 可以提前运行, 并将命令排队放入 GPU. 所以 GPU 和 CPU 之间的这种解耦或者说分离, 是我们能够使用这种优秀的高级编程语言, 却仍然能充分利用 GPU 的关键原因之一. 酷. 好的. 在我切换回去之前有什么问题吗？因为在这次讲座中, 我将永远离开 NSight Systems 了. 酷. 是的, 不过你们在第二次作业中会用到它. 我想你们会喜欢它的, 因为它能让你以一种非常有趣的方式看到你的硬件为了训练这些语言模型到底在做什么. 好的, 以上是关于基准测试和性能分析的内容. 现在你们拥有了进行性能相关工作所需的所有工具. 在剩下的时间里, 我们将编写一些核函数. 还记得核函数融合(kernel fusion)吗？这是我在讲座中给你们看的图. 这里有一个小工厂. 每次我需要做一个操作, 我就得把东西从仓库运到工厂, 然后再运回来. 所以, 如果我天真地按顺序执行一堆操作而不去思考, 我就会为来回运输付出很多成本. 我应该做的是, 拥有一个能一次性完成所有操作的工厂, 这样我就不用多次支付这个成本了. 这非常重要. 现在我们来处理 GELU, 我们会为 GELU 编写一个核函数. 我会用几种不同的方式来编写这个核函数, 然后我们会看看这样做的性能影响. 我们有 PyTorch 实现的 GELU, 它看起来就是这样 `torch.nn.functional.gelu`. 我调用时设置了 `approximate='tanh'`, 因为我希望它和我接下来要做的朴素实现完全匹配. 所以这不会是真正地乘以高斯分布的累积分布函数(CDF), 而是对它的一种更容易计算的近似. 好的, 这是 PyTorch 的 GELU. 现在我来做一件蠢事. 你们看到这个代码会说, 这性能肯定很低. 我将在 PyTorch 中这样实现 GELU:`0.5 * x * (1 + tanh(sqrt(pi/2) * (x + 0.044715 * x^3)))`. 一个神奇的公式. 但这是对 GELU 的一个很好的近似. 你们可以查一下或者自己验证一下这是对的. 但如果你这样做, 你会看到发生了很多操作, 对吧？有 `tanh`, 有 `x` 的三次方, 有乘以一个常数再相加, 还有乘以 0.5 和 x. 如果这涉及到多个不同的 CUDA 核函数, 那它可能会很慢, 对吧？从融合的角度来看, 这应该是我们的直觉. 那我们来看看是不是真的这样. 好的, 这两个是相同的. 你可以在左上角看到, 它们计算出的数字完全一样. 我们可以在随机高斯输入上系统地检查这一点. 现在, 我们来对这两个进行基准测试. 好的, 对于一个非常大的 GELU 计算, 手动实现的时间是 8.1 毫秒. 而 PyTorch 的时间是 1.1 毫秒. 融合后的版本要快得多. 事实上, 快了 8 倍. 哇, 编写一个简单的核函数竟然有这么大的差别. 当然, 你的矩阵乘法可能仍然是瓶颈. 但如果我们能从 8 毫秒优化到 1 毫秒, 那会非常酷, 对吧？那会让人很有成就感. 所以, 在讲座的接下来几部分, 我们会尝试接近那 1.1 毫秒. 现在我们来看看底层发生了什么. 我不需要看 NSight Systems, 因为我真正想知道的只是一些非常高层次的东西. 对于手动实现的 GELU, 就像我说的, 它会做一大堆操作. 它会做一堆乘法. 虽然是向量化的, 但这里启动了一堆 CUDA 核函数. 注意右边, 这个 CUDA 核函数被调用了三次, 因为我们这里有一大堆乘法运算.
说话人 1 47:23
我们还有加法, 还有 `tanh`. 其中每一个都可能比较慢. 最终, 这样做会产生相当大的开销.
说话人 1 47:34
现在我们对 PyTorch 的 GELU 做同样的事情. 这真是太棒了. 只有一个 CUDA 核函数被启动. 它只发生一次, 就处理了所有的事情. 这是我们希望看到的. 当然, 这非常快, 因为它只是一个单独的 CUDA 核函数. 所以这非常好. 我们希望能够以某种方式实现这个 CUDA 核函数. 所以, 根据你对编写高效 GPU 代码的了解程度, 你可能首先想到的是, 好吧, PyTorch 的开发者们肯定是用尽可能底层的语言来编写这个的. 所以我们也要做同样的事情. 我们将使用 C++ API, 虽然不是最低级的, 但我们将在 C++ 中编写 CUDA 核函数. 所以, 让我们打开它, 编写我们自己的 CUDA 核函数. 那要怎么做呢？好的, 我们已经进去并创建了整个东西的 C++ 版本. CUDA, 当我们说 CUDA 时, 实际上是指用于与 GPU 交互和编程的 C++ API. 就像我们描述的 GPU 的逻辑模型一样, 我们要编写某个函数 F. 然后当我们调用这个 CUDA 核函数时, 它会自动在向量或矩阵的所有元素上调用 F. 然后我们就可以并行计算我们想要的一切. 按照命名法, 我们会有一个网格(grid), 它是一组线程块(thread blocks)的集合. 可以这样想, 我有一个任务, 我把它切成小块, 就会有若干个块. 比如在二维网格中, 会有一个行坐标和一个列坐标. 这在你处理矩阵时非常有用. 然后会有每个块的大小, 比如这些块在线程块数量方面的尺寸. 这是块的维度. 然后在这些块内部有一组线程. 这是, 例如, 一个线程块所在的坐标. 然后每个线程都在每个块内部. 所以这里有一个层级结构. 有一个网格, 网格内部有线程. 基本上每个函数都会接收三个东西:块索引, 也就是我属于哪个线程块; 块的维度; 以及我的索引, 也就是我的线程索引. 有了这些, 我就能知道我在矩阵或向量中的坐标, 然后我就可以决定我想要的逻辑.
说话人 1 50:05
在看实际的 C++ 代码之前, 最后一件事是, 无论何时你尝试调试 CUDA, 你都应该使用 `CUDA_LAUNCH_BLOCKING=1` 来启动. 这能让你实际调试你的 CUDA 核函数. 它会返回错误信息, 但会牺牲一些运行时间. 如果你不这样做, 在编写需要调试的 CUDA 代码时, 你的体验会很糟糕.
说话人 1 50:31
好的, 这是我的 GELU 代码, 我们来逐段看一下, 然后我会解释所有部分的作用. 这部分可能会比我们接下来要看的其他部分花更长的时间, 除了机器码. 一旦你理解了这个, 你应该就能理解所有其他部分了. 所以我们会稍微慢一点地过这部分. 这段代码有两部分. 第一部分, 上面这个 `GeluKernel`, 是实际的核函数. 它负责计算. 它会被发送到 GPU, 进行计算, 然后返回结果.
说话人 1 51:06
这部分, 下面的 `gelu` 函数, 是一个包装器. 它运行在 CPU 上. 它负责协调核函数的启动, 而核函数将实际存在于 GPU 中. 所以, 我们或许可以从这个包装器部分, 也就是 `gelu` 函数开始. 我们基本上总是在 Triton 或 CUDA 代码中检查两件事. 我们总是会检查, 哦, 抱歉, 后面有个问题. 好的, 抱歉, 是我的错. 好的, 这是一个容易解决的问题, 但我需要知道你们看不见. 好的, 很好. 是吗？好的, 太好了. 好的, 我们从 `gelu` 函数开始. 有两件事我们总是需要做的. 第一是确保 X 存在于 GPU 设备上, 是某种 CUDA 张量. 如果不是, 那就会有问题, 我们将无法在 GPU 上做任何事情. 第二件事, 可能不那么明显, 是我们要检查以确保 X 是连续的(contiguous). 这意味着它存在于一块连续的内存中, 因为当我们索引 X 时, 我们会做大量的索引算术, 我们会假设 X 存在于一块内存中. 如果不是, 那么要以任何通用的方式来做这件事基本上是不可能的.
说话人 1 52:26
所以当我们计算 GELU 时, 我们输入一个 X, 然后会输出一个 Y. 所以我们需要分配输出. `torch::Tensor y = torch::empty_like(x);` 这只是说, 给我一个输出张量的空间, 或者一个指向输出张量的指针, 它的维度和 X 一样.
说话人 1 52:48
注意我没有调用 `zeros`. 这可以节省额外的操作. 我不需要把这些 Y 清零, 因为我反正要往里面写数据. 这是一个小优化, 但你不妨这么做. 然后, 基本上在我们写的所有代码中, 我们都需要确定网格(grid)的配置.
说话人 1 53:06
总共有多少个元素？每个块的大小是多少？每个块里有多少线程？总共有多少个块？
说话人 1 53:16
当需要计算块的数量时, 我会调用 `cdiv`, 它基本上是取元素总数与块大小的比值, 然后向上取整. 因为我需要向上取整, 以确保最后一组不能被块大小整除的元素仍然能被计算到. 所以我用向上取整而不是向下取整. 然后这些都是非常简单的簿记工作. 然后我说, 好的, 启动核函数. `GeluKernel` 被启动了. 这种尖括号表示这是... 带着给定的块数和每个块的大小.
说话人 1 53:51
这会被传递到核函数命令中. 然后我会传入指向 X 和 Y 的指针. 我实际上不会传入 X 和 Y 的值, 而是传入总元素数. 我需要这个来计算我的核函数的边界条件.
说话人 1 54:09
现在我们来看核函数本身. 我有 `__global__ void GeluKernel`, 并且我接收了输入和输出的指针, 以及元素数量 `nitems`. 这里的 `__global__` 关键字,
说话人 1 54:21
网站这里的渲染把它弄乱了一点, 但你应该把它看作 `__global__`. 这是一个关键字, 用以区分它是一个 CUDA 核函数.
说话人 1 54:33
那么我在做什么呢？这个线程实际上应该操作单个元素 `i`. 但是我没有得到 `i` 作为输入. 代码并没有直接告诉我你在向量的坐标 `i` 处. 所以我需要计算我所在的位置, 我该怎么做呢？
说话人 1 54:51
我会用我的块索引, 因为我只有一个维度, 所以是 `blockIdx.x`, 也就是第一个坐标, 然后乘以每个块的大小, `blockDim.x`. 这基本上告诉了我当前块的起始点.
说话人 1 55:07
然后我现在加上 `threadIdx.x`. 所以, 我知道我当前块的起始位置, 然后我加上我在块内的偏移量, 这就得到了我的全局坐标 `i`. 所以, 这是一些为了得到坐标而做的簿记计算.
说话人 1 55:21
然后这个也很重要. 你基本上在所有人们写的 CUDA 代码中都能看到这个模式. 它天然没有越界检查. 所以你要做的是, 我有了我的坐标, 然后我要检查以确保我应该处理的东西在界内. 在你的块的最后面, 一些线程会处理内存中越界的东西, 你不想让它们碰到那些. 所以你基本上用 `i < n_elements` 作为条件, 如果你在那个范围之外, 就什么也不做.
说话人 1 55:50
抱歉. 是的, 抱歉. 这只是你用来编写 CUDA 代码的扩展名, 用以区别于你标准的 C 代码. 好的, 所以这只是个文件名的事情吗？是 `.cu` 吗？它并没有什么特别之处.
说话人 1 56:11
好的, 然后, 现在, 在这里面, 我们就要进行计算了. 很简单, 我会写出来, 我有了我的输入 `in`, 我会索引到第 `i` 个元素, 然后像我之前做的那样计算我的 GELU, 并把它赋值给 `out[i]`, 然后我就完成了. 这就是我需要做的所有事情. 因为这全是关于指针的操作, 我不太需要担心这里实际发生了什么. 所以基本上就是这样.
说话人 1 56:38
然后我可以把我已有的 CUDA GELU 代码, 通过内联方式加载这段 C++ 代码, 然后让它在 Python 内部编译成一个模块. 这一切都非常方便. 你不需要真的去命令行做这些事情. 所以现在我们定义了 `cuda_gelu`. 这很好. 它基本上是这个的编译版本, 我可以在 Python 内部调用它, 我们会使用 C 语言绑定来调用这个家伙. 好的, 我们调用 `cuda_gelu` 结束了. 我有了, 我可以检查一下手动的 `gelu` 和 `cuda_gelu` 是一样的. 现在我们来对这两个进行基准测试. 我有运行 PyTorch 的时间, 和上次一样, 大约是 1.1 毫秒. 手动实现的时间, 记得是 8.1 毫秒. 那么, 激动人心的时刻来了, 我们的 CUDA 时间是多少呢？嗯, 我们把它降到了 1.8 毫秒. 虽然不如 PyTorch 的实现, 但我们已经非常接近 PyTorch 的时间了, 对吧？我们从 8 毫秒降到了 1.8 毫秒, 这还不错, 因为那段 C 代码写起来并不难. 现在我们也做一些性能分析, 我们可以看到现在发生了什么. 它调用了 `GeluKernel`, 对吧？这是被发送到 GPU 的代码. 然后它调用了 `empty_like`, 这是初始化, 然后是 `empty_strided`. 然后是 `cudaLaunchKernel` 和 `cudaDeviceSynchronize`. 基本上就是这些了.
说话人 1 58:11
注意, 这又是一个单一的 CUDA 核函数, 它消耗了 100% 的 GPU 时间, 这正是我们想要的. 所以还有一些进一步的优化可以做, 但这已经解决了核函数融合的问题. 我们把所有的算子融合在了一起. 所以, 相当不错.
说话人 1 58:30
这类逐元素操作在 CUDA 中很容易编写. 比如, 如果你有一种新的非线性激活函数, 如果你真的想, 你可以很容易地自己为它编写一个 CUDA 核函数. 但更有趣的操作会需要读取多个值, 比如进行规约(reduction)操作. 那些会变得更复杂一些. Flash attention 会稍微复杂一点, 但不会太多. 所以当你在作业中需要做的时候. 好的, 关于这个简单的 C++ CUDA 核函数有什么问题吗？是的. 所以, 你.
说话人 2 59:03
知道, 在一开始检查. 是, 是为了错误吗？是像 Calson 的低级核函数吗？
说话人 1 59:09
问题是, 如果内存不是连续的会发生什么？至少在我们写的代码中, 它会直接抛出一个错误, 因为它是一个 `assert`. 你当然可以写代码来处理这种情况, 但内存几乎没有理由会变得不连续, 因为它会连续地分配. 你也不会重新分配内存的中间部分, 除非你在做一些非常巧妙的操作. 所以, 除非你在做一些非常高级的事情, 你应该预期内存是连续的.
说话人 2 59:37
做一个转置操作, 让所有东西都变得不连续, 对吗？所以, 当你在更高层次上进行布局时, 要小心多样性, 我可能会被迫.
说话人 1 59:47
我的覆盖范围. 问题是, 如果你在做转置, 那么内存就不再是连续的了. 如果你是按行遍历一个按列存储的矩阵, 那么元素和索引之间就会有一个跳跃. 是的, 所以我觉得转置或者像 `view` 或者本质上是打乱维度的操作是这个规则的一个例外, 但这是可以处理的. 在像外层的包装器部分, 你可以基本上给它传递一个连续索引的东西, 对于很多矩阵来说, 你并不会真的在意. 所以是的. 如果你选择一个不同的块大小会发生什么？与 GPU 相关的考量就会开始起作用, 比如:你是否有足够的块来饱和你的 SM？你每个块内是否有足够的工作量？这些是可能在这里起作用的两件事. 但我猜, 对于相对较大的块大小, 比如 1024, 超过某个点后可能就不会有太大影响了, 因为我们没有做任何高级的操作. 对于这个非常简单的例子, 所有操作都是逐元素的.
说话人 2 01:00:56
我们非ug版本这么慢的原因是这个队列要求我们做小操作, 而每个组都非常出色.
说话人 1 01:01:08
问题是, 为什么我们的非 CUDA 核函数的手动实现这么慢？这并不是说它在每次操作后都把数据从 GPU 发回 CPU, X 会一直存在于 GPU 中, 我们在 GPU 上分配它, 比如用 `to(device='cuda')`. 但它基本上不会一直待在 SM 里. 所以, 一旦我们做了 `x` 的平方, 这是一个 CUDA 核函数. 这个乘法操作会从全局内存中读取向量到 SM 中, 进行计算, 然后把它写回去. 所以这都是 DRAM 到 SM 的通信成本, 而不是 CPU 到 GPU 的通信成本. 当然, 如果你写了 `to(device='cpu')`, 那你除了 DRAM 的传输成本之外, 还会遇到 CPU 的传输成本.
说话人 1 01:01:58
好的, 现在你已经看到了, 感觉还好, 不是太痛苦, 但如果我们能有一些更好的 Python 抽象来编写 CUDA 核函数, 那就太好了. 这就是 Triton. Triton 非常棒. 它提供了一个很好的中间地带, 你不需要去管理 GPU 的所有细节. Triton 是 OpenAI 在 2021 年开发的一种领域特定语言. 它使得 GPU 编程变得更加容易. 你基本上在 Python 中编写所有东西, 你不再真正需要考虑线程, 你考虑的是线程块. Triton 会管理很多烦人但可以被自动优化的事情. 它可以管理内存合并. 还记得吗, 从 DRAM 中, 你可以通过一种叫做突发模式(burst mode)的方式一次性获取四个相邻的值. 所以你真的需要确保你的内存读取被分组为一次性读取四个或更多相邻元素. 它会自动处理这些. 当你需要管理 SM 内部多个线程向哪个内存写入时, 它会进行共享内存管理. 你可能需要停止或启动线程, 这些都由它自动管理. 但跨 SM 的调度, 或者说不同 SM 做什么, 那是手动的.
说话人 1 01:03:29
所以, 编程模型是, 你从以 SM 为中心的层面进行思考, 而编译器会处理更多底层的细节. Triton 相当不错, 因为它的性能可以比很多 PyTorch 的实现高出不少. 这就像是直接去写 CUDA, 但你仍然处于非常熟悉的 Python 环境中. 我认为一个被低估的优点是, 正如这里所写, 它全是用 Python 写的. 你可以单步调试, 可以相当方便地进行调试.
说话人 1 01:04:00
那么, 我们来看一个 Triton 核函数. 我们再次要写 GELU, 这次我们用 Triton 来写. 我把代码结构弄得和我们之前的代码尽可能相似. 这可以看作是 CPU 端的代码. 这是包装器 `triton_gelu` 代码. 它接收一个 PyTorch 张量 X. 我在顶部放了两个断言. 我将再次使用 `empty_like` 来分配一个输出张量 Y. 它有着完全相同的坐标计算部分. 甚至核函数的启动看起来也非常相似. 我有一个 `num_blocks` 的注释, 然后我的块大小在最后这里, 不在这对方括号里, 但基本上我向我的核函数传递了相同的信息. 现在 `triton_gelu_kernel` 就是这边的代码. 它将做和我们之前做的一样的事情, 但现在它用 Python 写得很好看. 这里的思维模型是, 输入是 X 指针, Y 指针是输出向量的起始坐标. 块大小是每个块有多大. `n_elements` 是数组的末尾. 现在我需要这几行, 从 557 到 561. 这是在计算我的索引. 我之前用了 `i = ...` 某个公式. 这里做的是同样的计算. 我计算我当前块的起始位置在哪里. 那就是我的块 ID 乘以块的大小. 这会得到, 比如说我住在第 1 个块, 它会给我中间这个点. 然后, 我需要知道我住在块内的哪个位置. 那将是偏移量. 但现在注意一个不同之处. 我没有得到一个偏移量, 因为我不是在为线程编程, 我是在为块编程. 这意味着什么呢？嗯, 我的偏移量实际上是一个向量, 而不是一个单一的值. 因为这基本上是, 我要做一个向量化操作, 而这个向量化操作将由不同的线程来处理. 所以这里我的偏移量是块的起始位置加上一个向量, 这个向量是一系列块大小的偏移量. 所以我的偏移量是第 1 块内所有这些坐标的一次性集合. 当然, 如果我在最末尾, 我可能会越界. 所以我需要一个掩码来处理任何超出我向量边界的东西. 现在我将用一个单一的向量化操作一次性加载所有东西. `x_pointer + offsets`, 这些是我负责的值, 经过掩码处理后, 被加载到 X 中, 这是我的内部值, 我需要的内部临时向量. 有了这个临时向量, 我将做完全相同的旧 GELU 计算. 这里没有 `tanh`, 所以我手动计算它. 但你可以自己验证, 这个公式和我们这里的公式是一样的. 然后 Y 将是上面计算出的公式. 现在, 一旦我完成了, 我需要把它写回到我的输出缓冲区或输出向量中. 所以我计算我的目标. 也就是 `y_pointer + offsets`. 我取我的值, 我的临时值 Y, 然后我存储它. 所以这和之前非常相似, 但这一个是向量化的版本. 我能一次性操作一整个块. 所以, 我不再是从一个线程的角度思考, 而是从一个块的角度思考, 但差别不大, 对吧？这些都是相当相似的东西. 所以现在我写好了我的 Triton 版 GELU. 好的, 我会很快地讲完这个. 好的, 最后一件事, 我只会指出几点, 因为我不想让你们陷入细节以至于起身离开. 但最后一件很酷的事情是, Triton 当然会编译成 GPU 的底层、近乎机器码的代码. 我们可以看一下, 在 Triton 编译器处理之后, 这种非常底层的叫做 PTX 的代码. 这其实挺酷的. 你大概能看到 GPU 在线程层面是如何工作的. 这是 Triton GELU 核函数. 它是由编译器生成的. 一开始, 它会做一些非常基础的事情. 它在做什么呢？它在说, 嗯, 我需要存储一些值. 我需要存储中间计算结果. `b` 实际上表示无类型, 基本上就像字节. 所以我需要 32 位大小的字节. 我需要用于计算的浮点数, 叫做 `f`. 我还需要另一组 64 位的寄存器. 那就是另一组寄存器. 所以, 我有了所有这些用于临时计算的寄存器. 然后从这里开始, 我将开始计算我的坐标. 抱歉, 这部分是加载函数的各种参数. 比如 X 指针和 Y 指针在这里被加载. 从这里开始, 我开始计算我的 Triton 核函数的坐标偏移量. 然后当我到这里时, 这个 `ld.global`, 是用来将值从 X 指针加载回我的临时寄存器的代码. 它基本上是说, 使用 RD1 中的内存位置加载 R2、R3、R4、R5. 注意它是如何一次加载四个东西的, 因为它巧妙地处理了内存合并. 我们知道我们可以免费得到四个值. 我们应该一次性操作所有这四个值, 因为我们得到了它们. 然后你再做同样的事情, 再做一次, 然后你开始得到基本上是浮点运算的 `mul.f32`, 它会遍历并进行 `tanh` 计算. 我不会解释所有不同的部分, 但, 这里它在乘以一个常数. 它通过多次乘以相同的数字来计算 X 的三次方. 然后它在这里计算 2 的 X 次方, 但我们想要 e 的 X 次方. 所以它乘以 log2 来得到指数化的底数. 你真的可以看到 GPU 为了得到最终结果所做的每一个字面上的、一步步的操作. 所以我会跳过所有这些, 直接到最后. 这些都是它需要做的浮点计算. 然后在最后, 它将它拥有的值, R38 到 R41, 存储到 RD4 中, 这是我们输出的内存位置. 所以这大概就是底层实际发生的事情. 我们看到每个线程一次操作四个值, 它的临时存储是寄存器, 这是它本地拥有的非常高速的存储. 所以我们可以看到, 仅仅看一眼就知道这可能是非常快的代码. 好的, 以上是关于 PTX 的内容, 我们可以继续深入了解它在各种情况下的具体行为. 但现在让我们回到正题, 进行实际的基准测试. 我们有手动实现的 GELU, 耗时 8.1 毫秒; PyTorch 版本, 1.1 毫秒; CUDA 版本, 1.84 毫秒; Triton 版本, 1.848 毫秒. 所以我们并没有变得更快, 但是写 Triton 代码要容易得多, 对吧？我们用 Python 写的, 我们考虑的是块, 我们可以做向量化的加法. 如果你在做更复杂的东西, Triton 基本上会为你处理很多内存方面的事情. 所以它其实相当不错. 然后是性能分析, 我们再次看到单个核函数启动就消耗了所有的 GPU 资源. 这太棒了. 这就得到了 Triton 核函数. 最后一件, 至少在这部分, 哦, 等一下, 我想说的是 torch.compile. 当然, 写 CUDA 核函数很酷, 让你感觉很好, 但也许我们不需要那么做. 我们在这里做的事情非常简单. 我们只是把这些像 x 的三次方和指数运算之类的操作, 全都塞进一个 CUDA 核函数里. 所以也许我们不需要做太多就能实现这一点. 我们已经展示了几种不同的方法. 现在我想谈的最后一种是这个叫做 `torch.compile` 的东西, 它会接收一个未经优化的 PyTorch 代码, 然后生成更优化的代码. 在这里, 它会尝试自动进行像核函数融合这样的优化, 这个编译后的 GELU 在实际输出上是等效的. 但现在我们来看看运行时间. 运行时间有一些波动, 但基本上是同样数量级的数字. 手动实现 8.1 毫秒, PyTorch 1.1 毫秒, CUDA 1.8 毫秒, 然后 `torch.compile` 是 1.47 毫秒. 所以结论是, 现代的 JIT 编译器相当不错. 它可以做像算子融合这样的优化, 而你根本不需要做太多. 如果你深入底层看, 你大概能看到基本上, 又是一次性完成所有事情, 这是一个融合了加法、乘法和 `tanh` 的 Triton 代码. 所以它在底层生成了 Triton, 基本上和我们的 Triton 代码做着类似的事情, 但它实际上比我们做的稍微更优化一些. 所以它的性能甚至比我们的代码还要好一点. 所以 `torch.compile` 相当不错. 是的.
说话人 2 01:13:49
首先, 要做得更好. 就像你只需要尝试实现他们的价格转换, 然后看看我是否会像它到达斜线和转移一样移动, 对吧？
说话人 1 01:14:00
问题是, 你什么时候知道, 我想也许更好的提问方式是, 你什么时候知道你能比 `torch.compile` 做得更好, 这是相关的问题. 我认为对于像简单的算子融合这样的简单事情, 或者它非常擅长的另一件事是优化矩阵乘法. `torch.compile`, 正如我之前所说, 可以做一些事情, 比如如果它知道矩阵的形状, 它就能确定要分派哪些核函数. 它在这些方面非常擅长. 我怀疑你能比它做得好多少.
说话人 1 01:14:31
但是有一些事情, 比如如果你看过 FlashAttention 1、2 和 3, 那些都是相当不平凡的优化. 像现在, Torch compile 和像 JAX 的 XLA 编译器可以做那些. 但那是因为我们事后诸葛亮, 知道那些是正确的优化方向. 我认为其中一些东西用 JIT 编译器是很难发现的, 比如 FlashAttention 3 有额外的硬件级优化, 利用了 H100 的硬件特性, 这用 JIT 编译器是不容易做到的. 所以有一些我认为用 `torch.compile` 很难做到的事情, 我觉得你可以做得更好. 但总的来说, 我认为这里的重点是, 你不应该回家后就说:“我要写 CUDA 核函数, 我要为我的语言模型的每一个部分都写 CUDA 核函数. ”那可能不是对你时间的良好利用. 但是, 如果你正在写一个带有某些复杂部分的新架构, 并且你没有得到很好的利用率, 但你认为你可以, 那也许就是真正拿出 Triton 的时候了. 好的. 我们基本上到时间了, 但我们可以快速地过一下 Triton 的最后一个例子. 也许这对你们做第二次作业中的 Softmax 会有用. 一个不同之处是, 到目前为止我们做的都只是基本的逐元素操作, 那非常容易, 因为你只是对每个元素进行操作, 这类事情没有什么复杂性. 现在我们来做 Softmax, 它有一个规约操作, 你必须对所有元素进行求和. 我们该怎么做呢？嗯, 我们想做的是对矩阵的每一行进行归一化. 我们希望让这个过程变快. 这个的朴素版本会相当慢. 现在我们来写 Triton 核函数. 如果我想偷懒, 最简单的方法是, 再一次, 实际上, 你可以想一下做这个最简单的方法是什么.
说话人 1 01:16:23
现在, 假设你想写一个 Softmax. 你要对一个矩阵的每一行进行归一化. 并且想象这些矩阵都非常小, 所以你只是在为小矩阵写一个核函数. 那么, 如果你这样做, 正确的块设计是什么样的呢？嗯, 也许我们应该做的是, 我们的网格(grid)应该就只是行. 所以每个 SM 将处理单一一整行. 这大概是最好的做法, 因为如果我们能把一整行都放进一个 SM 里, 那我们就在 SM 里对那一行求和, 然后做除法, 对吧？这太棒了. 所以这将是我们非常朴素的 Softmax 核函数的简单设计.
说话人 1 01:16:59
所以我们要做的就是, 我们会让块的大小基本上, 抱歉, 我们会让每个块成为一行. 所以块的大小应该是列数, 再加上一点缓冲, 以便能容纳所有的列.
说话人 1 01:17:13
这就是 `triton.next_power_of_2(N)`. 这是对你的列进行填充的一个好方法. 然后我会让每个块都是一行, 块的数量正好是行的数量. 然后我就有了 `triton_softmax_kernel`, 它的写法和你预期的差不多. 所以现在我们有了一个矩阵而不是一个向量. 我们有 X 指针, 有 Y 指针, 我们需要矩阵的步长(stride). 然后我们基本上可以算出我在哪个行索引. 我可以得到列的偏移量. 这将和之前的代码差不多. 事实上, 获取行的偏移量更简单, 因为每一行就是一个块.
说话人 1 01:17:52
然后, 现在我基本上要做同样的事情. 我会把每一行加载到我的 SM 的本地内存中. 然后我会以一种看起来就像 Softmax 的方式进行计算. 我有了我的一行, 我减去最大值, 我取指数, 我求和, 然后我做除法, 这就得到了我的 Softmax 归一化后的一行, 然后我把它写回全局内存. 一点都不复杂. 只要你的计算能很好地放入 SM 中, 写 Triton 代码就和写普通的 Python 代码非常相似, 只是多了一点加载和存储, 以及追踪块的位置.
说话人 1 01:18:27
对吧？所以生活很简单. 我们回去. 等等, 我们到哪了？到 Triton. 好了. 然后我们可以看看我们所有不同部分的代码有多快. 我再缩小一点, 确保……好的. 所以手动实现的时间是 3.7 秒. 我们 `torch.compile` 的编译时间是 1.3 秒, PyTorch 的时间是 1.5 秒, Triton 的时间是 1.9 秒. 还是有点慢. `torch.compile` 实际上可以比原生的 PyTorch 实现做得更好, 特别是当它知道某些操作的形状和大小时.
说话人 1 01:19:06
最后, 我们可以在性能分析器里看一下. 手写的 Softmax 简直是一场灾难. 你在这里看到各种疯狂的操作到处发生. 让我清除这个, 如果我们回到上面. 好的. 是的, 我们看到各种操作在发生.
说话人 1 01:19:20
我们有 X, 有最大值, 有求和, 因为我们是朴素地实现的, 到处都有内存读写. 编译后的 Softmax 就只是一个融合的 Softmax 操作, 运行得非常快. 然后我们有 PyTorch 的 Softmax, 它也是一个 CUDA 核函数调用. 我们的 Triton Softmax 也是一样. 我们有我们漂亮的 Triton Softmax 核函数, 它是一个融合了所有东西的单一核函数. 好的, 我不会再过一遍这个的 PTX 代码了.
说话人 1 01:19:51
我想, 我们时间也差不多了, 我不想再拖着你们看那些底层的东西了. 但希望, 这能让你们对为了让语言模型运行得更快而进行的底层 CPU 编程有了一点感觉. 希望你们在做第二次作业时会觉得有趣. 下次见.