**说话人 1 00:00**
现在, 我们来谈谈多机优化. 今天的重点将完全围绕跨机器的并行化展开. 我们的目标将从优化单个 GPU 的吞吐量, 转向理解训练真正大型模型所需的复杂性和细节. 当模型规模变得庞大时, 它们将无法再装入单个 GPU. 因此, 我们必须将模型拆分到不同的机器上. 同时, 我们还需要能够利用所有可用的服务器来快速训练这些模型. 

**说话人 1 00:37**
所以, 我们需要处理计算和内存两方面的问题, 以及跨不同机器的通信. 这将是一个相当异构的环境. 我们在不同层级的 GPU 之间有不同类型的通信. 这催生了不同的并行化范式. 人们会同时使用多种不同的并行化策略, 我们将逐一探讨其中最流行的几种. 然后, 我们将讨论如何将它们组合在一起, 以高效地训练一个非常大的模型. 讲座最后, 我会通过一些案例来结束, 展示人们在实践中是如何运用这些并行化策略来运行他们的大规模分布式训练任务的. 好的, 这大致对应了本次讲座的不同部分. 我们首先会讨论网络的基础知识, 然后将探讨这些网络硬件概念如何映射到不同的并行化策略上. 

**说话人 1 01:35**
最后, 通过一些案例研究来收尾, 展示所有这些技术是如何整合在一起的. 上周我向你们介绍了 GPU 的规模扩展, 看到这条超指数级增长的曲线——每个 GPU 的浮点运算性能(flops)飞速提升, 确实令人印象深刻. 但是, 如果我们想要快速地扩展我们的计算和内存, 单个 GPU 是不够的. 我们得再等上好几年, 才能看到这条曲线继续攀升. 

**说话人 1 02:06**
因此, 如果我们想在此时此地训练一个真正强大的语言模型, 就必须依赖多机并行. 我们看看世界上最快的超级计算机, 也就是右图所示的这些. 最快的超级计算机拥有百亿亿次级(exaflops)的计算能力. 你看到的那些绿线就是. 如果你想训练当今最大、最强的语言模型, 就必须依赖这样的计算资源. 这就是从计算角度考虑多机并行的原因. 

**说话人 1 02:39**
但我们同样可以从内存的角度来思考这个问题. 计算和内存, 这两者确实是你必须考虑的核心资源和核心问题. 就内存而言, 许多模型正变得非常庞大. 当然, GPU 的内存容量也在增长, 但速度没有那么快. 单个 GPU 将无法容纳这些模型. 或许在遥远的未来, 我们不必再担心这些问题. 但现在, 我们有数十亿甚至更多的参数, 它们无法轻松地装入单个 GPU. 

**说话人 1 03:09**
所以我们必须非常认真地对待我们所面临的内存限制. 这些是我们必须处理的现实情况. 那么, 我们有哪些工具来应对这些挑战呢？

**说话人 1 03:20**
嗯, 你们肯定在课程的集群中注意到了, GPU 不是以单个独立的形式出现的. 一台机器, 在同一个物理机架内, 会包含多个 GPU. 这里有一个例子. 

**说话人 1 03:33**
这张图我记得是从 GPT-NeoX 的论文中引用的, 这是一个较早的例子, 但同样的道理也适用于你们在课程中使用的 H100 机器. 这里有八个不同的 GPU, 它们通过高速内部互连(interconnects)连接到各个 CPU. 你可以在底部看到这个 NV Switch 的东西, 它实现了这八个 GPU 之间非常快速的连接. 但是, 如果这八个 GPU 想要与另一台机器上的 GPU 通信, 它们就必须通过一个网络交换机. 你可以看到这条紫色的线, 这是 HDR InfiniBand, 与 NVLink 连接相比, 这是一个慢得多的连接. 你可以从吞吐量上看出差异, 大概每通道慢了大约八倍. 我们所拥有的这种硬件层级结构, 将对我们最终在实践中如何并行化模型产生重大影响. 在我讲解这些内容时, 你可以一直记着这个心智模型. 

**说话人 1 04:30**
在一台机器内部, 我们有非常快速的连接. 当我们跨机器通信时, 速度就会变慢. 而且, 根据我们使用的硬件类型, 一旦我们超过(比方说)256 个 GPU 组网的规模, 可能还会遇到另一个层级的速度下降. 

**说话人 1 04:47**
你们中许多上过系统或网络课程的人可能已经了解这些, 但这里我还是对集体通信操作(collective communication operations)做一个非常简要的回顾. 我之所以要提这个, 是因为有一个特别重要的恒等式或等价关系, 你需要了解它, 才能真正理解并行化算法性能特性中的一些精妙之处. 我会逐一讲解这些操作, 然后再谈一个重要的性能影响. 

**说话人 1 05:17**
第一个是 `All-Reduce`, 你们可能都听说过. 假设有四台机器, 也就是四个 `rank`, 每个都有自己的一份数据. 你想执行某种规约(reduction)操作, 比如说, 我想将所有这些输入求和, 然后希望输出被复制到每一台机器上. 

**说话人 1 05:36**
这个操作的通信成本大致是你所处理数据总量的两倍. 然后是广播(`Broadcast`)操作. 在这里, 我从 `rank` 2 取一个输入, 然后想把它复制到所有其他的 `rank`. 这个操作的通信成本大致是输出总量的 一倍. 

**说话人 1 05:59**
接着是规约(`Reduction`), 我们有不同的输入, 它们会被求和, 然后只发送到一台机器. 还有两个非常重要的操作, 尽管它们可能不那么常见, 那就是 `All-Gather` 和 `Scatter`. `All-Gather` 是这样一个操作:我从 `rank` 0 取出(比如说)我参数的一部分, 然后把它复制到所有的 `rank`. 

**说话人 1 06:22**
`rank` 1、2、3 也做同样的事情. 所以, 每个 `rank`都在处理参数的不同部分, 并将它们复制到其余的机器上. 这就像是把我拥有的东西复制给其他人. 而 `Reduce-Scatter` 则是, 我接收每一行, 假设我将它们求和, 然后只把结果发送给 `rank` 0. 这是 `All-Reduce` 的一个部分版本. 

**说话人 1 06:44**
希望这个图能清楚地说明 `Reduce-Scatter` 的工作方式. `All-Gather` 和 `Reduce-Scatter` 非常重要, 因为在某种程度上, 它们是许多并行化算法构建的基础原语. 这里有一个很重要的等价关系或者说恒等式, 我会在这次讲座的关键点上提到一两次. 

**说话人 1 07:07**
如果你想执行一次 `All-Reduce`, 假设有不同的 GPU, 比如 A、B、C、D, 每个 GPU 处理一个不同的数据点. 这样, 你就会为每个数据点得到不同的梯度, 并且需要将这些梯度相加. 

**说话人 1 07:21**
然后, 你需要将所有这些梯度传回给各个 GPU. 这是在我的四个 GPU 之间可能需要执行的经典数据并行操作. 这就是一次 `All-Reduce`. 然而, 一个重点是, 这个操作可以被替换为两个操作:一次 `Reduce-Scatter` 和一次 `All-Gather`. `Reduce-Scatter` 会对每一行进行求和, 然后将结果分别留在 GPU 0、1、2、3 中. 然后, 我会执行一次 `All-Gather`, 将这些结果复制回其余的 GPU. 这样, 每个 GPU 现在都得到了部分参数的完整和, 然后它会将其复制回其余的工作节点(worker). 

**说话人 1 08:01**
在受带宽限制的情况下, 这基本上是你能做到的最优方案. `All-Reduce` 的最优性能, 大致与你通过 `Reduce-Scatter` 和 `All-Gather` 所能获得的带宽相当. 你可以通过写出 `All-Reduce` 和右侧操作中发生了多少次通信来验证这一点. 

**说话人 1 08:22**
在开始讨论并行化算法之前, 我想简单谈谈最后一点. 这也是我唯一一次会讨论 GPU 与 TPU 的地方. 

**说话人 1 08:32**
今天的大部分讨论实际上可以抽象掉底层硬件. 但确实有一个重要的事情, 我会提前说明, 以便后续讲解时可以引用. 

**说话人 1 08:44**
我们如何将不同的机器或不同的加速器, 比如 GPU, 连接在一起？正如我之前在 GPT-NeoX 的幻灯片中展示的那样, 在 GPU 的世界里, 通常是这样运作的:你有一些节点, 即单台机器, 其中包含(比如说)八个 GPU, 然后有交换机将它们非常快速地连接在一起. 这些机器被连接起来, 最多可达约 256 个 GPU. 这是一个重要的阈值, 在此范围内, 机器之间可以进行非常快速的任意通信. 超过这个范围, 你实际上就需要速度慢得多的通信, 比如通过叶交换机(leaf switches)和脊交换机(spine switches), 一旦你超出了大约一个机架所容纳的 GPU 数量. 

**说话人 1 09:28**
另一方面, 如果你看谷歌的 TPU 设计, 他们在网络方面采取了截然不同的方法. 他们的机器, 每个 TPU 芯片都能与它的邻居非常快速地通信. 这是一种非常容易扩展的结构, 他们称之为环形网格(troidal mesh), 但你只能与你的邻居通信. 我之所以在讲完 `All-Reduce` 后马上谈这个, 是因为如果你考虑执行像 `All-Reduce` 或 `Reduce-Scatter` 这样的集体通信操作, 在环形网格上实现它们的效率会比在全连接(all-to-all)网络上更高. 因此, 如果你的优化目标纯粹是集体通信, 那么考虑像 TPU 这样的网络结构比 GPU 的网络结构更有意义. 

**说话人 1 10:11**
稍后在讲解不同并行化操作时, 我会谈谈这种方法的优缺点. 好的, 总结一下, 现在我们开始讨论一个新的计算单元. 不再是 GPU, 新的单元是数据中心. 整个数据中心将是我们操作的对象. 

**说话人 1 10:29**
现在, 我们将尝试设计算法和分片(sharding)策略, 以实现两个目标. 第一个是线性内存扩展. 也就是说, 随着我增加 GPU 的数量, 我能训练的最大模型规模也随之线性增长. 这样, 如果我真的想, 我就可以训练越来越大的模型. 我也想要线性的计算扩展. 也就是说, 随着我获得越来越多的 GPU, 我用于训练模型的有效计算量也呈线性增长. 

**说话人 1 10:58**
最后, 这些算法中的很多都是通过以不同方式调用这些非常简单的集体通信原语来实现的. 因此, 当我们思考这些并行算法的性能特征时, 我们只需考虑计算集体通信原语的数量就足够了. 这是一个重要的思考方式. 我们在这里不会深入到这些算法的底层实现. 好的, 有什么问题吗？第一部分. 请讲. 

**说话人 2 11:27**
根据上一张幻灯片, 这是否意味着执行 `Reduce-Scatter` 再加 `All-Gather` 比直接执行 `All-Reduce` 更好？

**说话人 1 11:33**
是的, 关于这张幻灯片, 结论是它们是等价的. 如果你考虑像并行计算梯度这样的场景, `All-Reduce` 是一个非常自然的操作, 因为你会将你的数据分发到不同的机器上, 然后你必须将梯度进行 `All-Reduce` 汇总. 但我想说的是, `All-Reduce` 这个非常自然的操作, 实际上可以写成两个不同操作的和, 并且它们是等价的. 因此, 从左边的表示方法换到右边的表示方法, 至少在带宽方面, 不会有性能损失. 这将在大约五张幻灯片后产生重要的影响. 所以你可以稍等片刻, 看看我为什么会提到这一点. 好的, 还有其他问题吗？好的. 

**说话人 1 12:23**
现在我们正式开始. 从某种意义上说, 这是本次讲座激动人心的算法核心部分. 我们应该重点关注三种并行化策略. 第一种是数据并行(Data Parallelism). 从高层次来看, 数据并行的思想是, 我大致上会将参数复制到我所有的 GPU 上. 我不用担心拆分参数, 但我会拆分我的批次(batch), 不同的 GPU 或不同的机器将处理我批次的不同部分. 这就是数据并行. 在具体执行上有很多微妙之处. 

**说话人 1 13:02**
模型并行(Model Parallelism)则是说, 我不希望我所有的 GPU 都持有我模型的全部部分. 当我的模型变大时, 这将成为一个非常大的问题. 所以我需要用非常巧妙的方式来切分我的模型, 并让我的 GPU 处理模型的不同部分. 这就是模型并行. 

**说话人 1 13:19**
最后一部分是激活并行(Activation Parallelism). 我们日常工作中不太会过多考虑激活值(activations), 因为 PyTorch 等框架非常透明地处理了它们. 但是, 随着模型越来越大, 序列长度越来越长, 激活值的内存占用开始成为一个非常大的问题. 所以, 如果你想用大的批次大小来训练这些真正大的模型, 你必须设法管理激活值的内存足迹. 因此, 我们也必须把它们拆分开. 有一些方法可以处理这个问题. 当我们把所有这些方法结合起来, 我们就拥有了所需的全部工具, 以便在我们拥有大量机器时, 优雅地扩展计算和内存. 这些就是核心的概念. 

**说话人 1 14:02**
现在, 我们将讨论如何高效地实现这些想法. 数据并行的起点就是随机梯度下降(Stochastic Gradient Descent, SGD). 如果我们采用非常朴素的批次随机梯度下降, 其公式就像我幻灯片上展示的这个方程. 我取一个大小为 B 的批次, 将所有这些梯度求和, 然后更新我的参数. 所以, 朴素的数据并行就是说, 好的, 把你的批次大小 B 拆分开, 发送到不同的机器上. 每台机器计算总和的一部分, 然后在每次梯度步进之前, 我会交换我所有的梯度来进行同步, 之后再进行参数更新. 

**说话人 1 14:50**
我一直在跟你们谈论计算和内存的扩展以及所有这些事情. 现在让我们来逐一分析一下它们具体是什么样的. 对于计算扩展来说, 数据并行非常棒. 每台机器, 每个 GPU 将得到 B/M 个样本. 如果我的批次大小足够大, 每个 GPU 都能得到一个相当可观的微批次(micro-batch)大小, 并有望使其计算单元饱和. 

**说话人 1 15:16**
好的, 这很好. 通信开销是多少呢？我需要在每个批次传输两倍于我参数数量的数据. 记住, 一次 `All-Reduce` 的通信成本大约是你进行 `All-Reduce` 的数据量的两倍. 如果批次大小很大, 这是可以接受的. 如果我的批次很大, 我就可以掩盖偶尔需要同步梯度所带来的通信开 ઉ销. 

**说话人 1 15:42**
内存扩展方面, 我完全没有改进. 每个 GPU 都需要复制一份完整的参数, 还需要复制优化器状态. 这对内存扩展来说非常糟糕. 所以, 如果我们完全不用担心内存问题, 这是一个可行的策略. 但在实践中, 内存是个大问题. 我想在座的每一位都经历过, 试图将一个大模型加载到 GPU 上, 然后 PyTorch 告诉你内存不足. 这在训练中确实是个问题, 因为如果你能容纳越来越大的批次, 数据并行会更高效. 所以理想情况下, 你会希望节省内存. 

**说话人 1 16:24**
让我们仔细看看朴素数据并行的内存使用情况. 情况实际上比看起来更糟, 可以说相当糟糕. 因为, 就像你们在作业一中做过的那样, 我们可以计算出需要存储多少份模型的副本, 这个数量非常大. 根据你训练时使用的精度, 每个参数可能需要存储多达 16 字节的数据. 实际上, 你可能需要存储大约五份你的权重. 

**说话人 1 16:57**
这真的非常糟糕, 因为如果你只考虑模型参数本身, 理论上你只需要 2 个字节. 那么多出来的 8 倍是从哪里来的呢？嗯, 你至少需要梯度. 如果你用 BF16 精度计算梯度, 那就又多了 2 个字节. 然后, 优化器状态就出现了, 这是一个大问题, 因为你有 4 个字节的主权重(master weights), 也就是你在 SGD 中累积的那些中间和. 你还需要 4 或 2 个字节用于 Adam 优化器的一阶矩估计, 因为 Adam 需要跟踪历史梯度. 然后 Adam 还需要二阶矩估计, 这有点像过去梯度的方差, 这又需要 4 或 2 个字节. 

**说话人 1 17:40**
因此, 最初看起来还好的情况, 现在变得相当严峻. 所以, 如果我把这 16 倍的内存占用画成一幅图, 你会发现你的大部分内存使用, 至少在参数相关的内存方面, 实际上是被 Adam 优化器的状态所主导的. 所以你的内存消耗将是优化器状态所用字节数的函数, 而这通常比核心的参数和梯度内存使用量还要多. 

**说话人 1 18:16**
举一个简单的例子, 一个 75 亿参数的模型, 分布在 64 个加速器上, 你将使用大量的内存. 并且这个内存总量会随着 GPU 数量的增加而线性增长. 这完全不行. 

**说话人 1 18:34**
但是, 一旦我们看到这张图, 就会产生一些非常简单的想法. 你可能会想, 很明显, 或者不那么明显, 我需要参数和梯度在设备间复制, 这对于数据并行来说似乎是必需的. 但是我真的需要把所有的优化器状态都放在每一台机器上吗？一旦你问出这个问题, 你可能就会想到这里的第二行方案. 这就是所谓的优化器状态分片(optimizer state sharding). 如果我们能做到这一点, 那么至少在这种情况下, 我们可以将总内存使用量从 120GB 降低到 31.4GB. 然后, 我们或许可以开始对梯度进行分片, 这样就可以降到 16.6GB. 如果我们再对参数进行分片, 就可以一直降到 1.9GB. 那将是一个非常理想的状态, 因为我们现在已经完全分片了所有需要的优化器状态、参数和梯度内存. 是的. 

**说话人 2 19:33**
优化器状态, 如果我们要在每个设备上进行梯度计算并分散处理, 我们怎么能……？

**说话人 1 19:45**
这是一个非常好的问题. 问题是, 我们如何对优化器状态进行分片？当我们进行数据并行时, GPU 0 必须负责数据点 1, 所以很明显, 它需要知道所有的参数并进行更新. 那么它怎么可能对优化器状态进行分片呢？在某种程度上, 我认为 ZeRO——也就是“零冗余优化器”(Zero Redundancy Optimizer)——是一个非常聪明的想法. 

**说话人 1 20:07**
因为它向你展示了, 即使在进行数据并行时, 你实际上也并不需要把所有东西都复制到每台机器上. 你可以非常巧妙地处理通信, 以避免所有这些开销. 我会详细解释这是如何做到的, 这是一个很好的问题. 

**说话人 1 20:25**
正如我所说, 我们将把优化器状态分割开. 现在, 一阶矩和二阶矩被分散到所有的 GPU 上, 但每个 GPU 仍然拥有完整的参数和梯度. 为什么这很重要？如果我, 比如说 GPU 0, 拥有所有参数和梯度, 这些信息就足以让我计算出完整的梯度, 对吧？也就是说, 针对这个样本的完整梯度更新是可以计算出来的. 我唯一不能做的是, 我不能用那个梯度来进行一次 Adam 步进. 也就是说, 除非我能看到所有的优化器状态, 否则我无法更新我的参数. 这就是关键思想. 

**说话人 1 21:00**
所以现在会发生的是, GPU 0 将计算所有参数的梯度, 但它现在只负责更新它自己所拥有的那部分分片(Shard)的参数. 这就是关键思想. 我们将更新参数的工作分散开, 然后再将参数同步回来. 让我用更详尽的细节向你展示这是如何运作的, 以及为什么它被称为零开销(zero overhead). 

**说话- 1 21:28**
第一步, 每个 GPU 获得一个不同的数据点. 为了简化, 我将整个批次计算过程简化. 假设我有从 GPU 0到 4 的几个 GPU, 每个 GPU 得到一个样本, 并计算出它们所拥有的那个样本的完整梯度. 

**说话人 1 21:43**
接下来我要做的是对梯度进行 `Reduce-Scatter`. 也就是说, 我将收集每个 GPU 拥有的梯度. 假设 GPU 0 负责这前四分之一的参数. 这里的 Y 轴是参数, X 轴是 GPU. 我们将执行 `Reduce-Scatter`, 以确保 GPU 0 拥有来自所有其他 GPU 的、关于它所负责的参数子集的全部梯度信息. 

**说话人 1 22:16**
现在, 它从 GPU 1、GPU 2 和 GPU 3 那里获取了梯度信息, 并且这些信息都在 GPU 0 中被规约(reduce)了. 希望现在清楚了. GPU 0 拥有更新它自己那部分参数所需的所有信息, 因为它有对应这第一部分参数的优化器状态, 也有这第一部分参数的完整梯度和. 

**说话人 1 22:38**
现在, 它将使用梯度和状态对自己那部分的参数进行一次梯度更新. 这样, 我现在在我的 GPU 0 中就有了这个子集的完整更新后的参数. 我需要做的就是将所有更新后的参数通过 `All-Gather` 操作汇集回所有的 `rank`. 好的, 这里有很多问题. 我从这边开始. 是的. 

**说话人 2 23:02**
我不明白……

**说话人 1 23:04**
是的. 那个 16 倍参数数量的通信成本, 是每台机器的, 还是总的？问题是通信成本中的参数数量是单机还是总和. 这里是总和, 因为这四分之一的参数需要被发送三次到这台机器, 然后这个过程要重复四次. 那个也是总和. 哦是的, 两倍参数数量是总和, 因为每个块都必须被发送到其他所有机器. 好的. 好的. 是的. 

**说话- 2 23:45**
这个问题并不局限于你现在展示的内容, 但它让我想到了这一点. 你展示的 AdamW 优化器似乎在很大程度上假设了参数的独立性, 但我们画的所有这些图表都显示了相反的情况, 你知道, 我们有相互连接的节点. 当我们试图将这些参数分开并独立更新时, 这似乎特别……这会造成任何问题吗？

**说话人 1 24:12**
好的, 问题是, AdamW 似乎假设参数是独立运作的, 我猜你这么说是因为我们跟踪梯度的和, 然后对参数进行对角线式的更新, 对吧？但我们知道这并非完全对角. 所以这会有问题吗？确实有过一些更好的尝试来改进 AdamW, 使其不只是对角更新. 比如有 K-FAC 和其他各种二阶风格的优化器被提出来. 尽管它们确实有优势, 但它们并没有取代 Adam. 而且, 使用这些改进的二阶预处理方法可以做一些非常有趣的事情. 

**说话人 1 24:50**
是的. 我们是在哪些行上进行规约(reducing)操作？所以你在问, 这张图里的行代表什么？是的, 你可以想象这里的行代表参数. 所以, GPU 0 负责一定数量的参数, 这是顶部的一个参数块. 当我们做 `Reduce-Scatter` 时, 我们是说, 取样本 0 在这个参数块上的梯度, 再取样本 1 在同一个参数块上的梯度, 然后将它们全部相加, 并把结果放在 `rank` 0. 这就是我们这里的意思. 很酷. 好的. 这里的关键在于我们执行了一次 `Reduce-Scatter` 和一次 `All-Gather`. 如果你还记得我之前说的, 一次 `Reduce-Scatter` 和一次 `All-Gather` 的成本与一次 `All-Reduce` 是相同的. 所以这里发生了一件有点令人惊讶的神奇事情, 那就是, 我们之前对所有梯度进行了一次 `All-Reduce` 来确保每个节点的梯度都同步了, 这花费了我们两倍参数数量的通信成本. 

**说话人 1 25:54**
但如果我们巧妙地安排更新的方式, 我们可以执行一次 `Reduce-Scatter` 和一次 `All-Gather`. 在这两个步骤之间, 我们可以进行一些计算, 而这带来的通信成本是相同的. 但现在, 至少对于优化器状态来说, 我们已经完全地将它在模型中进行了分片. 所以 ZeRO Stage 1 在带宽受限的情况下, 在某种意义上是“免费”的, 并且能为你带来内存上的收益. 是的. 

**说话人 2 26:22**
为了减少高阶矩对内存的贡献, 人们会修改 Adam 以……

**说话人 1 26:32**
当……你说的“抑制高阶贡献”是什么意思？

**说话人 2 26:37**
好的, 对于一阶和二阶矩, 每个 GPU 的内存量是除以……是的. 所以看起来分片是值得的. 

**说话人 1 26:52**
我明白了. 你的意思是, 你可以跟踪更多的优化器状态. 换句话说, 你可以使用更复杂的优化器状态, 因为你可以将它的内存占用除以 GPU 的数量. 虽然这是对的, 但我们接下来要做的是让其他组件的内存占用也随着 GPU 数量的增加而缩减. 这样一来, 事情在某种意义上就不再是免费的了. 也就是说, 如果我们能把所有东西都除以 GPU 的数量, 那么优化器状态将继续成为瓶颈. 希望这是一个合理且有说服力的回答. 好的, 我们将一步步构建到 ZeRO Stage 3, 它更为复杂. ZeRO Stage 2 相对来说还比较简单. 

**说话人 1 27:29**
希望你已经理解了优化器状态分片这个技巧, 我觉得它非常酷. 现在我们想分片更多的东西. 我想把梯度也分散到不同的机器上. 我们大致可以采用和 Stage 1 相同的技巧, 但会增加一个复杂性. 这个额外的复杂性是什么呢？那就是, 我们永远不能实例化一个完整的梯度向量. 如果我执行了完整的反向传播并试图计算一个完整的梯度向量, 我可能会内存不足. 所以我希望我的最大内存使用量基本上由这个上限决定, 即:完整的参数、分片的梯度、分片的优化器状态. 

**说话人 1 28:09**
因此, 我们在进行反向传播时, 当计算梯度向量的过程中, 我们不能先实例化整个梯度向量, 然后再进行通信. 我们必须做的是, 在反向计算梯度的过程中, 一旦我们计算出比如一个层的梯度, 就必须立即把它发送给它所属的那个 GPU. 这大概就是它的工作方式, 思想和之前大致相同. 

**说话人 1 28:34**
现在, 每个 GPU 都有自己的批次数据. 每个 GPU 在计算图上逐步进行反向传播. 我们假设按层进行操作, 每一层被原子地分片到不同的 GPU 上. 

**说话人 1 28:46**
我们要做的是, 在计算图上反向传播时, 每当我们计算完一个层的梯度, 就立即调用一个规约(reduction)操作, 将这个梯度发送给正确的 `worker`. 一个层属于某个 `worker`, 比如在这个例子中是 GPU 2号. 所以我们会在那个时间点立即进行规约, 把数据发送过去. 梯度现在就不再需要了, 也就是说, 我不需要在 `rank` 0、1 和 3 上存储梯度了, 所以我可以立即释放这部分内存. 

**说话人 1 29:13**
然后我们继续这个过程. 这样, 所有的机器都拥有了它们完全更新后的梯度. 现在, 它们有了自己那部分参数的完整梯度, 也有了自己那部分参数的完整优化器状态. 每台机器都可以更新自己的参数, 然后通过 `All-Gather` 将所有参数汇集回来. 这看起来可能通信量更大了, 因为你每一层都要做这种规约操作. 但请注意, 这只是针对一小部分参数的, 因为它是分片的. 所以总的通信量保持不变. 因此, ZeRO Stage 2 有一些额外的开销, 因为我们必须逐层同步, 并确保梯度被正确地发送给对应的 `worker`. 但这个开销非常小, 它仍然非常简单、直接. 现在是最后一个. ZeRO Stage 3 无疑更复杂, 但它能让你获得最大的收益, 那就是现在基本上所有东西的内存占用都除以了你拥有的 GPU 数量. 你能获得可能的最大节省. 如果你听说过 FSDP, 你过去在某些方面可能用过它. FSDP 就是 ZeRO Stage 3. 所以希望今天你能明白 FSDP 的工作原理. 思想是一样的. 我们将分片所有东西, 包括参数. 我们将做和 ZeRO Stage 2 同样的事情, 那就是增量地进行通信和计算, 这样我们就不会让那些巨大的梯度向量到处流传. 我们将在计算图的前向和反向传播过程中, 按需发送和请求参数. 在我们逐步处理的过程中, 我们会按需传递数据. 当然, 关键是以尽可能低的开销来完成这件事. 我认为关于 FSDP 真正令人惊讶的不是这事是可能做到的, 而是它能以相对较低的开销做到. 下一张幻灯片你会看到为什么它的开销会比较低. 我承认这张图作为开始可能不是最友好的, 但我保证, 这是 FSDP 的简化版. 下一张幻灯片会更复杂一些, 但从概念上讲, 这张图已经解释了一切. 我们要做的是, 我们会有模型权重, 并且会随着计算的进行, 对模型权重进行 `All-Gather`. 对于每一层, 没有单个 GPU 会拥有所有的参数. 所以我不能像通常那样说, “嘿, GPU 0, 你去跑一个前向传播吧”. 这不可能. 假设 GPU 0 只拥有最底层的那一层. 它完成那部分的计算, 然后停下来, 向所有其他 `worker` 请求所有的参数. 所以它停下来, 执行一次 `All-Gather`, 就是这里. 你看到有一个 `All-Gather` 步骤. 它收集了所有的参数. 现在它有了进行前向传播所需的参数. 于是它可以向前一步, 计算它之前没有的那一层. 然后现在它可以释放权重了, 它不再需要那些权重了. 摆脱掉它们. 现在我可以 `All-Gather` 下一层的权重. 我可以再做一次前向传播, 释放权重, 然后重复这个过程. 激活值必须被存储起来, 所以这里的激活内存是在增长的. 这最终会成为一个问题. 但如果我们暂时忽略激活值, 这个方法很棒, 因为我加载一层, 做一次前向传播, 然后释放它. 这里的内存开销非常低. 当我到达终点时, 现在我可以在反向传播中做同样的事情. 我可以调用 `backward`. 每次我通过神经网络向后移动时, 我会 `All-Gather` 我需要的参数. 在梯度计算完成后, 我可以通过一次 `Reduce-Scatter` 来更新. 现在我可以释放权重, 我可以释放我不需要的梯度和参数. 在最后, 我就得到了一个完全更新的模型. 所以我们这里有三种不同的操作需要关注. 我们有一次 `All-Gather`, 又一次 `All-Gather`, 然后还有一次 `Reduce-Scatter`, 基本上是在我们执行梯度更新步骤后用来更新模型的. 所以从概念上讲, 这只是比 ZeRO Stage 2 多了一步, 但你确实能看到有更多的开销. 所以总的通信成本现在更高了, 对吧？我们之前大概是两倍参数数量的通信成本, 一切在某种意义上是“免费”的. 现在不是了. 总共有三倍参数数量的通信成本, 并且还会有等待这些通信完成所带来的开销. 但我认为 FSDP 真正酷的地方在于, 它的开销惊人地低. 你可能会想, 因为我们在做这种疯狂的事情,  постоянно地请求和来回发送参数, 所以事情会变得非常慢. 我们必须一直进行通信. 但你可以利用这个核心思想, 即重叠通信和计算. 你希望你的 GPU 在后台进行通信的同时也在工作, 几乎就像预取一样. 这样, 到你需要某条信息的时候, 它已经被加载好了, 已经被传输给你了, 你就可以直接使用了. 我会在下面通过这个例子来讲解, 但这就是让 FSDP 变得高效的关键. 假设我们有一个计算图, 看起来像这样:`W1`, `W0` + `W2`, `W0` * `x`. 某个输入, 比如说 `y`. 一个像这样非常简单的计算图. 然后你可能会运行 FSDP, 你会得到一个计算和通信流程, 就像最后这个框图所示. CPU, 很好的是我们上周做了 Insight Systems 的例子, 所以希望这个图现在会更清楚. CPU 基本上会分派一堆命令, 请求 GPU 的通信部分去获取一些参数. 它会分派任务给 GPU 说, “好的, 做一些矩阵乘法”. 它在某种程度上会远超前于 GPU 的执行进度. 我们上周在看性能分析器时已经看到了这一点. 现在我们来看看设备上发生的通信和计算序列. 记住, 我需要按需收集数据. 所以在最开始, 我必须确保每个节点都有第 0 层的权重, 或者说这里的 `W0`. 所以我执行 `all_gather_0`, 然后等待它完成. 一旦完成, 我就可以在 `W0` 上执行前向步骤. 我可以计算 `x` 乘以 `W0`. 在 `all_gather_0` 结束的同时, `all_gather_1` 就开始了. 所以在我做这个矩阵乘法的同时, 我基本上已经在开始加载我需要的下一组参数了. 当然, 我的通信速度更慢, 所以会有一些间隙, 但我最终完成的速度比最初的加载要快得多. 所以现在 `forward_1` 可以发生了, 并且在后台, 我又一次开始加载参数 2 号. 在这个黄色的区域, 我现在正在释放与 `forward_1` 相关的参数. 然后另一件事是, 我正在重复使用 `W0` 的计算结果, 它被用了两次. 所以我不需要再次通信它. 这发生得非常快, 我能很快地完成. 在需要 `forward_2` 之前, 我已经把它加载好了. 所以这里没有空闲(bubble). 然后我可以释放 2 号参数. 这就是整个前向传播过程. 你可以看到这里的间隙相对较小, 我们在计算需要发生之前就完成了大量的加载操作. 所以通过这种非常聪明的做法, 即在你实际需要权重之前就将请求排入队列, 你可以避免大量与通信相关的开销. 然后, 在 `forward_2` 这里, 我完成了前向传播. 我可以释放权重 2 号, 然后开始反向传播. 你可以看到, 用于反向传播的 `all_gather_2` 已经完成了. 所以我可以开始 `backward_2`. `backward_0`, 权重 0 已经存储好了, 所以也完成了. 然后, 反向传播中的高开销发生在这里, 因为我需要做 `Reduce-Scatter` 和 `All-Gather` 等等. 希望你看到这张图后会说, “哇, 这真令人惊讶”, 尽管我们在做这种疯狂的分片, 对吧？如果你回到这张图, 我们完全分片了参数、梯度和优化器状态, 但我们需要的总带宽只是三倍而不是两倍. 这看起来不算太糟. 而且我们看到的实际空闲时间也并不可怕. 通信几乎被完全利用了, 而计算也没有长时间停顿. 所以我们实际上非常高效地利用了我们拥有的资源, 这很酷. 

**说话人 2 37:43**
好的. 是的. 我们从哪里获取……这像是……据我理解, GPU 的内存是……权重是从哪里获取的？

**说话人 1 37:53**
是的, 所以你需要一个缓冲区来存储这些权重. 所以, 这张图不完全准确. 你会需要一些额外的开销来读取当前层的这些权重. 另外一个显而易见的大问题是, 我完全没有谈到激活值. 这将是很大一块内存, 因为你有一个完整的模型的大量激活值在某种意义上是存放在这里的. 

**说话人 1 38:15**
酷, 对吧？好的, 所以这就是分布式数据并行, ZeRO 在某种程度上是人们高效实现分布式数据并行的方式. 它有不同的阶段. Stage 1 基本上是免费的, 它使用的通信模式和朴素数据并行相同, 但你能分片你的优化器状态. 这很棒, 你没理由不一直用它. ZeRO Stage 2 的通信成本是两倍参数数量, 所以总带宽消耗相同, 但有额外的开销, 因为在反向传播时需要增量地释放梯度. ZeRO Stage 3 更复杂, 通信成本是三倍参数数量, 但这也不算太糟. 就像我们之前在图表中看到的, 确实有一些开销, 但如果你非常巧妙地掩盖你的通信模式, 效果实际上是相当不错的. 所以人们即使在网络连接相当慢的情况下也会使用数据并行. 

**说话人 1 39:16**
好的, 这在概念上也相对简单. 其中一个优点是, 特别是数据并行, 它不太关心模型架构. 我完全没有谈到我们具体如何实现一个 Transformer. 这一切都非常抽象. 这也是为什么, 例如, FSDP 如此流行的原因之一. 它非常容易写一个包装器(wrapper)来并行化任意的神经网络, 而不需要深入了解或反思架构具体在做什么. 所以, 这里有一些例子. 我计算了一些例子, 因为我自己的 GPU 总是内存不足. 你可以看到, 在一个拥有 8 块 80GB A100 的节点上, 我能装下的最大模型尺寸是多少. 对于基线情况, 你可能最终只能勉强装下一个 60 亿参数的模型. 而如果我使用 ZeRO Stage 3, 我能装下一个大约 500 亿参数的模型. 通过使用像 FSDP 这样的技术来巧妙地节省内存, 我在容纳更大模型的能力上获得了巨大的提升. 好的. 哦, 抱歉, 有个问题. 是的. 

**说话人 2 40:24**
我想我有点……我好奇一旦你对参数进行了分片, 那它和模型并行有什么区别？

**说话人 1 40:30**
是的, 模型并行从根本上讲是确保参数只存在于不同的、独立的……让我看看我能不能找到……从某种意义上说, 我们确实对参数进行了分片, 所以你可以称之为一种并行. 但模型并行的核心思想是确保参数完全存在于一台机器中. 我们不会试图以各种方式将它们传来传去, 只有激活值会被传递. 所以, 在模型并行的部分, 你会看到非常不同的讨论, 重点将是通信激活值, 而不是通信参数. 这将是一个很大的区别. 是的. 

**说话- 2 41:12**
如果你的参数是在……为什么你要用 `All-Gather`？

**说话人 1 41:20**
哦, 是的, 你在问这个步骤, 为什么我们要用 `All-Gather` 把权重收集到所有的机器上？当它们只在一台机器上的时候？是这个意思吗？所以我们需要把存在一台机器上的权重……是 `gather` 还是 `scatter`？抱歉, 我想确认一下我用词正确. 这个术语对我来说有点模糊. 所以我想确保……抱歉. 

**说话人 1 41:53**
是的, 我们想做的是和这个一样的事情. 每台机器都会有一些参数, 我希望把它们收集(gather)到所有机器上, 以确保每一层都在所有的 GPU 上被正确地复制. 

**说话人 1 42:12**
这是你问的那个问题吗？还是你在说, 我们是否可以调用一个更简单的原语？比如, 你是不是认为 `Broadcast` 是正确的选择, 而不是 `All-Gather`？我想, 之所以这么写, 可能是因为有些情况下层并不只存在于单个 GPU 上, 但我不是百分之百确定. 我同意你的观点, 如果参数只存在于一台机器上, `Broadcast` 应该也能做到同样的事情. 好的, 很酷. 好的. 让我确认一下……好的, 明白了. 

**说话人 1 42:46**
好的. 在数据并行中, 有一个关键资源. 这其实是一个我希望你们记住的重要概念. 在数据并行中, 批次大小(batch size)实际上是一个非常关键的资源, 因为你的并行度不能超过你的批次大小. 因为你最多只能在每台机器上放一个样本, 你不能让每台机器处理零点几个样本. 这意味着, 如果你的批次大小有限制, 你就无法继续使用数据并行, 而且增加批次大小的收益是递减的. 在你们的作业一中, 你们可能尝试过改变批次大小, 但你们应该知道, 当你把批次大小调高到某个点之后, 你的优化速度的收益就会开始迅速递减. 关于这个主题有很多论文. 

**说话人 1 43:39**
OpenAI 有一篇非常好的论文, 讨论了所谓的“临界批次大小”(critical batch sizes). 他们基本上认为, 超过某个点后, 每个样本对优化能力的贡献会迅速出现递减回报. 其背后的直觉是, 在某个点之下, 你有大量的梯度噪声, 减少这些噪声非常有价值. 但在某个点之后, 你真正受限的是你执行的梯度步数, 而不是方差的减少. 这基本上意味着, 单靠数据并行无法让你实现任意大规模的并行化. 

**说话人 1 44:11**
这个批次大小是一个非常重要的资源. 本质上, 你有一个固定的最大批次大小, 你可以用不同的方式来“花费”它. 我稍后会谈到这一点, 因为其他类型的并行化也能从更大的批次中受益. 所以, 你会把你的批次大小用在某些地方. 

**说话人 1 44:28**
好的. 数据并行仍然存在一些问题. ZeRO Stage 1 和 2 不能让你扩展内存. ZeRO Stage 3 原则上很好, 但可能很慢. 也许更重要的是, 这一点与之前的问题有关, 它不减少激活值的内存占用. 我理想的情况是能完全切分我的模型, 让它们完全独立地存在, 因为这样激活值的内存也会相应减少. 所以现在我需要更好的方法来分割模型, 以便能将这些真正大的模型装入这些 GPU 中. 这就引出了模型并行(model parallelism). 我们希望在不改变批次大小的情况下扩展内存, 并且我们希望有一个替代的并行维度, 在这个维度上我们不需要花费或者说拥有大的批次大小来进行并行化. 所以我们将把参数分散到不同的 GPU 上. 在某些方面, 这就像 ZeRO Stage 3, 但我们不再通信参数, 而是传递激活值, 这将有所不同. 有时候激活值会比参数小得多, 这对我们来说会非常好. 我们将介绍两种不同类型的并行. 

**说话人 1 45:39**
我将讨论流水线并行(pipeline parallel), 它在概念上更简单, 但在实现上要糟糕得多; 以及张量并行(tensor parallel), 它在概念上可能不那么直观, 但实现起来要好得多, 也更常用. 它们对应着两种不同的模型切分方式. 我认为流水线并行可能是切分神经网络最直观的方式. 你知道, 深度神经网络是由层组成的. 所以如果我有多个层, 一个非常自然的切分点就是在层的边界上进行切分. 因此, 每个 GPU 将处理一部分层, 并且我将在它们之间传递激活值. 在这种情况下, 每一层属于一个 GPU, GPU 之间会依次传递激活值. 在反向传播时, 它会从 GPU 3 到 0 反向传递梯度. 好的, 这很酷, 很棒. 

**说话人 1 46:35**
这张图有什么问题呢？嗯, 你应该能看到, 大部分时间里, 你的大部分 GPU 都是空闲的. 这实际上是非常糟糕的利用率. 所以, 如果我采用我之前描述的这种朴素的并行方式, 假设每个层处理一个样本, 那么就会得到一个像这样的图表. 这张图中, 不同的行代表不同的层, 也代表不同的 GPU, 而 X 轴是时间, 从左到右. 

**说话人 1 47:07**
你看到了什么？嗯, 首先, 我在最左边计算我的第一层. 然后激活值被传递到第二层. GPU 2 被唤醒, 心想, “好了, 轮到我了”. 它完成它的工作, 传递给 GPU 3, 然后是 GPU 4. 现在反向传播可以开始了, 依此类推. 你看到了这个巨大的, 人们称之为“气泡”(bubble)的东西. 这是一个巨大的开销, 在这段时间里你什么也没做. 你看到 GPU 的活跃时间只有总时间的 1/n. 

**说话人 1 47:32**
所以从某种意义上说, 这是最糟糕的并行方式. 我增加了四个 GPU, 但得到的吞吐量只有一个 GPU 的水平. 所以, 你可以做的一件事是, 你可以更聪明一点, 你可以说, “好吧, 我建立一个流水线”. 我不仅仅是按层切分, 我还会让每个 GPU 处理一个序列的任务. 

**说话人 1 47:55**
现在, 假设我有一个微批次(micro batch). 每台机器将处理四个样本. 我要做的是, 我可以完成我的第一个样本, 我的第一个数据点, 并且在我一完成就可以把它的激活值发送给我的第二个 GPU. 然后我就可以开始处理我的第二个数据点了. 这样, 我就重叠了通信和计算. 第二个 GPU 可以在第一个 GPU 继续工作的同时开始工作. 现在, 通过使用更大的批次大小, 这个“气泡”的大小就有可能被减小. 你现在应该能理解为什么我之前说批次大小是一种资源了. 如果你有一个有限的批次大小, 并且你正在使用流水线并行, 你可以用这个批次大小来减小你的流水线气泡, 或者你也可以用它来做数据并行. 所以有很多不同的方式来利用你这一个批次大小, 把它拆分到不同的用途上. 好的, 现在你的微批次大小可以控制“气泡”时间. 实际上, 你的开销与有效计算的比率是(阶段数 - 1)/ 微批次数. 所以如果你有大的批次大小, 流水线并行可能会很高效. 但正如我们之前所说, 批次大小是有限的, 我们不能随意把它调到任何我们想要的值. 所以, 总的来说, 流水线看起来很糟糕. 我们为什么要承受这个“气泡”的代价来做并行化呢？

**说话人 1 49:28**
嗯, 有几个原因. 与数据并行相比, 流水线有助于节省内存. 当然, ZeRO Stage 3 也会对参数进行分片, 但流水线并行还会对激活值进行分片, 这很好. 流水线也可以有很好的通信特性, 因为它只依赖于激活值, 而且是点对点通信. 所以, 根据你的网络拓扑和具体情况, 流水线可能对你网络中较慢的部分非常有利. 因此, 流水线并行通常会被用在你较慢的网络链路上. 比如, 节点间, 甚至有时跨越不同机架或数据中心——其实不是数据中心, 是跨不同机架——你可能会使用流水线并行. 

**说话人 1 50:11**
我最近从一些谷歌员工那里听到的一个例子是, 他们说 TPU 的一个巨大优势在于我们不怎么需要做流水线并行, 因为我们所有的连接都快得多. 他们有那个巨大的环形网格(toroidal mesh), 他们没有在 256 个 GPU 处的那个限制, 一旦超过那个限制, 你就会突然转向一个更慢的网络链接, 这时候你可能就想切换到流水线并行了. 这是一个现实世界中, 你会开始考虑流水线并行的例子. 这是来自一篇 NVIDIA 论文的例子, 我稍后会更详细地讨论这篇论文. 他们做了一些非常好的工作, 展示了不同类型并行化的性能特征. 你可以看到, 当批次大小为 8 时, 随着你增加流水线并行的大小, 即设备数量, 你的每个 GPU 的利用率开始急剧下降. 而如果你有一个 128 的大批次大小, 对于合理规模的流水线并行, 你仍然可以获得相当不错的利用率. 所以, 批次大小是隐藏“气泡”大小的关键, 否则你就会遇到问题. 当然, 你可以采用不同类型的流水线策略. 比如, 你可以不使用这些标准的调度模式, 而是将任务切分得更细, 将不同的子层分配给不同的设备, 在不同的部分进行不同的计算. 这样你就可以更好地交错流水线. 我想花点时间谈谈一个高级版本, 这非常聪明, 叫做“零气泡流水线”(zero-bubble pipelining), 或者在 DeepSeek 的术语里, 他们称之为“双流水线”(dual pipe), 但核心技巧是一样的. 

**说话人 1 51:58**
在这里, 如果你想一下, 假设我们正在进行反向传播来计算梯度. 你可以把这个过程分成两个不同的部分. 第一部分是关于反向传播激活值. 也就是说, 当我沿着残差连接向下传播时, 我需要计算关于激活值的导数. 然后, 当我到达一个参数时, 我也想计算梯度本身, 即我将如何更新参数, 而不仅仅是激活值相对于前一层的变化. 

**说话人 1 52:31**
为了给你一个具体的例子, 让我们看看左下角的这张图. 在这张图中, 你看到的是前向传播. 这是一个简单的 MLP. 我们有一个权重矩阵乘法, 然后我做一个非线性变换, 然后我直接输出这个非线性结果. 这是一个朴素的、MLP 的一部分. 

**说话人 1 52:49**
现在让我们看看反向传播. 我有一个关于损失的导数输入, 然后我可以计算出这将如何改变 `x`, 也就是我 MLP 的输入. 这在某种意义上是关于激活值的导数. 然后, 在我计算这些导数的同时, 我当然可以用它们来计算我更新权重所需的梯度. 但重要的是, 计算权重梯度的这一部分, 可以随时进行. 它没有任何依赖关系. 所以我可以重新安排这个计算在计算图的任何部分进行. 所以你可以做的是, 对于那些有串行依赖的部分, 你采用标准的流水线并行. 但任何时候你只需要为更新参数而进行计算时, 你可以把它们重新调度到任何地方. 关键思想是, 当你从一个所谓的 `1F1B` 流水线开始时, 这是一个很好的优化, 可以减少气泡大小的调度. 然后你可以把这个 `b`, 也就是反向传播部分的计算, 和计算权重梯度所需的计算分离开来. 

**说话- 1 54:00**
现在我可以在原本会有气泡(bubble)的地方进行权重的计算, 也就是 `W` 的计算. 也就是说, 在那些我原本有白色空闲利用率的部分, 我现在可以用 `W` 的计算来填充它们. 所以, 通过仔细思考串行依赖关系到底是什么, 我现在可以得到一个非常好的结果, 那就是我能够从我的 GPU 中获得良好的利用率. 

**说话人 1 54:24**
需要明确的是, 这非常复杂. 如果你真的想以这种方式实现流水线并行, 你将不得不介入你的自动微分(auto-diff)系统计算这些东西的方式. 你还需要一个队列来跟踪任务的去向. 

**说话人 1 54:43**
我最近在一次谈话中, 从一个前沿实验室训练大语言模型的人那里听到了一个有趣的轶事. 他说, “实际上, 我们组里只有两个人懂我们基础架构里的流水线并行是怎么工作的. ”

**说话人 1 54:55**
其中一个人离职了. 所以现在我们训练基础架构里只有一个‘顶梁柱’了. 像这样的故事是存在的, 流水线并行的基础架构非常复杂. 在这里它看起来很简单. 如果你有兴趣尝试实现它, 它确实会很快变得非常棘手. 我认为这是一个很好的转折点, 让我们切换到另一种模型并行, 因为它简单得多, 并且经常被许多框架和甚至训练大型模型的人干净利落地使用. 他们非常严重地或者说主要地依赖这种模型并行. 

**说话- 1 55:32**
我们还有什么其他方法可以分割模型呢？如果我们想一下, 我们做的大部分工作都是矩阵乘法. 在一个大模型中, 大部分的计算是矩阵乘法, 大部分的参数也是矩阵. 那么我们能做什么呢？如果我们能把矩阵乘法并行化, 那就相当不错了. 张量并行(tensor parallel)就是这样一个想法:我们可以把一个大的矩阵乘法, 分解成一组可以并行相乘的子矩阵. 所以如果我有, 右上角的这个矩阵乘法, `X` 乘以 `A` 等于 `Y`. 我可以做的是, 把 `A` 切成两半, 然后我也可以把 `X` 切成两半. 我可以计算这些子矩阵的乘积, 然后把它们相加, 最后就能得到我的答案. 所以, 从概念上讲, 流水线并行是沿着深度维度, 即层的维度进行切分; 而张量并行, 也就是我们现在讲的, 是沿着矩阵乘法的宽度维度进行切分. 我们将分解成子矩阵, 然后做部分求和. 

**说话人 1 56:37**
这是一个在 MLP 中可能的样子. 每个 GPU 处理一个大的 MLP 矩阵乘法的不同子矩阵. 然后我们将通过集体通信来同步我们需要的激活值. 

**说话人 1 56:56**
我们要做什么呢？这是一个 MLP, 上半部分和下半部分有两条不同的路径, 它们是对矩阵进行拆分. 我想执行这个操作 `y = GELU(x * A)`. 我将把我的矩阵 `A` 分成 `A1` 和 `A2`. 然后在右边, 我想计算 `dropout(y * B)`, 然后将结果返回为 `Z`. 

**说话人 1 57:20**
所以我也要切分 `B`. 我把我的两个大参数矩阵 `A` 和 `B` 都切分成了两部分. 在前向传播中, 我要做的是, 我将我的输入 `x`, 直接复制两份. 每个 GPU 将得到相同的输入, 然后它们将分别用 `A1` 和 `A2` 对其进行操作. 它们有相同的行维度, 所以这样操作没问题. `x * A1` 和 `x * A2` 会得到一些激活值 `Y1` 和 `Y2`, 这些会进入 `B1` 和 `B2`. 

**说话人 1 57:51**
然后, 我将执行一次 `All-Reduce` 来将它们相加. 这正是我之前给你看的图. 你复制, 然后进行 `All-Reduce`, 就得到了答案 `Z`. 在反向传播中. 

**说话人 1 58:01**
现在, 当梯度反向传播时, 情况正好相反. 在反向传播步骤中, 这个 `g` 将是恒等(identity)操作. 所以我将在两边都复制导数. 我将一路执行反向操作. 当我到达 `f` 时, 这不是 `All-Reduce`, 因为我有从两条路径传来的两个导数, 然后我将它们加起来. 

**说话人 1 58:22**
所以 `f` 和 `g` 是同步屏障(synchronization barriers). 在前向传播中, 我执行一次 `All-Reduce`. 在反向传播中, 我也执行一次 `All-Reduce`, 只是在计算图的两个不同位置. 所以现在你 hopefully 可以看到, 这是一种非常好的方式, 无论你在哪里有矩阵乘法, 你都可以把它切分开, 并在不同的设备上并行化. 好的. 你可能已经想到了, 这实际上是有些昂贵的. 我们有一个同步屏障, 它存在于每一层. 它需要在一个前向-反向传播路径中, 通信两次激活值, 其大小与残差激活值相当. 所以, 张量并行这个非常简单的想法, 将需要非常高速的互连. 有一个经验法则, 一个非常简单的经验法则需要记住, 那就是张量并行是在一个设备内部或者说一个节点内部应用的. 一个装有(比方说)NVIDIA GPU 的机箱, 会配备八个在同一个机箱内的 GPU. 正如我今天讲座开始时向你们展示的, 它们之间有非常高速的连接. 所以这八个 GPU 可以非常快速地互相通信. 因此, 在那些非常需要带宽的八个设备之间使用像张量并行这样的技术是很有意义的. 所以我们通常会看到, 张量并行被应用在最多八个 GPU 上, 而这八个 GPU 都在同一台机器内, 因为这样性能下降最少. 这是一个来自 Hugging Face 的并行化教程的例子, 向你展示了不同级别的张量并行所带来的吞吐量下降. 你可以看到, 当你做张量并行时, 吞吐量确实有下降, 分别是 10% 和 12%. 

**说话人 1 01:00:05**
但是直到 8 个设备, 这也许是可控的. 这算是你为了能更方便地并行化而付出的代价. 但当你增加到 16 个设备时, 你会看到性能惊人地下降了 42%. 增加到 32 个设备时, 吞吐量又下降了 65%. 所以, 希望你能从视觉上看到, 你真的应该在 8 个设备时停止增加张量并行度. 由于你能获得的硬件互连类型, 这确实是最佳点. 

**说话人 1 01:00:34**
那么, 现在与流水线并行相比情况如何呢？与流水线并行相比, 我们不需要处理之前那个“气泡”问题. 我们不需要消耗更大的批次大小来减少“气泡”, 这很好. 而且应用张量并行的复杂度相对较低. 你真正需要知道的只是, 大的矩阵乘法在哪里？我能把它们拆分开并让它们在不同的设备上运行吗？

**说话人 1 01:01:00**
前向和反向操作仍然保持不变. 与实现像“零开销”或“双流水线”这样的流水线并行相比, 这样做你的处境会好得多. 缺点是通信开销大得多. 在流水线并行中, 对于每个微批次, 你有(批次大小 * 序列长度 * 残差维度)的点对点通信. 而在张量并行中, 每层你有八倍于此的 `All-Reduce` 通信. 这可能需要进行大量的通信. 

**说话人 1 01:01:35**
所以, 如我之前所说, 经验法则是, 张量并行通常用于有低延迟、高带宽互连的地方. 在实际应用中, 你会看到 2 到 16 路的张量并行, 具体取决于你拥有的机器类型. 在最后, 当我讲解张量并行的例子时, 我会给你们展示一些案例. 好的. 在进入第三部分, 即序列并行和激活值分片之前, 关于流水线并行或张量并行有什么问题吗？是的. 

**说话人 2 01:02:08**
它们可以同时使用吗？

**说话人 1 01:02:11**
是的, 问题是它们是否可以同时使用. 答案是, 是的, 你确实会同时使用它们. 我们稍后会看一些例子, 但我认为你通常看到的情况是, 对于大规模的训练任务, 你经常会看到张量并行, 而流水线并行通常是在此之上使用的. 我所知道的唯一一个只用流水线并行而不用张量并行的例子, 据我所知是 DeepSeek-V3. 

**说话人 2 01:02:38**
所以在单个……我想, 如果你有, 比如说五台不同的机器, 你可能会把前 20% 的参数分布在……然后通过流水线并行到第二台机器……

**说话人 1 01:02:53**
是的. 问题是, 你是否在机器内部做张量并行, 而在机器之间做流水线并行？是的, 你可能会在机器内部做张量并行, 而在机器之间结合使用数据并行和流水线并行. 我稍后会告诉你经验法则, 但基本上, 你做流水线并行是因为你的模型放不下了. 如果你能放下整个模型, 你可能就只做数据并行加张量并行, 或者甚至只做数据并行. 好的, 非常好. 

**说话人 1 01:03:23**
那么, 我们一直在谈论内存, 而内存, 在某种意义上, 是并行化中非常重要的一部分, 因为我们要训练大型模型. 所以, 当你审视你的内存使用情况时, 你会发现激活值实际上是内存占用的一个非常大的部分. 如果你看一个标准的、有点像前向-反向传播过程, 我想这是来自 PyTorch 的一个教程, 你会看到内存使用是非常动态的. 我会讲解一下这个, 因为我觉得这是一个通常很有趣的图. 你的参数在你训练时总是在那里的, 因为它们是静态的. 但在迭代 0 时, 你甚至还没有优化器状态. 所以实际上, 你没有那部分内存使用. 但当你进行前向和反向传播时, 你会看到激活值内存随着你累积所有激活值而增长. 当你开始反向传播时, 你的激活值内存会下降, 因为你在使用完激活值后会释放它们, 然后你开始累积梯度. 所以你的梯度内存使用会上升, 而峰值实际上是在你反向传播过程中的某个中间点, 那时你还没有释放所有的激活值, 但仍在累积梯度. 所以在迭代 2 中, 你基本上会看到同样的情况. 所以, 这张图的要点是说, 我们已经考虑了所有其他部分, 我们考虑了参数, 考虑了优化器状态, 考虑了梯度, 但我们至少没有深入思考过激活值. 所以, 让我们来做这件事. 我想跟你们讲的最后一个复杂点就是激活值内存. 张量并行和流水线并行可以线性地减少大部分东西, 但它们实际上无法减少所有的……

**说话人 1 01:05:00**
激活值的内存使用量. 这是一个来自 NVIDIA 论文的例子, 它讨论了如何减少激活内存. 我认为一件非常有趣的事情是, 当你把模型做得越来越大, 从左到右, 你会看到, 如果我们积极地进行并行化, 参数和优化器状态的内存可以保持不变, 但激活内存却在持续增长, 因为它的某些部分无法被干净地并行化. 所以, 无论你有多少设备, 你实际上都无法摆脱每个设备上激活内存的增长. 稍后我会告诉你们原因. 而如果你做一些更聪明的事情, 比如重计算(recomputation), 你就可以保持激活内存的低水平. 这对于并行化一些最大的模型来说是至关重要的. 

**说话人 1 01:05:45**
好的, 每层的激活内存是多少？你们之前已经做过一些 Transformer 的数学和微积分计算, 所以希望你们现在对这些都很熟悉了. 

**说话人 1 01:05:57**
但我们可以计算出每层需要多少激活内存. 这里有一个方便的公式. 这是你需要的内存量:`S * B * H * (34 + 5 * S / H)`. 其中一些数字可能令人费解, 但实际上它们并不那么神秘. 你可以很清楚地看到有一个左项和一个右项. 左项来自 MLP 和其他逐点(point-wise)操作, 这就是 `S * B * H * 34` 的来源. 这些都取决于你的残差流(residual stream)的大小, 也就是 `H`. 在右边, 你有一个项, 如果你把它乘出来, 就是 `S^2 * B`, 因为 `H` 被消掉了. 这是你在 Softmax 项和其他注意力机制中的二次项所需要的内存. 当然, 如果你使用 FlashAttention, 你可以通过重计算大幅减少这个第二项. 

**说话人 1 01:06:52**
那么, 假设我们做张量并行. 我们在所有可以的地方都做张量并行. 我们在 MLP 中做, 在 KQ 计算中做, 在注意力计算中做. 我们最终会得到这样的结果. 这看起来相当不错, 但还不完美. 每层的激活内存除以 `t`, `t` 是我们进行张量并行的设备数量. 所以如果我们除以 8, 理想情况下我们应该把所有的激活内存都除以 8. 但你会看到有一个落单的项, `S * B * H * 10`, 它没有被减少. 如果你想想这些是什么, 它们是非矩阵乘法的部分. 比如 LayerNorm、Dropout、以及 Attention 和 MLP 的输入. 所有这些项不幸地会随着规模继续增长, 并且它们不会被很好地并行化. 所以我们最后需要考虑的事情是, 把那些简单的逐点操作, 就是到目前为止我们还没有并行化的那些, 把它们也分割开来. 有一个非常简单的方法来分割它们, 那就是, 如果我们正在做一个 LayerNorm, 序列中不同位置的 LayerNorm 之间完全没有交互. 它们根本不关心其他任何事情. 所以我们将要做的是, 假设我们有一个长度为 1024 的序列, 我们将把它切分开. 然后每个设备将处理那个 LayerNorm 的不同部分, 或者那个 Dropout 的不同部分. 这些逐点操作现在可以完全沿着序列维度被分割开来. 因为我们现在是沿着序列维度切分, 我们将不得不做一些同步, 以确保我们所做的并行计算能够被重新聚合起来. 所以在前向传播中, 这些 `g` 将是 `All-Gather`, 而 `g_bar` 将是 `Reduce-Scatter`. 在反向传播中, 这两者是相反的. 在某种意义上, 两者之间存在一种对偶性. 我们在这里做的是, 对于 LayerNorm, 我们已经把东西分散开了. 所以我们将不得不把它们重新收集起来, 以便我们能做标准的计算. 然后, 每当我们到 Dropout 这一步时, 我们想把它们重新分散到我们拥有的并行组件中. 在反向传播中, 我们做的正好相反. 好的, 希望这很清楚. 

**说话人 1 01:09:13**
这是一个非常简单的想法. 我们只是在并行化我们之前未能并行化的最后一部分组件. 所以现在我们可以把所有这些不同的部分组合在一起, 最终得到……我们从这里开始, 完全没有并行. 然后我们做了张量并行, 这让我们能把所有不是逐点操作的部分除以 `t`. 然后如果我们应用序列并行的思想, 我们可以把这个组件再除以 `t` 一次. 然后, 我们可以做像激活值重计算这样的事情, 也就是 FlashAttention 的技巧, 来移除第二项. 你能轻易获得的最小内存占用将是底部的这个东西, 也就是 `S * B * H * 34 / t`. 如果你在查阅关于 Transformer 运算的不同公式, 比如“我用了多少激活内存？”, 你经常会看到像 `S * B * H * 34` 这样的东西. 然后如果你有 `t` 路张量并行, 就再除以 `t`, 因为这是你能轻易获得的该类内存的最小值. 好的. 关于序列并行和激活值有什么问题吗？是的. 

**说话人 2 01:10:18**
我想知道, 随着……我假设组合图会越来越像……组合图就像一个有向无环图(DAG), 对吧？GPU 之间的所有负通信会……

**说话人 1 01:10:34**
你是说, 如果我们有一个比单线性链更复杂的计算图, 那会成为一个问题吗？这是个好问题, 我没想过. 我猜不会. 至少对于张量并行来说, 它纯粹是按层操作的, 不关心依赖关系. 也许对于流水线并行来说, 如果有多于一个分支, 可能会有增加并行化的机会. 但我不太确定这一点. 

**说话- 2 01:10:59**
因为这里应该是线性的, 对吧？

**说话人 1 01:11:07**
是的. 好的, 很酷. 好的. 还有一些其他的并行策略我不会讲, 主要是出于时间和避免你们疲劳的考虑, 因为我想我已经带你们了解了一大堆关于如何做并行化的底层细节了. 第一个我想谈的是上下文并行(Context Parallel)或者叫环形注意力(Ring Attention). 你可能以前听说过环形注意力这个词. 这是一种基本上可以同时分割计算和激活值开销的方法, 用于计算非常大的注意力. 本质上, 你只是在不同的机器之间传递键(keys)和值(values). 每台机器负责不同的查询(query), 然后键和值会以一种环形的方式在机器之间传递, 以计算你的 KQV 内积. 这里很酷的一点是, 你已经知道怎么做了, 因为你已经为 FlashAttention 做过分块(tiling). 所以你知道注意力可以以这种在线的、逐块的方式计算. 这基本上就是环形注意力中发生的事情. 

**说话人 1 01:12:14**
另一件事, 既然你已经了解了张量并行, 那就很简单了, 那就是专家并行(Expert Parallelism). 你可以把专家并行看作几乎像是张量并行, 因为你也是把一个大的 MLP 分割成多个小的专家 MLP, 然后把它们分散到不同的机器上. 与专家并行的关键区别在于, 专家是被稀疏激活的. 所以你必须稍微考虑一下路由(routing)问题. 而且路由不会像我们之前在张量并行中看到的全员到全员(all-to-all)通信那样可预测, 因为现在, 可能某个专家会过载. 你的网络会变得稍微复杂一些. 但除此之外, 从概念上讲, 你和张量并行是处于同一个世界里的. 

**说话人 1 01:12:59**
好的, 总结一下我们讨论的所有内容, 我做了一个关于我们拥有的不同策略的小表格. 我们有 DDP 和 ZeRO Stage 1, 这是你做的朴素数据并行. 这里你每个批次都有一些开销, 没有内存扩展, 带宽属性尚可, 但你为了能做到这一点消耗了批次大小, 你需要大的批次大小才能有大的数据……. 然后你有 FSDP, 这有点像是 ZeRO Stage 1 的一个更好的版本, 因为你能获得内存扩展, 但你会在不同层之间付出一些开销. 所以现在你有更高的通信成本, 并且有可能存在导致利用率低的同步屏障. 流水线并行, 好的一点是, 我们不再依赖于每个批次的这个方面, 而且我们能获得线性的内存扩展, 但我们有另一个问题, 那就是它也消耗批次大小, 而且设置和使用起来非常麻烦. 所以很多人如果可能的话都喜欢避免使用流水线并行. 

**说话人 1 01:14:08**
最后, 张量并行在带宽和所需同步量方面成本非常高. 但它有一个非常好的特性, 就是对批次大小没有影响. 所以它像是你能使用的唯一一种在全局批次大小方面没有成本的并行策略, 这很好. 所以我们必须平衡一些有限的资源. 我们有内存, 这是一种资源. 我们有带宽和计算, 这是另一种资源. 然后我们有批次大小, 这是一种非传统的资源, 但你应该把它看作一种你可以花费的有限的东西, 用在这些的不同方面来提高你的效率. 

**说话人 1 01:14:46**
有一本非常好的关于 TPU 并行或者说 TPU 的书, 是谷歌出的, 我上周提到过. 实际上他们有一个非常好的并行部分, 里面有一张很棒的图, 我想在进入一些例子之前给你们看一下. 关键的量, 正如我之前所说, 是批次大小. 根据批次大小与你拥有的 GPU 数量的比率, 不同类型的并行会成为最优选择. 他们用了一些公式来计算这些模型中每种情况下的通信和计算量. 

**说话人 1 01:15:22**
这是一个简化的公式, 用来生成这张图. 你可以看到, 如果你的批次大小太小, 你有很多 GPU, 但批次大小真的非常小, 那你就不可能高效. 你总是受限于通信, 也就是这下半部分. 实际上, 你大部分时间都花在了通信上. 随着你获得越来越大的批次大小, 最终你可以达到一个点, 如果你混合使用 FSDP(也就是 ZeRO Stage 3)和 MP(在这里是张量并行), 你基本上可以达到一个计算受限(compute bound)的状态. 所以现在你不再是浪费你的浮点运算能力来等待通信了. 最后, 如果你达到一个批次大小很大的点, 那你就可以只用纯粹的数据并行, 比如纯粹的 FSDP 就能让你进入一个状态, 你花在计算上的时间高于花在通信上的时间. 所以, 如果你的批次大小足够大, 你就可以只用 FSDP. 

**说话人 1 01:16:23**
所以这很酷地展示了这个想法, 即你为什么要混合使用这些策略？你什么时候会混合使用它们？为什么批次大小是一种资源？希望这能以一种非常直观的方式向你展示这一点. 好的. 所以当你把所有这些组合在一起, 你就会得到人们所说的 3D 或 4D 并行. 我最近好像听到了 5D 并行的说法, 但我还不太确定第五个维度是什么, 我得去读读相关资料. 但现在你可以把它们都整合起来了, 不同的并行维度. 这是一个非常简单的经验法则, 我最初是去年查阅并整理的, 但事实证明今年还是一样的. 所以你现在可以遵循这个. 首先你必须把你的模型和激活值都装入内存. 如果你做不到这一点, 你根本无法训练. 所以这是一个必要条件. 所以在你的模型能装入内存之前, 我们必须分割我们的模型. 所以我们会做张量并行. 我们知道, 在每台机器的 GPU 数量范围内, 这样做非常高效, 非常快. 所以我们会先做张量并行到那个点. 

**说话人 1 01:17:23**
在那之后, 根据你是否愿意处理流水线并行或者你的带宽限制等因素, 你要么在机器之间使用 ZeRO Stage 3, 要么使用流水线并行, 直到你能把模型装入内存. 

**说话人 1 01:17:38**
现在, 在那之后, 直到你用完所有的 GPU, 你现在可以运行整个流程, 你唯一的目标就是增加你手头的总浮点运算能力. 所以你剩下的部分将通过数据并行来扩展, 因为数据并行在低带宽的通信信道上工作得很好, 而且它非常简单. 所以这将给你一种利用所有 GPU 的方法. 

**说话人 1 01:18:03**
现在, 如果你的批次大小非常小, 那么有一种方法可以用批次大小来换取更好的通信效率. 如果你还没有把你的批次大小这个资源用完, 你可以在你的设备上使用梯度累积. 这将让你有效地拥有更大的批次大小, 即使你的内存受限. 这将让你用批次大小换取更好的通信效率, 因为你跨机器同步的频率降低了. 

**说话人 1 01:18:32**
好的, 简单的经验法则. 无论你在做什么, 这都能让你以合理的效率训练模型. 为了让这个更具体, 我最后会讲几个例子, 快速过一下 2021 年 Megatron-LM 的一篇非常好的论文, 它用图片和大量的实验准确地展示了这些东西, 以及一些去年的模型. 这是一张大表格, 展示了他们如何训练从 17 亿到 1 万亿参数的模型. 他们在所有这些模型上都获得了很高的利用率. 你看到他们获得的理论峰值浮点运算性能的百分比, 范围从 40% 到 52%. 

**说话人 1 01:19:14**
这相当不错. 你可以看到张量并行从 1 开始, 然后他们最终增加到 8, 然后就封顶在 8 了. 所以他们是先使用张量并行. 然后流水线并行一直保持为 1. 但一旦模型变得足够大, 他们就无法容纳这些大模型了, 所以流水线并行必须增加以作补偿. 然后数据并行的大小基本上是从尽可能大开始, 然后慢慢减小. 因为, 随着我们增加流水线并行的量, 这在某种意义上消耗了批次大小. 所以如果批次大小被用于流水线并行, 你就不能有效地拥有那么大的批次大小了. 好的. 所以, 精心的 3D 并行将给你带来总浮点运算能力的线性增长. 所以你看到, 如果你做精心的 3D 并行, 你会看到每个 GPU 实现的总浮点运算能力非常平稳, 这意味着, 如果你增加更多的 GPU, 总的聚合吞吐量会呈线性扩展. 这很棒. 

**说话人 1 01:20:21**
8 路张量并行通常是最佳的. 你看到这是流水线并行大小和张量并行大小. 你会看到, 当批次大小为 30 或 128 时, 8x8 是最优的. 即使你的批次大小较小, 8 路张量并行仍然是最佳的. 激活值重计算可以实现更大的批次大小. 记住, 更大的批次反过来又能帮助你掩盖流水线并行的开销. 所以激活值重计算, 尽管需要更多的浮点运算, 但它是值得的. 我们已经在 FlashAttention 中看到过这个故事了. 好的, 最后一部分是关于最近的语言模型, 它们是怎么做的. 我翻阅了几篇论文, 看看人们的并行化策略是什么样的. 

**说话人 1 01:21:10**
在 Llama 的论文中, 他们对 70 亿参数的模型使用了 FSDP. DeepSeek 的第一篇论文使用了 ZeRO Stage 1, 并结合了张量、序列和流水线并行. 这是我告诉你们的标准做法. V3 实际上做了一些不同的事情. 他们使用了 16 路流水线并行, 64 路专家并行(这有点像张量并行), 然后用 ZeRO Stage 1 作为他们的数据并行策略. Yi, 另一个中国模型, 再次使用了 ZeRO Stage 1、张量和流水线并行. 而 Yi-Lightning, 因为他们在做 MoE, 所以用专家并行替换了张量并行. 

**说话人 1 01:21:50**
最后一件事, 如果你对最先进的、包含大量细节的分布式训练感兴趣, Llama 3 的报告读起来真的很有趣. 他们有很多关于如何做网络、发生了什么事情的细节. 你会再次看到我之前说过的那些东西. 你会看到 8 路张量并行, 你会看到 CP, 也就是上下文并行, 这只与长上下文训练相关, 也就是这最后一步. 所以你可以忽略那个. 你会在前两个阶段看到流水线并行和数据并行. 你甚至可以忽略第一阶段, 因为那是他们为了保持稳定而做的小批次大小训练. 如果你看他们关于如何制定并行策略的理由, 你会看到和我之前说的完全一样, 基本上就是, 好的, 你想按照 TP、CP、流水线并行、DP 的顺序来安排, 这个顺序是根据你需要的带宽大小来的, 其中数据并行可以容忍那些长的网络延迟, 因为你可以异步地获取分片的模型权重. 所以他们正在使用我告诉你们的一种策略来训练一些最大的模型. 

**说话人 1 01:22:58**
关于 Llama 3 的一个有趣的旁注, 你可能在传闻中或者和朋友的闲聊中听说过, 就是当你以巨大的规模训练模型时, 会有大量的 GPU 故障. 他们有 148 次由故障 GPU 引起的中断, 占了他们总中断次数的约 30%. 还有像机器的计划外维护这样的事情, 那就有 32 次不同的中断事件. 所以当你训练这么大的模型时, 我讲了算法, 但你也需要容错的架构来处理这类事情. 我还听说过各种各样的故事, 人们说更可怕的其实不是显式的模型失败, 而是数据损坏. GPU 可能会悄无声息地失败, 给你垃圾数据, 完全毁掉你的训练. 

**说话人 1 01:23:49**
好的, 最后一个例子是关于 Gemini 2 的. 我想用这个来结束, 因为这是一个 TPU 的例子. 他们用了 ZeRO Stage 3, 这大致相当于 FSDP, 然后他们做了模型并行和数据并行. 所以在这里, 正如我之前所说, TPU 让他们能把模型并行扩展得更远一些. 好的, 总结一下, 扩展到某个点以上就需要多 GPU、多节点的并行. 没有单一的解决方案. 所以你想结合所有三种方法来发挥各自的优势. 然后, 有一些简单且可解释的经验法则, 指导你如何在实践中执行这种并行. 谢谢. 