2025年7月14日 下午 10:12|1小时 19分钟 21秒
关键词:
same model、many models、linear model、simple model、parameters times、model size、train model、model training、good models、model design、large models、particular model、language modeling data、first time、small models、model flops utilization、preach model、model parallelism
文字记录:
说话人 1 00:05 
Okay, so last lecture I gave an overview of language models and what it means to build them from scratch and why we want to do that. Also talked about tokenization, which is going to be the first half of the first assignment.
说话人 1 00:18
Today's lecture will be going through actually building a model. We'll discuss the primitives in Pytorch that are needed. We're going to start with tensors, build models, optimizers, and training loop. And we're going to place close attention to efficiency, in particular how we're using resources, both memory and compute.
说话人 1 00:43
Okay, so to motivate things a bit, here's some questions. Okay, these questions are going to be answerable by napkin math. So get your napkins out. So how long would it take to train a 70 billion parameter dense transformer model on 15 trillion tokens on 1024H1 hundreds. Okay, so, you know, I'm just gonna sketch out the sort of the give you a flavor of the type of things that we want to do.
说话人 1 01:11
Okay, so here's how you go about reasoning it. You count the noble total number of flops needed to no train.
说话人 1 01:22
So that's six times the number of parameters times the number of tokens. Okay, and where does that come from? That will be what we'll talk about in this lecture.
说话人 1 01:32
You can look at the promised number of flops per second that H100 gives you, the MFU, which is something we'll see you later. Let's just set it to point five. And you can look at the number of flops per day that your hardware is going to give you at this particular MFU. So 1,024 of them for, you know, one day. And then you just divide the total number of flops you need to train the model by the number of flops that you're supposed to get.
说话人 1 02:07
Okay, and that gives you about 144. Okay, so this is very simple calculations at the end of the day, we're going to go through a bit more where these numbers come from and in particular, where the 6 times number of parameters times number of tokens comes from. Okay, so here's the question. What is the largest model you can train on H8H1HUNDREDS using atom w if you're not being to cover.
说话人 1 02:38
Okay, so h 100 has eighty gigabytes of hpm memory, the number of bytes per parameter that you need for the parameters, the gradients optimizer state is 16. And we'll talk more about where that comes from. And the number of parameters is basically a total amount of memory divide by number of bytes unique per parameter. And that gives you about 40 billion parameters. Okay, and this is a very rough because it doesn't take you into activations, which depends on batch size and sequence length, which I'm not really gonna talk about, but will be important for assignment 1. Okay, so this is rough back envelope calculation and this is something that you're probably not used to doing. You just implement model, you train it and what happens. But remember that efficiency is the name of a game.
说话人 1 03:34
And to be efficient, you have to know exactly how many ops you're actually expanding. Because when these numbers get large, these directly translate to dollars and you want that to be as small as possible.
说话人 1 03:46
Okay, so we'll talk more about the details of how these numbers arise. You know, we will not actually go over the transformer. So Tatsu's gonna talk at a over the conceptual overview of that next time.
说话人 1 04:03
And there's many ways you can learn about transformer if you ever already looked at it. There's assignment 1. If you do assignment 1, you'll definitely know what a transformer is.
说话人 1 04:13
And the handover actually does a pretty good job of walking through all the different pieces. There's a mathematical description. If you like pictures, there's pictures, there's a lot of stuff you can look on online. So, but instead, I'm going to work with simpler models and really talk about the primitives and the resource accounting piece. Okay, so remember last time I said what kinds of knowledge can you learn?
说话人 1 04:35
So mechanics in this lecture, it's going to be just Pytorch and understanding how Pytorch works at a fairly primitive level. So that's will be pretty straightforward. Mindset is about resource accounting. And it's not hard. It's just you just have to do it. And intuitions, unfortunately, this is just gonna be broad strokes of, you know, for now, actually, there's not really much into it.
说话人 1 05:00
That I'm going to talk about, in terms of how anything we're doing translates to good models. This is more about the mechanics and mindset. Okay, so let's start with memory accounting, and then I'll talk about compute accounting, and then we'll build up bottom up. Okay, so the best place to start is a tensor.
说话人 1 05:24
So tensors are the building block for storing everything in deep learning parameters, gradients, optimizer, stay data, activations. There's sort of these atoms. You can read lots of documentation about them. You're probably very familiar with how to create tensors. There's creating tensors different ways.
说话人 1 05:44
You can also create tensor and not initialize it and use some special initialization for the parameters to if you want. Okay, so those are, you know, tensors. So let's talk about memory and how much memory tensors to take up.
说话人 1 06:06
So every tensor that will probably be interested in is sort of a floating point number. And so there's many ways to represent floating point. So the most default way is flow 32 and float toy 2 has thirty two bits. They're allocated 1 for sine, 8 for exponent, and 23 for the, you know, the fraction. So exponent gives you dynamic range and fraction gives you, you know, different, basically specifies different values.
说话人 1 06:37
So flow 32 is also known as FP32 or single, you know, precision is sort of the gold standard in, you know, computing. Some people also refer to flow 32 as full precision. That's a little bit confusing because full is really, depending on who you're talking to. If you're talking to a scientific computing person, they will kind of laugh at you when you say flow 32 is really full because they'll use flow 64 or even more. But if you're talking about machine learning person, flow 32 is the max you ever probably need to go because deep learning is kind of sloppy like that.
说话人 1 07:17
Okay, so let's look at the memory. So the memory is very simple. It's determined by the number of values you have in your tensor and the data type of each value. Okay, so if you create a torch sensor of a 4 by 8 matrix, the default, it'll give you a type of flow 30 two.
说话人 1 07:36
The size is 4 by 8 and the number of elements is 32. Each element size is 4 bytes. 32 bits is 4 bytes and the memory usage is simply the number of elements times the number of size of each element. And that will give you 128 bytes. Okay, so this is should be pretty easy. And just to give some intuition, if you get the one matrix in the fee 4 layer of GPG3 is this number, by this number, and that gives you 2.3 gigabytes.
说话人 1 08:11
Okay, so that's one matrix is, you know, can these num matrices can be pretty big. Okay, so flow 32 is a default, but of course these matrices get big, so you naturally you want to make them smaller. So use less memory. And also it turns out if you make them smaller, you also make it go faster too. Okay, so another tie of representation is called flow 16. And as the name suggests, it's 16 bits where both exponent and fraction are shrunk down from 8 to 5 and 23 to ten. Okay, so this is known as half precision and it cuts down half the memory. And that's all great except for the dynamic range for these float 16 isn't great.
说话人 1 09:10
So for example, if you try to make a number like 10 e 1, e minus 8, inflow 16, it basically rounds out to zero and you get underflow. Okay, so the flow 16 is not great for representing very small numbers or very big numbers as a matter of fact. So if you use flow 16 for training for small models, it's probably going to be okay. But for large models, when you're having lots of matrices and you can get instability or underflow or overflow and bad things happen. Okay, so one thing that has happened which is nice is there's been another representation of B. Float 16, which stands for Brain Float.
说话人 1 10:03
This was developed in 2018 to address the issue that for deep learning, we actually care about dynamic range more than we care about this fraction. So basically, BF16 allocates more to the exponent and less to the fraction.
说话人 1 10:20
Okay, so it uses the same memory as floating point 16, but it has a dynamic range of flow 32. Okay, so that sounds really good. And it actually, the catches at this resolution, which is determined by the fraction, is worse. But this doesn't matter as much for deep learning. So now if you try to create a tensor with 1 E minus 8 and BF16, then you get something that's not zero.
说话人 1 10:51
Okay, so you can still dive into the details. I'm not gonna go into this, but you can stare at the actual full specs of all the different floating point new operations. Okay, so BF16 is basically what you will typically use to do computations because it's sort of, you know, good enough for fee before computation. It turns out that for storing optimizer states and parameters, you still need flow 32 for otherwise your training will go haywire. So if you're bored, so now we have something called FP8 or 8 bit. And as the name suggests, this was developed in 2022 by, you know, Nvidia.
说话人 1 11:40
So now they have essentially, if you look at FP and BF16, it's like this. And FP, wow, you really don't have that many bits to store stuff, right? So it's very crude. There's two sort of variants, depending on if you want to have more resolution or more dynamic range.
说话人 1 12:01
And I'm not gonna say too much about this, but FPA is, you know, supported by H100. It's not really available on a previous generation. But at a high level, you know, training with flow 32, which is, I think, you know, is an what you would do if you're not trying to optimize, you know, too much and it's sort of safe. It requires more memory. You can go down to FP8 or BF16 and, but you can get some instability. Basically, I don't think you would probably want to use a float 16 at this point for deep learning.
说话人 1 12:45
And you can become more sophisticated by looking at particular places in your pipeline, either for pass or backward pass or optimizers or gradient accumulation, and really figure out what the minimum precision you need at this particular places. And that's called gets into kind of mixed precision training. So for example, some people like to use flow 32 for the, you know, the attention to make sure that doesn't kind of, you know, get messed up of 4 simple fee 4 passes with matt malls BF 16 is fine. Okay, pause a bit for questions. So we talked about tensors and we looked at depending on how, what representation, how much storage they take.
说话人 2 13:37
Can you just clarify about the mixed position? Like when you would use 32 in the B4?
说话人 1 13:42
Yeah, so the question is, when would you use flow 32 or BF16? I don't have time to get into the, you know, the exact details and it sort of varies depending on the model size and everything. But generally for the parameters and optimizer states, you use flow 32. You can think about BF16 as something that's more transitory. Like you basically take your parameters, you cast it to be of 16, and you kind of run ahead with that model. But then the thing that you're gonna accumulate over time, you want to have higher precision.
说话人 1 14:21
Okay, so now let's talk about compute. So that was memory. So compute obviously depends on what the hardware, you know, is by default, tensors are stored in CPU. So for example, if you just in pi torch, say x equals torch at 0:30 two 32, then you'll put it on your CPU. It'll be in the CPU memory. Yeah, of course that's no good because if you're not using your GPU, then you're gonna be orders of magnitude too slow. So you need to explicitly say in Pytorch that you need to move it to the GPU. And this. This is, you know, it's actually, just to make it very clear in pictures, there's a CPU, it has RAM, and that has to be moved over to the GPU. There's a data transfer, which is cost, which takes some work, take some time.
说话人 1 15:20
Okay, so whenever you have a tensor in Pytorch, you should always keep in your mind where is this residing? Because just looking at the variable or just looking at the code, you can't always tell. And if you want to be careful about computation and data movement, you have to really know where it is. You can probably do things like assert where it is and various places of code just to document or be sure.
说话人 1 15:48
Okay, so let's look at what hardware we have. So we have in this case, we have one GPU. This was run on the H100 clusters that you guys have access to. And this GPU is, you know, h, 180 gigabytes of high band of memory. And there's a, you know, that it gives you the cache size and so on. Okay, so if you have, remember the X is on CPU, you can move it just by specifying, you know, 2, which is a kind of a general Pytorch function. You can also create a tensor directly in GPU, so you don't have to move it at all. And if everything goes as well, I'm looking at the memory allocated before and after. The difference should be exactly two 30 by 32 matrices of 4 byte floats. Okay, so it's a 1,9,2. Okay, so this is just a sanity to check that the code is doing what is advertised. Okay, so now you have your sensors on the GPU. What do you do? So there's many operations that you'll be needing for assignment 1 and in general to do any deep learning application and most sensors you just create by performing up and operations on other tensors and each operations has some memory and compute footprints.
说话人 1 17:32
So let's make sure we understand that. So first of all, what is actually a tensor in Pytorch, right? Tensors are a mathematical object and pi torch, there are actually pointers into some allocated memory. Okay, so if you have, let's say, a matrix, 4 by 4 matrix, what it actually looks like is a long array. And what the tensor has is metadata that specifies how to get to address into that array. And the metadata is going to be, you know, two numbers, a stride for each, or actually number of one number per dimension of the tensor in this case, because there's two dimensions, it's, you know, stride 0 and stride 1. Strides 0 specifies if you are in dimension 0 to get to the next row to increment that index, how many do you have to skip? And so going down the rows, you skip four. So stride 0 is four. And to go to the next column, you skip one. So stride one is 1. Okay. So with that, to find an element, let's say 1,2,1 comma 2, it's simply just multiply the indexes by the stride and you get to your index, which is 6 here. So that would be here or or here. Okay, so that's basically what's going underneath the hood for tensors. Okay, so this is relevant because you can have multiple tensors that use the same storage. And this is useful because you don't want to copy the tensor all over the place. So imagine you have a 2 by 3 matrix here. Many operations don't actually create a new tensor. They just create different view and doesn't make a copy. So you have to keep make sure that your mutations, oh, if you start mutating one tensor, it's gonna cause the other one to mutate. Okay, so for example, if you just get row 0, okay, so remember why. Is this tensor and sorry, X is 1,2,3,4,5,6 and y is x zero, which is just the first row. Okay, and you can sort of double check.
说话人 1 20:12
There's this, you know, function and wrote that says if you look at the underlying storage, whether these two tensors have the same storage or not. Okay, so this definitely doesn't copy the tensor. It just creates a view. You can get column 1. This also doesn't copy the tensor. Oops, don't need to do that. You can call a view function which can take any transfer and chain, look at it in terms of a, the different dimensions, 2 by 3. Actually, this should be maybe the other way around as a 3 by 2 tensor. So that's also doesn't, you know, change, do any copying. You can transpose that also doesn't copy. And then, you know, just like I said, if you start mutating X, then y actually gets mutated as well because X and y are just pointers into the same underlying storage. Okay, so things are, one thing that you have to be careful of is that some views are contiguous, which means that if you run through the tensor, it's like a just slide going through the this array in your storage, but some are not. So in particular, if you transpose it now you're you know, what does it mean when you're transposing and you're sort of going down now? So you're kind of, if you imagine going through the tensor, you're kind of skipping around. And if you have a non contiguous tensor, then if you try to further view it in a different way, then this is not gonna work. Okay. So in some cases, if you have a non contiguous tensor, you can make it contiguous first and then you can apply whatever viewing operation you want to write. And then in this case, X and y do not have the same storage because contiguous in this case makes a copy. Okay, so this is just ways of slicing and dicing. Intenser views are free, so feel free to use them. Define different variables to make your. It's sort of easier to read your code because if they're not allocating any memory. But you know, remember that contiguous or reshape, which is basically contiguous top view, can create a copy. And so just be careful what you're doing. Okay, questions before moving on? All right. So hopefully some, a lot of this will be reviewed for those of you have kind of done a lot of Pytorch before, but it's helpful to just do a systematically make sure we're on the same page.
说话人 1 23:12
So here are some operations that do create new tensors. And in particular, element wise operations. I'll create new tensors obviously because you need us somewhere else to store the new value. There's a, you know, triangular U is also of element operation that comes in handy when you want to create a causal attention mask, which you'll need for your assignment. But nothing is interesting that interesting here. Okay, so let's talk about map malls.
说话人 1 23:49
So the bread and butter of deep learning is matrix multiplications. And I'm sure all of you have done a matrix multiplication, but just in case, this is what it looks like. You take a 16 by 32 times a 30 by 2 matrix, you get a 16 by 2 matrix. And, but in general, when we do, you know, our machine learning application, all operations are you want to do in a batch. And in the case of language models, this usually means for every example in a batch and for every sequence in a batch, you wanna do something. Okay, so generally what you're gonna have in instead of just a matrix is you're gonna have a tensor where the dimensions are typically batch sequence and then whatever thing you're trying to do, in this case, it's a matrix for every token in your and your dataset. And so, you know, Pytorch is nice enough to make this work well for you. So when you take this for, you know, dimension tensor and this matrix, what actually ends up happening is that for every batch, every exam. In every token, you're multiplying these two matrices. Okay? And then the result is that you get your resulting matrix for each of the first two elements. So this is just like, there's nothing fancy going on, but this is just a pattern that I think you is helpful to, you know, think about. Okay, so I'm gonna take a little bit of a digression and talk about INOPS. And so the motivation for INOPS is the following. So normally you in Pytorch, you define some, you know, tensors and then you see stuff like this where you take Xin, multiply by y, transpose minus 2-1. And you're, you kind of look at this and you say, okay, what is minus 2? Well, I think that's this sequence. And then minus 1 is this hidden because you're indexing backwards and it's really easy to mess this up because if you look at your code and you see minus 1-2, you're kind of, if you're good, you write a bunch of comments. But then the comments are can get out of date with a code and then you have a bad time debugging. So the solution is to use Iron Ops here. So this is inspired by Einstein's summation notation. And the idea is that we're just gonna name all the dimensions instead of, you know, just relying on indices, essentially. Okay, so there's a library called Jack's typing, which is helpful for as a way to specify the dimensions in the types. So normally in Pytorch, you would just define, write your code, and then you would comment, oh, here's what the dimensions would be. So if you use jacks typing, then you have this notation where as a string, you just write down what the dimensions are. So this is a slightly kind of more natural way of documenting.
说话人 1 27:06
Now, notice that there's no enforcement here, right? Because Pytorch types are sort of a little bit of a lion and Pytorch so can enforce, you can use a checker, right? Yeah, you can raise a check, but not by default.
说话人 1 27:27
Okay, so let's look at the, you know, Einstein. So Einstein is basically matrix multiplication on steroids with a good bookkeeping. So here's our example. Here we have X, which is, let's just think about this as you have a batch dimension, you have a sequence dimension, and you have 4 hidden and why is the same size you originally had to do this thing. And now what you do instead is you basically write down the dimensions, names of the dimensions of the two tensors. So batch sequence 1, hidden, batch sequence 2, hidden, and you just write what you dimension should appear in the output. Okay, so I write batch here because I just want to basically, you know, carry that over and then I write C1 and C2. And notice that I don't write hidden and any dimension that is not named and output is just summed over. And any dimension that is named is sort of just iterated over.
说话人 1 28:36
Okay, so once you get used to this, this is actually very, you know, helpful. I mean, it maybe looks, if you've seen this for the first time, it might have seemed a bit, you know, strange along, but trust me, once you get used to it will be better than doing minus 2-1.
说话人 1 28:53
If you're a little bit, you know, slicker, you can use dot to represent broadcasting over any number of dimensions. So in this case, instead of writing batch, I can just write dot, dot dot. And this would handle the case where instead of maybe batch, I have batch 1, batch 2, or some other arbitrary long sequence. Yeah, question is for compile this, like, is it guaranteed to compile to like, I guess so the question is, is it guarantee to compile to if something efficient this, I think the short answer is yes. I don't know if you have any, you know, nuances.
说话人 2 29:38
Figure out the best way to reduce, the best order of dimensions to reduce and then use that. If you're using within torch compile, only do that one time and then, you know, reuse the same implementation over and over. Yeah.
说话人 1 29:54
Okay. So let's look at reduce. So reduce operates on one tensor and it basically aggregates some dimension or dimensions of the tensor. So if you have this tensor before you would write mean to sum over the final dimension. And now you basically say, actually, okay, so this replaces with some, so reduce. And again, you say hidden and hidden is disappeared. So which means that you are aggregating over that dimension. Okay, so you can check that this indeed kind of works over here.
说话人 1 30:39
Okay, so maybe one final example of this is sometimes in a, you know, tensor, 1 dimension actually represents multiple dimensions and you want to unpack that and operate over one of them and pack it back. So in this case, let's say you have batch sequence and then this 8 dimensional vector is actually a flattened representation of number of heads times some, you know, hidden dimension. Okay, so and then you have a wave, a vector that is needs to operate on that hidden dimension. So you can do this very elegantly using INOPS by re calling rearrange. And this basically, you can think about it, we saw view before, it's kind of like kind of a, you know, fancier version, which basically looks at the same data, but, you know, differently. So here it basically says this dimension is actually heads in hidden one. I'm going to explode that into two dimensions. And you have to specify the number of heads here because there's multiple ways to split a number into, you know, too.
说话人 1 31:59
Let's see, this might be a little bit long. Okay, maybe it's not worth looking at right now. And given that X, you can perform your transformation using Einstein. So this is something hidden 1, which corresponds to X, and then hidden 1, hidden 2, which corresponds to w. And that gives you something hidden to. Okay, and then you can rearrange back. So this is just the inverse of breaking up. So you have your two dimensions and you group it into one. So that's just a flattening operation. That's, you know, with everything, all the other dimensions kind of left along.
说话人 1 32:50
Okay, so there is a tutorial for this that I would recommend you go through and it gives you a bit more so on, you don't have to use this because you're building it from scratch. So you can kind of do anything you want. But in Assignment 1, we do give you guidance and it's something probably to invest in. Okay, so now let's talk about computation. No cost of 10 tensor operations. So we introduce a bunch of operations and you know how much do they cost. So a floating point operation is any operation floating point, like addition or multiplication. These are them. And these are kind of the main ones that are going to, I think, matter in terms of flop count. One thing that is sort of a pet of mine is that when you say flops, it's actually unclear what you mean. So you could mean flocks with a lowercase s, which stands for a number of floating operations. This measures amount of computation that you've done, or you could mean flops, also written with a uppercase s, which means floating points per second, which is used to measure the speed of hardware. So we're not gonna in this class, use uppercase s because I find that very confusing and just write slash s to denote that is floating point per second. Okay. Okay. So just to give you a some intuition about flops GBD3 took about three 23 flops. GBD4 was two e 25 flops speculation. And there was a US executive order that any foundation model with over 2026 flops had to be reported to government, which now has been revoked. But the EU as a still. They're going still has a something that's hasn't the EUAI Act, which is 1E25, which hasn't been revoked.
说话人 1 35:09
So, you know, some intuitions, A100 has a peak performance of 312 tariff flop per second. And H100 has a peak performance of 1979 teraflop per second with sparsity and a approximately 50% without. And if you look at, you know, the Nvidia has these specification seat sheets. So you can see that the plots actually depends on what you're trying to do. So if you're using Bf FP32, it's actually really bad. Like the if you run Fe thirty two on H100, you're not getting, it's orders of magnitude worse than if you're doing FP16 or, and if you're willing to go down to FP8, then it can be even faster. And you know, for the for my first read that I didn't realize, but there's an asterisks here. And this means with scarcity. So usually you're in a lot of the major matrices we have in this class are dense. So you don't actually get this. You get something like, you know, exactly half. Okay. Okay.
说话人 1 36:33
So now you can do a backend level of calculations. 8, eight, one hundreds for two weeks is just, you know, eight times the number of flops per second times the number of seconds in a week. Actually, this is, this might be one week. Okay, so that's one week and that's 4.7 times e to the 21, which is, you know, some number and you can kind of contextualize the flop counts with other model counts. Yeah, absolutely. So that means if, so what does sparsity mean? That means if your matrices are sparse, is.
说话人 2 37:14
It specific like structured sparsity? It's like 2 out of 4 elements in each like group of 4 elements is zero. That's only case. If you get that speed, no one uses it.
说话人 1 37:26
Yeah, it's a marketing. The pharmacies is okay. So let's go through a simple example. So remember, we're not gonna touch the transformer, but I think even a linear model gives us a lot of the building blocks and intuitions. So suppose we have endpoints, each point is d dimensional and the linear model is just going to match map each d dimensional vector to a k dimensional vector. Okay, so let's set some number of points is B dimension is d K as the number of outputs. And let's create our data matrix x, our weight matrix w. And the linear model is just some map mall. So nothing, you know, too interesting going on.
说话人 1 38:20
And, you know, the question is how many flops was that? And the way you would you look at this is you say, well, when you do the matrix multiplication, you have basically for every IJK triple, I have to multiply two numbers together. And I also have to add that number to the total. Okay, so the answer is two times the basically the product of all the dimensions involved. So the left dimension, the middle dimension, and the right dimension. Okay. So this is something that you should just kind of remember if you're doing a matrix multiplication, the number of flops is two times the product of the three dimensions. Okay, so the flux of other operations are usually kind of linear in the size of the matrix or tensor. And in general, no other operation you encounter deep learning is expensive as matrix multiplication for large enough matrices. So this is why I think a lot of the napkin math is very simple because we're only looking at the matrix multiplations that are going are performed by the model.
说话人 1 39:51
Now, of course, there are regimes where if your matrices are small enough, then the cost of other things starts to dominate. But generally, that's not a good. Regime you want to be in because the hardware is designed for big much versus multiplication. So sort of by, it's a little bit circular, but by kind of we end up in this regime where we only consider models where the mammals are the dominant, you know, cost. Okay, any questions about the, this number two times the product of the three dimensions? This is just a useful thing.
说话人 2 40:28
Of negative motivation. Always be the same because a chip might have optimized that.
说话人 1 40:39
Yeah, so the question is like, is the, does this essentially, does this depend on the matrix multiplication, your algorithm in general? I guess we'll look at this the next week when we, or the week after when we look at kernels. I mean, actually there's a lot of optimization that goes underneath, under the hood when it comes to matrix multiplications. And there's a lot of specialization depending on the shape. So this is, I would say this is just a kind of a crude, you know, estimate that is BA basically like the right order magnitude. Okay, so yeah.
说话人 2 41:25
Additions and modifications are considered equivalent.
说话人 1 41:28
Yeah, additions and multiplations are considered. So one way I find helpful to interpret this, so admin, this is just a matrix multiplication. But I'm going to try to give a little bit of meaning to this, which is why I've set up this as kind of a little toy machine learning problem. So B is really stands for the number of data points and DK is the number of parameters. So for this particular model, the number of flops that's required for Forward Pass is two times the number of tokens or number of data points times the number of parameters. Okay, so this turns out to actually generalize to Transformers. There's an asterisk there because there's, you know, the sequence length and other stuff, but this is roughly right for if your sequence length isn't too large. So, okay, so now this is just a number of floating point operations, right? So how does this actually translate to a wall clock time, which is presumably the thing you actually care about? How long do you have to wait for your run? So let's time this. So I have this function that is just going to do it five times. And I'm gonna perform the matrix multiply operation. We'll talk a little bit later about this two weeks from now, why the other code is here. But for now, we get an actual time. So that matrix took, you know, point one, six seconds. And the actual flops per second, which is how many flops to do per second is 5.4 e 13. Okay, so now you can compare this with, you know, the marketing materials and for the A100 and H100. And you know, as we look at the spec sheet, the flops depends on the data type. And we see that the promise flops per second, which you know, for H100 for, I guess this is for float 32 is us, you know, 67, you know, terror flops as we looked. And so that is the number of promise flops per second we had. And now if you look at the, there's a helpful notion called model flops utilization or MFU, which is the actual number of flops divided by the promise flops. Okay, so you take the actual number of flops, remember, which is what you actually witnessed. The number of floating point operations that are useful for your model divided by the actual time it took, divided by this promise fault for a second, which is, you know, from the glossy brochure you can get of MFU of point eight. Okay, so usually you see people talking about their MFE. Use and something greater than point five is you usually consider to be good. And if you're like, you know, 5% MFU, that's things would be really bad. You usually can't get, you know, close to that, close to, you know, you know, 90 or 100 because this is sort of ignoring all sort of communication and overhead. It's just like the literal computation of a forms. Okay, and usually MFU is much higher if the matrix multiplications dominate. Okay, so that's and if you any questions about this?
说话人 2 45:40
Yeah, you're using the promise plot per second, not considering this person.
说话人 1 45:46
So this promise flop per sec is not considering this as much. One, a note is like, this is actually a, you know, there's also something called hardware, you know, to flops you like lization. And the motivation here is that we're all, we're trying to look at the, it's everyone, it's called model because we're looking at the number of effective, useful operations that the model is, you know, performing. Okay. And so it's a way of kind of standardizing. It's not the actual number of flops that are done because you could have optimization in your code that cache a few things or redo, you know, recomputation of some things. And in some sense, you're still computing the same model. So what matters is that you're, this is sort of trying to look at the model complexity, and you shouldn't be penalized just because you were clever in your MFU. If you were clever and you didn't actually do the flux, but you said you did, okay, so you can also do the same with BF16.
说话人 1 47:04
And here we see that for Bf, the time is actually much better, right? So point zero three instead of point one six. So the actual fox per second is higher. The even when accounting for a sparsity, the promise flops is still quite high. So the MFU actually lower for Bf 16, this is, you know, s maybe surprisingly low, but sometimes the promise flops is a bit of a, you know, optimistic. So always benchmark your code and don't just kind of assume that you're going to get certain levels of performance.
说话人 1 47:56
Okay, so just to summarize, matrix multiplations dominate the compute. And the general rule of a thumb is that it's 2 times the product of the dimensions flops. The flops per second, floating points per second depends on, you know, the hardware and also the data type. So the fancier the hardware you have, the higher it is, those, the smaller the data type, the usually the faster it is. And MFU is a useful notion to look at how well you've, you're essentially squeezing your hardware.
说话人 2 48:44
Yeah, I've heard that often in order to get like the maximum utilization, you want to use these like tensor cores on the machine. And so like if this Python is by default use these tensor cores on like, are these penetrations accounting for that?
说话人 1 48:58
Yeah, so the question is, what about those tensor cores? So if you go to this spec sheet, you'll see that, you know, these are all on the Tensor Core. So the Tensor Core is basically, you know, a, you know, specialize hardware to do matt malls. So if you are, yeah, if your, so by default, it should use it. And if you, especially if you're using Pytorch, you know, compile, it'll generate the code that will use the hardware properly.
说话人 1 49:44
Okay, so let's talk a little about, you know, gradients. So, and the reason is that we've only looked at matrix multiplication or in other words, basically feed forward passes and the number of flops. But there's also a computation that comes from computing ingredients, and we want to track down how much that is. Okay. So just to consider a simple example, a simple linear model where you take that the prediction of a linear model and you look at the MSE with respect to 5. So not a very interesting loss, but I think it's illustrative for looking at the gradients. Okay. So remember in the forward pass, you have your X, you have your W, which you want to compute the gradient with respect to. You make a prediction by taking a linear product and then you have your loss. Okay? And in the backward pass, you just call lots of backwards. And in this case, the gradient, which is this variable attached to the tensor is turns out to be what you want. Okay, so everyone has done yoke gradients in Pytorch before. So let's look at how many flops are required for computing gradients. Okay, so let's look at a slightly more complicated model. So now it's a two layer linear model where you have X, which is bid times W1, which is d by d. So that's the first layer. And then you take your hidden activations, h 1, and you pass it through another linear layer w 2, and to get a k dimensional vector and you do some compute some loss. Okay, so this is a two layer linear network. And just as a kind of review, if you look at the number of forward flops, what you had to do was you have to multiply, look at W1. You have to multiply x by W1 and add it to your H1. And you have to take H1 and W2 and you have to add it to your H2. Okay, so the total number of flops again is 2 times the project of all the dimensions in your map mall plus 2 times of product dimension 0 amount for the second matrix. Okay, in other words, two times the total number of parameters in this case.
说话人 1 52:46
Okay, so what about the backward pass? So this part will be a little bit more involved. So we can recall the model x to H1 to H2 and the loss. So in the backward path, you have to compute a bunch of gradients. And the gradients that are relevant is you have to compute the gradient with respect to H1, you know, H2,W1 and W2 of the loss. So d loss, the each of these variables. Okay, so how long does it take to compute that? Let's just look at W2 for now. Okay, so the things that touch W2, you can compute by looking at the chain rule. So W2 grad. So the gradient with of d loss DW two is you sum H1 times the gradient of the loss with respect to H2. Okay, so that's just a chain rule for W2. And this is, so all the gradients are the, you know, the same size as the underlying, you know, of vectors.
说话人 1 54:15
So this turns out to be essentially looks like a matrix, you know, multiplication. And so the same, you know, calculus holds, which is that it's two times the number of the product of all the dimensions, b times d times k. Okay, but this is only the gradient with respect to W2. We also need to compute the gradient with respect to H1 because we have to keep on back propagating to W1 and so on. Okay, so that is going to be your, the. The product of W2, you know, times your H2. Sorry, I think this should be that grad of h two h 2 dot grad. So that turns out to also be essentially, you know, looks like the matrix multiplication and it's the same number of flops for computing the gradient of each one.
说话人 1 55:31
Okay, so when you add the two, so that's just for W2, you do the same thing for W1. And that's which has d times fee parameters. And when you add it all up, it's, so for this, for W2, the amount of computation was 4 times B times d times K. And for W1, it's also 4 times B times d times d because W1 is d by D. Okay, so I know there's a lot of symbols here.
说话人 1 56:12
I'm gonna try also to give you a visual account for this. So this is from a blog post that I think may work better. We'll see. Okay, I have to wait for the animation to through back. So basically, this is one layer of the known out where has, you know, the hidden s and then the weights to the next layer. And so I have to, okay, promise this animation is up to wait.
说话人 1 56:41
Okay, ready, set. Okay, so first I have to multiply W and and have to add it to this. That's a forward pass. And now I'm gonna multiply this these two and then add it to that. I'm gonna multiply and then add it to that. Okay. Any questions in which the way to slow this down? But you know the details, maybe I'll let you kind of ruminate on, but the high level is that there's two times the number of parameters for the forward pass and four times the number of parameters for the backward pass. And we can just kind of work it out via the chain row here. Yeah, for.
说话人 2 57:25
The homeworks, are we also using, you said some titoration implementation is allowed, some isn't. Are we allowed to use grad or we're doing the like entirely by hand through ingredient?
说话人 1 57:38
So the question is, in the homework, are you going to compute gradients by hand? And the answer is no. You're going to just use Pytorch gradient. This is just to break it down so we can do the counting flops. Okay, any questions about this before I move on?
说话人 1 58:05
Okay, just to summarize. The forward pass is for this particular model is 2 times the number of data points times the number of parameters. And backward is four times the number of data points times the number of parameters, which means that total it's six times number data times parameters. Okay, and that's explains why there was at 6 in the beginning when I asked the motivating question.
说话人 1 58:31
So now this is for a simple, you know, linear model. But it turns out that many models, this is basically the bulk of a computation when essentially every computation you do has, you know, touches essentially a new parameters roughly. And, you know, obviously this doesn't hold. You can find models where this doesn't hold because you can have like one parameter through parameter sharing and have, you know, a billion flops. But that's generally what not what models look like. Okay, so let me move on.
说话人 1 59:16
So far, I've basically finished talking about the resource accounting. So we looked at tensors. We looked at some computation tensors, we looked at how much tensors take to store and also how many flops takes, tensors take when you do various operations on them.
说话人 1 59:35
Now let's start building up different, you know, models. I think this part isn't necessarily going to be no, that's, you know, conceptually, you know, interesting or challenging, but it's more for maybe just, you know, completeness. Okay, so parameters in high torch are stored is these. And then parameter objects. Let's talk a little bit about parameter initialization. So if you have, let's say, a, your parameter that has, okay, so you generate up, okay, do you, sorry, your w parameter is an input dimension by hidden dimension matrix. You're still in the linear model case. So let's just turn an input and let's feed it through the output. Okay, so ran and Union nocashen is, you know, seems innocuous. What happens when you do this is that if you look at the output, you get some pretty large numbers, right? And this is because when you have the number of grows as essentially the square root of the hidden dimension. And so when you have large models, this is going to, you know, blow up and training can be a very unstable. So typically what you want to do is initialize in a way that some, you know, invariant to hidden or at least, you know, when you guarantee that it's not gonna, you know, blow up. And one simple way to do this is just rescale by the 1 of a square root of number of, you know, of inputs. So basically, let's redo this. W equals a parameter where I simply divide by the square root of the input dimension. And then now when you feed it through the output, now you get things that are stable around, you know, this actually concentrate to, you know, something like normal 0,1. Okay, so this is basically, you know, this has been explored pretty extensively and deep learning literature is known up to constant as savior initialization. And typically, I guess it's fairly common. If you want to be extra safe, you don't trust the normal because it doesn't have, it has unbounded tails and you just say, I'm gonna truncate to minus 3 so I don't get any large values and I don't want any to mess with that. Okay. Okay. So let's build a, you know, just a simple model. It's gonna have dimensions and two layers. There's just, you know, I just made up this name, cruncher. It's a custom model, which is a deep linear network, which has an num layers. And each layer is a linear model, which has essentially just a matrix multiplication. Okay, so this parameters of these, this model is looks like I have layers for the first layer, which is a d by d matrix, the second layer, which is also d by d matrix, and then I have a, I had or a final layer. Okay, so if I get the number of parameters of this model, then it's going to be d square plus d. Okay, so nothing too surprising there. And I'm gonna move it to the GPU. So I run, want this to run one fast. And I'm gonna generate some random data and feed it through the data and the forward pass is just going through the layers and then finally applying the head.
说话人 1 01:04:04
Okay, so with that model, I would, let's try to, I'm gonna use this model and do some stuff with that. But just one kind of general digression, randomness is something that is, is sort of can be annoying in some cases, be if you're trying to reproduce a bug for example. It shows up in many places initialization, dropout, data ordering and just the best practices. I would recommend you always pass a fix, a random seed so you can reproduce your model, or at least as well as you can. And in particular, having a difference random seat for every source of randomness is, you know, nice because then you can, for example, fix initialization or fix the data ordering, but very other things, determinism is your friend when you're debugging and, you know, encode.
说话人 1 01:05:02
Unfortunately, there's, you know, many places where you can use randomness and just be cognizant of, you know, which one you're using and just, if you wanna be safe, just set the C2 for all of them.
说话人 1 01:05:19
Data loading, I guess I'll go through this quickly. It's, it's not, it would be useful for your assignment. So in language modeling data is typically just a sequence of integers because this is remember output by the tokenizer. And you serialize them into, you can serialize them into an Empire arrays.
说话人 1 01:05:44
And one, I guess, thing that's maybe useful is that you don't want to load all your data into memory at once. Because, for example, the llama data is 2.8 terabytes, but you can sort of pretend to load it by using this handy function called Mem Map. So which gives you essentially a variable that is mapped to a file. So when you try to access the data, it actually on the on demand loads the file. And then using that, you can create a data loader that, you know, is a, you know, just samples data from your batch. So I'm gonna skip over that just in interest of time.
说话人 1 01:06:33
Let's talk a little bit about, you know, optimizer. So we've defined our model. So there's many optimizers, just kind of maybe going through the intuitions behind some of them. So of course there's stochastic gradient descent. You compute the gradient of your batch, you take a step in that direction, no questions ask. There's a idea called momentum, which dates back to classic optimization, nestraff, where you have a running average of your gradients and you update against the, you know, the running average instead of your instantaneous new gradient. And then you have adigrad, which you scale the gradients by your, your, the average over the norms of your, or I guess not the norms, the square of the gradients. You also have RMS prop, which is an improved version of adigraph, which uses exponential averaging rather than just like a flat average. And then finally, Adam, which appeared in 2014, which is essentially combining RMS prop and momentum.
说话人 1 01:07:50
So that's why you're maintaining both your running average of your gradients, but also running average of your gradients squared. Okay, so since you're gonna implement Adam in homework 1, I'm not gonna do that. Instead, I'm gonna implement, you know, at a grad. So, so the way you implement an optimizer and, you know, Pi George, is that you override the optimizer class and you have to, let's see, maybe I'll and I'll get to the implementation once we step through it.
说话人 1 01:08:30
So let's define some data, compute the forward pass on the loss, and then you compute the gradients and then you when you call optimizer dot step, this is where the optimizer actually is active.
说话人 1 01:08:52
So what this looks like is your parameters are grouped by, for example, you have one for the layer 0, layer 1, and then the final no weights. And you can access a state, which is a dictionary from parameters to, you know, whatever you want to store as optimizer state, the gradient of that parameter you assume is already calculated by the BA, the backward pass. And now you can do things like you're in in at a grad, you're storing the sum of the gradient squares. So you can get that G2 variable and you can update that based on the square of the gradient. So this is element wise squaring of the gradient and you put it back into the state. Okay, so then your obviously your optimizer is responsible for updating the parameters. And this is just the, you know, you update the learning rate times the gradient divided by this scaling. So now this state is kept over across multiple invocations of, you know, the optimizer.
说话人 1 01:10:17
Okay, so and then at the, you know, end of your optimizer stuff, you can, you know, free up the memory just to, which is I think going to actually be more important when you look, when we talk about model parallelism. Okay, so let's talk about the memory requirements of the optimizer states. And actually, basically at this point, everything. So you need to the number of parameters in this model is d square times the number of layers plus d for the final head. Okay, the number of activations.
说话人 1 01:11:02
So this is something we didn't do before, but now, so for this simple model, it's fairly easy to do. It's just b times, you know, d times the number of layers you have for every layer, for every data point, for every dimension, you have to hold the activations for the gradients. This is the same as the number of parameters and the number of optimizer states. And for ad grad, it's, you'll remember we had to store the gradient squared. So that's another copy of the parameters. So putting all together, we have the total memory is assuming, you know, FP32, which means 4 bytes times the number of parameters, number of activations, number of gradients, and number of optimizer states. Okay. And that gives us, you know, some number, which is 496 here.
说话人 1 01:12:10
Okay, so this is a fairly simple calculation. In the assignment 1, you're going to do this for the transformer, which is a little bit more involved because you have to, there's not just matrix multiplations, but there's many matrices, there's attention and there's all these other things. But the general form of the calculation is the same. You have parameters, activations, gradients, and optimizer states. Okay, and the. So and the flops required again for this model is 6 times the number of tokens or the number of data points times the number of parameters. And you know, that's basically concludes the resource accounting for this particular model. And if for reference, if you're curious about working this out for Transformers, you can consult some of these articles.
说话人 1 01:13:14
Okay, so in the remaining time, I think maybe I'll pause for questions. And we talked about building up the tensors and then we built a kind of a very small model. And you know, we talked about optimization and how many how much memory and how much compute was required.
说话人 1 01:13:38
Yeah, so the question is, why do you need to store the activations? So naively, you need to store the activations because when you're where you're doing the paper pass the gradients of let's say the first layer depend on the activation. So the gradients of the I layer depends on the activation vision there. Now, if you're smarter, you don't have to store the activations or you don't have to store all of them. You can recompute them. And that's something a technical called activation checkpoint in which we can talk about later. Okay, so let's just do this quick, you know, train.
说话人 1 01:14:23
Actually, there's not much to say here, but, you know, here's your typical, you know, training loop where you define the model, you define the optimizer, and you get the data fee forward backward and take a step in a parameter space. And just it'd be more interesting. I guess next time I should show like an actual 1B clock, which I isn't available on this version. But so one note about checkpointing. So training language models takes a long time and you're certainly will crash at some point. So you don't want to lose your progress. So you wanna periodically save your model to disk. And just to be very clear, the thing you want to save is both the model and the optimizer. And probably you know which iteration you're on, I should add that. And then you can just load it up.
说话人 1 01:15:33
One maybe final note and is I alluded to kind of mix a precision your training, you know, choice of the data type has the different tradeoffs. If you have higher precision, it's more accurate and stable, but it's more expensive and low precisions, vice versa. And as we mentioned before, by default, the recommendations use flow 32, but try to use BF16 or even FPA whenever possible. So you can use lower precision for the fifth forward pass, but flow 32 for the rest.
说话人 1 01:16:15
And this is an idea that goes back to the, you know, 2017. There's exploring mix precision training. Pytorch has some tools that automatically allow you to do, you know, mix precision training because it can be sort of annoying to have to specify which parts of your model it needs to be, you know, what precision. Generally you define your model as, you know, sort of this clean modular thing. And that's specifying the precision is a sort of like a, you know, something that needs to cut across that. And one, I guess maybe one kind of general comment is that people are pushing the envelope on what precision is. Needed. There's some, you know, papers that show you can actually use FPA, you know, all the way, you know, through. There's, I guess one of the challenges is, of course, when you have lower precision, it gets very numerically unstable. But then you can do various tricks to, you know, control the numerics of your model during training so that you don't get into these, you know, bad regimes.
说话人 1 01:17:26
So this is where I think the systems and the model architecture design kind of are synergistic because you want to design models now that we have a lot of model design is just governed by hardware. So even the transformer, as we mentioned last time, is governed by having GPUs. And now if we notice that, you know, MVDA chips have the property that if lower precision, even like INT four, for example, is one thing. Now, if you can make your model training actually work on in 4, which is, I think, you know, quite hard, then you can get massive, you know, speed UPS and your model will be more, you know, efficient.
说话人 1 01:18:12
Now there's another thing which we'll talk about later, which is, you know, often you'll train your model using more sane, you know, floating point, but when it comes to inference, you can go crazy and you take your preach model and then you can quantize it and get a lot of the gains from very aggressive quantization. So somehow training is a lot more difficult to do with low precision. But once you have a train model, it's much easier to make it low precision. Okay, so I will wrap up there.
说话人 1 01:18:47
Just to conclude, we have talked about the different primitives to use to train a model, building up from tensors all the way to the training loop. We talked about memory accounting and flops accounting for these simple models. Hopefully, once you go through assignment 1, all these concepts will be really solid because you'll be applying these ideas for the actual transformer. Okay, see you next time.
