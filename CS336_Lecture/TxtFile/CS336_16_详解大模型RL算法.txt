2025年7月14日 下午 10:16|1小时 20分钟 15秒

关键词:
reward model、different rewards、other rewards、process reward models、thinking model、GRPO reward、baseline reward、other models、interesting things、good model、other things、same thing、length reward、model performance、true reward、math model、token reward、negative rewarding

文字记录:
说话人 1 00:00 
First, whatever remaining content we have. And then we'll talk about reinforcement learning from verifiable rewards, kind of the new exciting thing that's been happening over maybe the last six months or so and really powering all of these reasoning models that you kind of see dominating the landscape. So before I get into the verifiable rewards part, the exciting, you know, math RL content, I'm gonna go back and I'm gonna finish off RLHF, right?

说话人 1 00:28 
So this is a very quick recap of the last few slides from Tuesday's lecture. Sure. Right. So we want to do reinforcement learning from human feedback, which is the setting where we observe pairwise preference data, like which of the two responses are better. And we'd like to get a language model policy that sort of maximizes some sort of underlying reward for this preference data, right?

说话人 1 00:50 
And if you remember, DPO is this algorithm that allows to allows us to optimize this reinforcement learning object. And this subjective is hard because our language model, the policy, so to speak, is on the bottom of this expectation, right? You're sampling from your language model. You're not just maximizing the likelihood under the language model. And the way we're going to do this, you know, if you remember the derivation is that we're going to, you know, make the non parametric Assumption that our policy classes, the set of all functions, we're going to rewrite the reward as a ratio of the policies and then plug that in to the Bradley Terry objective.

说话人 1 01:26 
And so what we're gonna do is we're gonna find a policy such that sort of the implied reward maximizes the probability of seeing sort of the references, sorry, the pair wise preferences that we see, right? So now this is great because it's basically supervised learning on a kind of alternatively parameterized objective, which is nice. BPO updates have this following form. The reason why, you know, BPO has kind of took the world by storm for a bit a year ago or so is that it can be written in this very nice form where you know what you're going to do is you're going to take these gradient steps, which are going to be, you know, multiplied by beta. That's kind of your regularization.

说话人 1 02:07 
And you're gonna have higher weight when your reward estimates are wrong. So you're gonna update more when you're sort of implied rewards are not correct. And then you're going to increase the likelihood of the good example, and you're going to decrease the likelihood of the bad exam, right? As I was kind of saying, if you sort of remember what I was saying last lecture, really RL algorithms kind of boil down often to just upweigh the good stuff, downweight the bad stuff, and the subtlety really isn't deciding, you know, w how, what the good stuff is and how much to operate, right?

说话人 1 02:38 
And so this is one particular choice of doing that. And we'll see, you know, other things happen. And so DPO works pretty well for a while. Essentially, all the open model releases use some variant of a DPO to do their post training. And really it's just very easy to get working compared to PPO. And so for a while, it was really the dominant approach.

说话人 1 03:04 
In the time since then, there was a huge flood of papers that was like, you know, asterisk, Po, basically everyone wanted to come up with a DPO variant. There was dozens and dozens of papers. There are so many that I don't think it's worth it to go over the big ocean of such papers. I'll mention too, not because necessarily I think they're like the right ones, but because I think they're ones that have been used recently by people that are sort of really pushing the limits of this kind of DPO style post training. One is sympo and sympo just makes a very simple modification or two simple modifications.

说话人 1 03:39 
The first one is to really normalize the update size by the length of the responses. We'll see kind of this theme appearing later. And the other thing they do is they just get rid of the reference.

说话人 1 03:50 
So now we've lost the DPO sort of mathematical argument that, you know, what we're doing is looking at ratios of policies. But well, this is more purely looking at something, looks like something where we upweigh, you know, the good stuff and downweigh the bad stuff under that sort of underlying motivation. Sympoo, totally fine. There's other variants where you don't do necessarily this like reference policy removal where you just sort of normalize by length. I'm called length normalized DPO, that kind of people have done.

说话人 1 04:20 
And this, you know, these two forms DPO with length normalization and sympo were the things that were tried, you know, pretty extensively by AI two folks when they did 2 Lu 3. And I think one thing that I'll point out, so I'm gonna pause for a moment, cuz I feel like this is an important conceptual, not conceptual, just so important empirical point, which is that in RL, a lot of the findings are very tingent on the specific setting, right? So depending on the environment in which you run it, depending on the base model that you have, depending on the post training preferences that you're running on, you will find pretty different conclusions. And so one example of this, right, so, you know, a bunch of the I 2. 2 folks have been doing a lot of really good post training empirical work. And they had one work where they were sort of comparing DPO and PPO.

说话人 1 05:07 
And they had found that PPO was better than BPO because of maybe it's on policy ness and they show this, you know, this jump exactly is the DPO to PPO gap. And then it'll later work in 2 Lu three. They find actually if you do your SFT in a sort of nicer, better way, actually that just eats up all of the gains of both PPO and DPO. So neither of these really get any gains. And the only thing that does better is maybe DPO with normalization, right?

说话人 1 05:31 
Like pretty different conclusions. Of course, a lot is different between this left and right paper, but it's not that one is wrong and the other is wrong. It's really that you should be careful about reading too many like generalized conclusions about any of this stuff on the basis of one paper, right? So that's an important note. Even as I go and talk about, you know, PPO and I go and talk about GRPO later in this, you shouldn't really take any single experimental result necessarily as gospel.

说话人 1 05:56 
Okay, so the last thing I want to end with for RLHF is sort of two important things. And one of them will really just motivate, I think, the entirety of today's lecture. Sure. So I think that's important. So the first thing is over optimization.

说话人 1 06:10 
So in some sense, this is just overfitting and fancier naming, but it is a term that I think is very important because what it's saying is essentially, as you optimize your policy more and more, so think about this X axis as how much RLI did initially, your reward kind of goes up and up, but eventually, you know, the reward that reward models that you fitted on human preferences are diverging from, you know, real human preferences. And the more you optimize, you eventually kind of diverge out, right? Like you're not doing any better. You're just, you know, optimizing but not actually improving your rewards, right?

说话人 1 06:42 
So this over optimization thing appears basically everywhere in RLHF. It is a very big problem. And so that is kind of a concern. And over optimization is a phenomenon that really happens in many ways because of the noisiness of human preferences and the complexities of human preferences. So some of my students had a study where, you know, they basically did RLHF on human preferences.

说话人 1 07:06 
They did RLHF on noisy versions of, you know, AI feedback. And then they did rlhf on non noisy versions of human feedback. And you see clear over optimization phenomena, both for humans and for noisy AI feedback, but not really for sort of clean, noiseless AI feedback. And so really, you know, if you're gonna be post training, you should expect to see curves that look like things on the left. As you train your model to do better and better on your sort of proxy rewards that you've measured, you're not necessarily going to get better and better models in terms of sort of human preference win rates.

说话人 1 07:40 
Okay. And then the other thing, which is an important side note is that when you do RL, right, remember that I said that we're no longer in probabilistic world, right?

说话人 1 07:49 
So when you're doing supervised fine tuning or when you're doing pre training, it's quite clear that what you're doing is you're trying to do probabilistic modeling on some distribution, right? You're doing distribution matching for something, right? But RLHF is all, it's all just a policy. There's no underlying necessarily distribution. And how this manifests is that often you'll get much less calibrated models, right? A number of results across many different papers have shown that, you know, RLHF, the models often, especially at temperature 1, you know, show much more over confident behavior. So this one was from one of the anthropic papers. This one was from, I think, the GPT for release.

说话人 1 08:24 
This one was from a paper that one of my post docs did. In all these cases, essentially the RLHF models are much less calibrated. And maybe that's fine, right? Because calibration isn't part of the reward that you're putting in. So that's, you know, as designed. But you should be very careful thinking of these models as sort of calibrated probabilistic models, which you might be sort of tempted to do if you're kind of coming into this from a generative modeling that.

说话人 1 08:48 
So, okay, so up until now, we've been kind of thinking about RLHF and all of these things. Actually, I'll stop for a moment. If anyone has any final questions for RLHF, I'll answer them before I go on. Cuz we're gonna totally switch topics to RL from verified rewards in a moment.

说话人 2 09:05 
Yes. Do you mind to the human preferences graph? What's what are the two axes that you're looking at?

说话人 1 09:11 
This one. So the x axis is proxy reward. So here we're fitting like a reward model. We're not doing BPO here, right? So let's say we're doing something like PPO where we fit a reward classifier and the x axis is saying how well did my RL algorithm optimize that reward classifier, right? So this is success at RL is on the x axis, y axis is this is kind of like true win rates, either measured by, you know, AI feedback here or human feedback. So this is like real human votes, you know, on the y axis here. And so this is saying, you might think that succeeding at RL will make you succeed at the actual task that you know you're trying to optimize for. That's not the case. It will overfit at some point for at least human preferences. Yes. So the question.

说话人 2 09:58 
Objective. Why is it really bad for them? Not like beta log, a minus log, B minus log, or just like long. If you're.

说话人 1 10:12 
Are you just asking like, why don't they like pull the fractions out in the log or because.

说话人 2 10:18 
Computation thing that like when you take these two ratios, taking the long is easy to compute. And that's why.

说话人 1 10:23 
We want to be. No, I don't think that. I think taking ratio is actually bad numerically. So I think this is really just like a save the space, you know, the ratios, you know, are more compact and they're also intuitive objects, right? So the.

说话人 2 10:34 
Moment you  simple, it becomes a different form because you no longer have the same beta constant.

说话人 1 10:38 
Yes, that's right. CPO is just like a very different motivation for why you would do this, right? There's no ratios. It's a very fundamentally different object.

说话人 2 10:47 
Yes, for your showed like a more sort of packing thing on them. I was wondering like how you like how the authors measure like the background truth of like reward score, right? Cuz presumably.

说话人 1 11:00 
The reward function is a monitor like purpose, right? But it's kind of like a train test gap, right? The X axis here, you know, you have a train set, you fit a reward model on it, and you're measuring that fitted reward model, right? And the y axis is fresh samples from the true reward Oracle, right? And so they're measuring in expectation the same thing, but they're not measuring in finite sample the same thing. Like the same, this is the same as like train test gaps in machine learning, right? Same conceptual object. They are in expectation measuring the same thing for any finite sample. That's not okay. Okay, good.

说话人 1 11:34 
So I think you know all this questions about reinforcement learning from human feedback, I think brings up a good point, which is that, you know, human feedback is very difficult to optimize. It's very difficult to scale. And so you might wonder, are there other kinds of more effective RL that we can bring to bear on post training, right? And that's gonna get us here, right? So up until now, we've been kind of talking about chat GBD 3.5 and that era of models. And now I think we want to talk about 0,1 and the, you know, the new set of kind of reasoning style models, right? And the way we're going to get there is kind of this thinking of saying, okay, we have this very powerful tool now. We have this reinforcement learning tool that we can use for post training. And initially, you know, the instinct was to say, you know, there's one true objective and that objective is whether people like the bot, right? And so if we just optimize for the true objective we care about, that's all we need.

说话人 1 12:28 
Turns out to be very difficult, right? Human approval is easy to hack. It's hard to collect at scale. It's hard to RL at scale for all those reasons you see, you know, over optimization and all these kind of issues. On the other hand, we could instead turn to where RL has really dominated, right? Things like alpha go or alpha fold. And then, you know, looking at those domains, you might say, you know, what we really need is kind of a domain in which we know the true reward and for which we can evaluate such true rewards very quickly and efficiently and at scale, right? And if we could do that, then we can bring to bear all of the successes of reinforcement learning from the past into language model, right? So that's the overall mindset that we're adopting now, right? We have the tools that we built. Now let's apply them into a very kind of different way of using the same tools, right? Taking inspiration from successes in RM.

说话人 1 13:20 
Okay, and so today I'm gonna talk in two parts, right? So the first part is just gonna be an algorithmic part, right? So this part on the very top, I'm going to talk about different algorithms. First, I'm going to talk about PPO in more detail because, you know, I kind of cut that from last lecture for length reasons. And then I'm gonna transform PPO into GRPO, a simpler version in some sense. And then we'll talk through those objectives and various implementation details of grpo to get us to multiple like canonical variations of GRPL. And having done that, we will then walk through the three, I think, big reasoning model sort of training things that have been done as case studies since after part 1. You have all of the tools that you need to understand everything that is happening in Part 2, right? And after you do part 2, you'll understand at least how these three Chinese Open LMS were made. Okay, so now we are going to return to PPO. It is the thing that I didn't want to do, but we have to do in order for you to understand why GRPO exists, right? So what is PPO? At least in my sort of mental model of what PPO is, we start from the simplest possible thing and then we move, you know, slowly but surely downwards to ppl. So the simplest possible thing, I mentioned this in last lecture, this is the policy gradient, right? So on the left, I would like to optimize the expected reward under my policy P of theta, right? And we're going to optimize that through gradient, the sense. So I take the gradient of that object and I get the right hand side of that equation, which is the policy gradient, the expectation under my current policy of the reward. And I'm going to take gradient steps that sort of either increase or decrease.

说话人 1 15:00 
The probability according to the sign of the reward Z, right? Hopefully straightforward. If this is not familiar to you, you can sort of brush up on how that policy gradient derivation happens.

说话人 1 15:11 
Now, there are two things that we might want to think about here that are kind of inefficient. So the first thing is that this is what you might call purely on policy, right? So the way that the reinforced gradients work, I have to sample from P theta, and then I will immediately take a step on those sampled examples, right? And so every time that I want to take a gradient step, I have to compute a reward, right? And I have to do a rollout.

说话人 1 15:36 
You will sort of understand this in your assignment 5, but the expensive part of RL is the rollouts, right? We have to actually run the language model and get samples. And we know that's slow, right? From Percy's inference lecture, we know that that's very complicated. It's very tricky. We would much rather sample less frequently, right? We do a rollout once, we sample once, and then we take multiple updates on those rollouts. So that motivates something called TRPO, right? Where instead of taking updates from p theta, I would like to allow my sort of updates to go stale. So what I'm going to do is I'm going to sample from p theta old, this sort of base distribution down here, but still get sort of valid policy gradients. Well, how can I do that? Well, I can do what's called an important sampling correction. And then I can keep, you know, my policies close to my old one so that I don't get too far from my old one and get sort of crazy reward estimates, right? Very simple idea. Just do a little bit of off to on policy correction, you get something called TRPO. This a of t is a lower variance version of R of Z. I'm not gonna talk about advantage estimation in much more detail. PPO is a simple extra step beyond this. It says, you know, instead of doing KL divergences, I'm just gonna clip the advantages. And this is naturally gonna force my policy to remain close because if I go too far, then I'm not gonna be able to get higher and higher rewards. These rewards will just get clipped off at sort of a 1 minus epsilon or 1+ epsilon. There's an upper and bound to how much reward I can collect. So there's no incentive for the RL algorithm to make the policy really different from the current one, right? So if a soft version that you can say, think of kind of this idea, okay, so PPO, very successful RL algorithm used in many different places, you know, used in sort of very toy RL environments. It was used in OpenAI sort of Dota bot works very well in such actual RL tasks. Oh yeah, there was a video I forgot about that you can run around. Okay.

说话人 1 17:33 
And at a conceptual level, it's not that complicated, right? If you go look at the opening I documentation for PPO, it looks fairly simple, right? You see that equation that I listed above, it's this PPO clip objective. And really the only thing that's maybe slightly complicated is that this a of t is actually calculated by using something called a value function. So you need a second neural network to essentially compute kind of the expected rewards. And that would be used to, in some sense, lower the variance of my gradients, right?

说话人 1 18:04 
I'm not going to go into the details once again of sort of all the RL details, but, you know, the one implementation difference here that matters is that we need this value function and that will become important in a moment here. But PPO in practice is a very different beast from PPO in theory. And you know you're in for a very bad time if you know there's a blog post that says 37 implementation details of PPO. You know, I don't wanna know all 37 of those guys. And, you know, they, it's a, there's a huge long list of sort of different PPO variants in this blog post, and all of them have the different scores on the RL benchmarks. And there's a whole paper written on why implementation details matter in PPO and how if you really mess it up, you're not even computing the policy gradient correctly anymore, actually works better, right? So it's kind of a crazy place if you look at the implementation details of PPO. So we actually do have to maybe look at a live implementation detail. I'll actually go over quickly, cuz PPO isn't the point of this lecture, but what I want to get across to you is, you know, you all haven't been necessarily working on this post training space. Like why do people care so much about alternatives PPO? It is because of things like this, right? PPO works well, but it is kind of a beast.

说话人 1 19:21 
And this is one example, I think from the early days of RLHF reimplementations, there was this nice diagram describing, you know, this is how you would do us pretty standard implementation of PPO. This diagram is pretty gnarly, right? We've got a reward model for RLHF. We've got a value model that's supposed to keep track of kind of the expected reward. We've got generalized advantage estimation. Like I haven't even explained what that is. And that's gonna go into the policy language model. And we're gonna do, you know, our policy gradient updates. So there's all this sort of machinery that has to go into place for PPO to work. So we're gonna look at implementation.

说话人 1 20:00 
A few years back now, actually, some of my students and I did, you know, reimplementations of algorithms like PPO. Many other people have used it. So this is an implementation that's been somewhat tested.

说话人 1 20:10 
Not to say that this is a good implementation. I'm sure you know, something like more modern implementations from Volcano Engine and other things probably work a little bit better. But I just want to walk you through all the different components also, because as you write GRPO in your assignments, your structure will probably mirror some aspects of this, right?

说话人 1 20:32 
All RL algorithms have similar kind of outer loop. So if we look at this, right, so a PPO step looks very similar to a standard gradient update step in a language model. So this is the outer sort of loop what we're going to do is we're going to, you know, get a bunch of rollouts. And once we get, you know, a bunch of rollouts, what we're going to do is we're going to compute some losses, and then we're going to just take backwards and then gradient steps with, you know, clips norm and that's it, right? Very unintimidating.

说话人 1 20:58 
The outer loop of RL is usually fine, right? You know, lost computation is also basically the same thing. So this is probably small for those of you in the back. But if you look at this, you know what you have is one block that's computing the loss for the value function. Remember, PPO has both a value function and a policy. So we have to update both of those at once. So we're computing how close is the value function to kind of the actual sort of returns, the rewards that we saw, and we want to keep those close because the value function is supposed to provide variance reduction. And then we've got the actual sort of rewards that we're going to update on for PPO. And then we have sort of the clipping constants, 1 minus epsilon, 1+ epsilon. This code is literally just copying this back over. And just to give you sort of some, you know, intuition for hyper parameters, you know, a typical clip range might be something like point two here, right? So you would be allowed to go essentially from 0.8 to 1.2 in likelihood ratio between your old and new policy. The rollouts is where things start to get gnarly, but this is still essentially just calling inference. The left side is really just sampling, you know, rollouts from your current policy, and then you evaluate the log problems of your samples and you move on to the right. And what do you see? Well, the only subtlety here is that your value and your reward and your policy may have different tokenizers. So you gotta do retokenization. But otherwise, you just feed your rollouts into the model. So, okay, at this point, you're like looking at this code and you say, okay, that's fine. Like this is not so horrible. But the places where things start to get a little bit tricky is first we have to do some reward shaping, right? So what does that mean? So stepping slightly back, one thing that's weird about reinforcement learning for language models is if, you know, think about it from the RL perspective, if technically it's kind of like a contextual bandit. Well, what is a contextual bandit is something where you get an input and you have a bunch of possible, you know, actions to take and you immediately get a reward, right? In language modeling, you get a prompt, you can give an output and you immediately get a reward. There's no like state transitions, there's no environment exploration, there's none of this like complexity, right? Reward shaping here is essentially constructing per token losses in order to be able to give you something more easy to learn for the RL algorithm. And so what happens in practice for PPO and for GRPO, which you'll be implementing, is that the KL terms, essentially the regularization that we're applying is actually computed per token. Whereas the actual true reward, like the video complete the task or not kind of thing that's computed at the very last token, right? So you see that there's essentially a per token reward for the regularization, and there's a single terminal reward at the end for success or not. Yes.

说话人 2 23:47 
What does not work?

说话人 1 23:50 
Oh, right, right, right. This is on, this is a very, this is a kind of funny. This basically we're only adding per token KL penalty. But once again, this is one of those funny PPO implementation things.

说话人 1 24:03 
The second line here, this is the actual true KL calculation. But when this number goes too negative, it becomes numerically unstable. And you can actually hit that pretty often. And so this gets clamped off at zero, which you know, all of you know, if you clip off a log likelihood ratio at zero, that's not a kale anymore, but it is some approximation to a KL. Yes.

说话人 2 24:25 
Are so we've done really boss function. What are our update steps, especially as it relates to the reward. It's not the same.

说话人 1 24:38 
Yeah, yeah, good point. So the updates, you know, as I was saying in this outer loop is actually just taking gradient steps. So in terms of your code, it will look no different than just taking, you know, normal gradient steps.

说话人 1 24:50 
What is happening here though is in some sense, let's go back to the policy gradient equation here, right? If you write down this thing, this is R of. Z times P theta z. This is kind of like just taking gradients with respect to theta of RZ log p theta z. It's a weighted loss that you then take gradients of. And what you're not doing is technically, if you're taking a real gradient, you should also take a gradient into P theta. But you don't do that. There's kind of a implicit soft gradient. And so all you do is you compute this inner loss, you feed it to the auto grad, and you'll get the right steps for the ref for the policy grading. Yeah, you'll have to do that for the assignment as well. And we have like a little tutorial that explains kind of what's going on in case that quick explanation was not clear. Okay.

说话人 1 25:37 
And finally, the part that I think is the narliest is there's a, the box that I did not explain called generalized Advantage estimation, right? So what you need to do whenever you have a policy gradient, the variances of the gradient are often very high, right? And so you want to do as much variance reduction as you can. And instead of multiplying the gradients with the rewards directly, you can instead, you can show that the following a of t quantity, which is essentially a discounted advantage estimate, that's a sort of RL term, is an appropriate substitute. And so one of the things that PPO does that's very different is to use this advantage estimate where you can tweak lambda and gamma to trade off bias and variance of your gradients. One of the funny things though is despite kind of the annoyance of this implementation complexity of like maintaining this value function and estimating all these quantities. You can just pick gamma equals lambda equals 1, which reduces this whole thing basically to a baseline policy gradient. So your gut sticking r minus the implied value. And that works well too. And so the point of making you go through this was a lot of the implementations details for PPO are both simple in that it is just an outer loop RL. You do just take gradients and also kind of annoying, right? You have to do all these sort of clipping things.

说话人 1 26:55 
You have to think about what am I going to do with the generalized advantage estimate? What am I going to do to train the value estimate? But you expect to see, you know, overall increasing rewards, including reward models and sort of negative KL rewards kind of go down. This is a once again contextual bandit. So you expect to see pretty reasonable training curves, not like crazy RL ones. Okay, so I went through PPO. That was kind of a whirlwind tour, but I think hopefully you get the context of a, what PPO is, and b, that it is sometimes a little bit tricky to get working. And so, you know, this has motivated a lot of people to try to find alternatives to PPO. What we're going to want to do is to apply, you know, essentially these RL algorithms to settings where PPO applies and PPO applies very well to general RL settings where you have sort of rewards, but we don't want the complicated implementation.

说话人 1 27:51 
And maybe more importantly, we want to get rid of the value model. That is actually, if you try to implement PPO, really annoying, right? Because the value model is usually as big as your policy. So now in terms of GPU memory, you're paying twice the cost of your language model right now. You might say, okay, why can't we use DPO?

说话人 1 28:09 
Well, DPO is well suited for like pairwise comparisons, like Bradley carry comparisons. It's not so good if what you want to do is, let's say, do reinforcement learning on math questions and check whether or not the answers are correct, right? There's no pair wise structure, so to speak, inherently there. So maybe DPO is not great, right? DPO also, you know, originally is kind of an offline algorithm in a sense, and that it has a whole bunch of sort of pairs that you initially collect and you just update your model on those. You could make it online by iterating, but that's not usually the way in which people apply deep.

说话人 1 28:42 
So then now this brings us to the new hotness, which I think is GRPO, right? So GRPO is actually very simple, both in motivation and in actual implementation. So where you start with is conceptually you start at PPO, right? You start with very similar pieces. You think about kind of this clipping thing, you think about policy updates in very similar ways. But what you do is you remove the really complicated generalized advantage estimation. You just get rid of it completely and you replace it with something that is much simpler, right? So what is the much simpler thing? Well, we are going to replace the advantage, which used to be this like Gae thing. It was the sum over returns and had the value function in there. Instead, it's going to be this equation 3 at the bottom of this slide here.

说话人 1 29:32 
And what is this? The advantage of, you know, response I is equal to the reward that response I receives minus the mean of the responses within my group, and I'll define what a group is in a moment, and then divided by the standard deviation of the rewards within the group. So this is a Z score, if you know what that is, of the rewards within the group.

说话人 1 29:55 
Now what is a group is a very, in some sense, natural object for. Language Model RL, you have an input question. Let's say like solve this math problem, right? That is a group and I have many different candidate responses. I have capital G different responses. And those are all the responses within my single group, right? So the nice thing is, if you think about it, right, maybe problems are harder or easier, right? You know, some math problems are much harder than others. And because of that, the average reward that my sort of other samples receive is a natural baseline for myself. And people have explored exactly these kinds of algorithms.

说话人 1 30:33 
If you look up like reinforcement learning with leave one out, or sorry, policy gradients with leave one out. This is the leave one out baseline in action minus the standard deviation piece right here. The yes.

说话人 2 30:47 
For the group that you're talking about here, like, so you said it's a it's like they will have a batch of questions, right? And then their answers to it, which are real warded. So like, is each like answer only corresponding to one deep question or are we doing like multiple like kind of.

说话人 1 31:02 
Like trajectory result the same question for multiple message, right? You do multiple answers per question and that's how you get kind of variance production.

说话人 2 31:11 
Like multiple different questions. And also like, you know, like multiple answers for each of the questions is that we want to.

说话人 1 31:18 
No, so for each question queue, GRPO samples a group of output, right? So I mean, this is not batch. Like normally you would, if you were doing this for real, you would have multiple questions. And for each question, you'd have G responses and those would get baseline together across questions. You would have no baselining or any interaction really other than the policies got updated.

说话人 2 31:38 
Okay, so the baseline is only doing within the same price streams.

说话人 1 31:42 
And that's why the baseline makes sense, right? Because a question is like hard or easy and the mean of that reward isn't sometimes capturing the question difficulties and you're subtracting that guy. Okay, good.

说话人 1 31:53 
And the other thing, this is a fun note, I'm gonna mention it because I just kind of think this is fine and cool. This DKL, if you've seen lots of KL diversions, computations, this is actually a little non standard, right? Because the natural kale diversions estimate is actually just you take a bunch of samples and then you compute the average of the log ratio, which is kind of this inner term over here, right? But this one from GRPO actually has these two extra terms. It has the log or it has the ratio of pi ref over pi theta and has this minus 1. And you can kind of convince yourself that if you take the expectation of this with respect to Pi Theta, you know, this is just going to be canceling with this one over here. So this is a control variant scheme that reduces the variance of this KL divergence estimate, which is cool because maybe you all need to estimate kale divergence from samples. This equation too is just a slightly nicer way of estimating that exact same thing. Okay. And GRP is really nice.

说话人 1 32:54 
Oh, one last note. If you're only taking one step, which is doing the pure online case, right? All this clipping stuff just kind of disappears. And all you're doing is policy gradients, right? Policy gradient is just upweighting good stuff and downwaiting bad stuff, where the rewards that I'm multiplying the policy gradients by is this a of I. So it's just like an incredibly simple algorithm for the truly online case where you're not doing multiple steps on a single example. So there's multiple kind of different repositories for GRPO implementations. I can point you to several, including this one that I sort of copied and put onto this slide here. But basically, I can just say that it's exactly the way you think it would go. Sort of in the outer loop, you would compute the reward for each rollout. You'd normalize for each group the mean and the variance of the rewards. You compute the KL term, in this case per sequence, which isn't quite right for the more heavyweight implementations. And then you do grading updates on the loss. And this is, you know, one example of the loss computation that you also have here. And the advantage computation, unlike in the PPO case, is just really simple. This is almost exactly, you know, line for line, the equations that I showed you with just one minor difference that's not shown in the equation, which is that, you know, as you do in almost everything, because we're dividing by the standard deviation, you have a tiny fudge factor of 1 e negative 4 to make sure it doesn't numerically blow up on you. Right? And you'll have to do this in the assignment as well. You'll have to add a little epsilon tier grpo set up.

说话人 1 34:29 
So how well does this work? GRPO works pretty well. This is from the original Deep Seek math paper, and I'll get back to this later because it is interesting to look at this plot and this result in light of later deep seek R1 results. They show essentially the two fine tuning based methods, RFT and online RFT. This is basically a fairly weak baseline, I would say, where what you're doing is you're only looking at examples that sort of get the right answer. They're doing math. In this case. So you only get examples where you get the right answer and you fine tune on your own outputs that got the right answer, right? Reinforcing correct answers with fine tuning GRPO, with outcome level rewards where you only get a correct or not answer is the yellow. The blue one is process level rewards where it's kind of you got a system that looks at each step of your reasoning and sort of gives you a grade for that. And they're arguing, you know, maybe process rewards are better. We'll talk a little bit more about that later. But in either case, you see that GRPO works and it works pretty well. Okay, any questions about the basic GRPO piece before we move on to kind of details about GRPO and thinking kind of deeply about what's actually happening in the algorithm.

说话人 1 35:42 
Okay, so now let's think about the difference between GRPO and PPO and what we've done and what's different. So really there's, you know, as I was saying, only one difference, although it's a really important difference. It's replacing the advantage estimator with this thing, right? The mean or the Z score, let's say, of the rewards. And so now I'm gonna kind of go back to the policy gradient theorem or the policy gradient result. And I'm gonna think through this result with you, right? So when we take a policy gradient update, right, what can we do? So I'll just go back couple slides. Right on this slide, we've got right here at the very top, policy gradients. This is the most basic RL algorithm that you can do, right? You take gradients where you multiply the log probability gradients with the reward, right? This is something that I'm always allowed to do. This is a mathematical one. That's right.

说话人 1 36:40 
Now, one other thing I can do is called baseline. I can take this reward Z and I can subtract any constant or any, in fact, any random variable that doesn't depend on Z itself. And this would still be a valid policy gradient, right? So this baseline thing is really important because what you're gonna try to do is you're gonna try to subtract constants that give you lower variances on this expectation, right? So that's called baselining. And if we go here, you know, that's kind of a classic result that you can look up in sun and embargo. They say, okay, look, we've got the policy gradient. You can subtract out any baseline B of s. Because B of s, you know, when we sum it up across the policy is gonna be zero, right? So this is fine, right? We can always baseline. But let's look at this. A of a of I. Is this a baseline? Well, we're subtracting the mean and that's kind of a baseline. Cuz all the other rewards are not dependent on ROI. So maybe that's okay.

说话人 1 37:39 
I mean, technically this notation includes ROI, but if I remove that, that's a valid baseline. But one thing that's really weird is I'm dividing by the standard deviation here, right? That's not something that really seems allowed according to this derivation in Sun Nembardo. And that turns out to be a problem. Some folks that have gone and, you know, reanalyze GRPO and its behaviors basically argue that GRPO has two things that at least mathematically are a little bit off. And the first thing is this division by standard deviation, right? As I was kind of talking you through just now, this breaks that sort of, you know, contract that a baseline just needs to be subtracting a zero mean variable that's, you know, independent of my draw. And the other thing that GRPO does, which I did it kind of like, sorry, I glossed over when I previously presented it is that it's actually dividing kind of the rewards by the length of the output. And that's gonna have, that's also a little bit weird according to the policy gradient theorem, this is not something that would naturally show up.

说话人 1 38:46 
And so these authors who did like this pretty interesting study of GRPO algorithms kind of argue that maybe we should just get rid of these two things. And if you do, then you'll actually have much shorter output length and higher reward without having much longer responses. And so let's talk about these results, you know, carefully for each one of these two fixes. And hopefully by talking through these, you will gain an intuition about how the RL algorithm works, right?

说话人 1 39:16 
So first, I want to talk about the standard deviation. This one's maybe somewhat obvious what it's doing. Okay, let me go back cuz I think it's easier to talk about when I'm highlighting the equation here. So I'm dividing the advantage by the standard deviation. So what does that mean? When the standard deviation is small, right? The reward is gonna be amplified. It's gonna be more important for me to optimize that group when the standard deviations are small, right? And when is the standard deviation small? Well, it's when the problem is too easy or it's too hard, right? Because that's when the rewards are either all zero or they're all ones. And so there's a bias in the standard deviation term that upweights problems that are too easy or too hard. The authors argue this slows convergence, maybe. Crew, at the very least, it certainly breaks the validity of the policy gradient. The second thing, which is subtle but also interesting is the length normalization. Now let's kind of look at what's happening here. So we have this length normalization before the GRPO reward. Now what does that do? If my model got a question wrong, right? Then the best thing to do is I'm receiving negative rewarding here. So the best thing to do is to make the response really long. And if I get the answer right, the best thing to do is to make the answer short so I can sort of maximize my positive reward, right? So what this does is it actually produces a model that kind of BS is as aggressively as possible. If the model thinks it can't get the answer right, it just produces the longest possible response, which is a very bad incentive for you to give the model. And so if you fix this, what happens is, you know, on various sort of toy tasks like GSMA, K and so on, you can get a reward that's just as good. The red one is sort of the modified version, but the output length doesn't keep growing and growing and growing. It sort of like stabilizes a certain point, right? And so there is actually some interesting observation that like maybe some of the really long cots that people are seeing in things like GRPO are a result of these like actual implementation details and choices rather than inherently long cots being a necessary part of the performance of these models. And I think that's like a very interesting, although not fully proven out, a class of hypothesis. Cool. Okay, so that's the GRPO algorithm. Hopefully now you're all familiar with it. And I think you now have the background to go through all three of these papers now. Deep C car 1, Kimi 1.5. And any questions? Yes, also question about why like.

说话人 2 41:42 
Operating is bad. Just to confirm like my understanding, I guess it's bad because like, you know, in either the very easy or the very hard cases, we don't want aggressy, like aggressy update during.

说话人 1 41:52 
The model. Yeah, I think this gets into sort of wishy washy folk theorem territory, but actually some of these papers will talk about this folk theory. And so I'll mention it, which is what you really want the RL algorithm to do is to get problems that it can like do somewhat well on, like it can get some reward on, but it's not so easy that it can like already solve them, right? So there's like kind of a curriculum effect where you want to feed the models the right, you know, level of difficulty. And if you're maximizing that standard deviation, that's kind of the wrong direction, right? You're really maxing out the stuff at the extremes that you either already all know or just way too hard for you to solve. Cool.

说话人 1 42:32 
Okay, so we're gonna talk about all three of these papers today are 1 Kimi K 1.5 and Quinn 3, r 1 and K 1.5 I think are pretty interesting because they came out at roughly the same time. Sadly, R1 was the only one to get like a gigantic social, what's it called, reception. Both of these actually show how to do RL based reasoning on like math and other things with LLMs, right? And they, because they're contemporaneous, you can kind of see almost two parallel ways to tackle the same problem, like which things are similar, which things are different, and so on, which is great. Queen 3 is the newest of the releases and they do some fairly interesting variations of ideas in R 1. And also they have some new kind of tricks that R1 doesn't have, which I think are pretty interesting things to look at, especially if you're interested in things like inference efficiency of reasoning models. So I'll start with R1. I think R1 is, you know, kind of amazing in the sense that it's a, you know, archive paper that launched a whole social phenomenon. Never let your advisers tell you that, you know, your archive papers will never matter. You know, this one lost like, well, almost half a billion dollars of Nvidia evaluation. YouTube can one day maybe cause that kind of a wave.

说话人 1 43:54 
R1, I think is quite remarkable because, you know, it in many ways replicates all of the qualitative properties of the 0,1 recipe in a way that is extremely simple. So you know the key properties that I wanna talk through and you know, make sure you all understand the first thing right is that, you know, it hits the performance targets that open AIO1 set. Everyone's really excited about reasoning model. So this is very exciting. The second thing is that, you know, it opens up a RL recipe that is not only just like a replicable one, but I think more importantly, one that is extremely simple, right? It doesn't have any search, it doesn't have any process reward models. I think lots of people at the time thought maybe we need all these complicated pieces to get reasoning models. R1 really shows you don't need any of that, right? And then finally, there's lots of interesting insights about the interaction between supervised fine tuning mnrl that I think continue to be really important.

说话人 1 44:52 
Okay. So the starting point of R1 is they build on deep seek map and actually some of the. Equations I showed you of GRPO are from Deep Seek math, where they originally proposed GRPO as an alternative, simpler or system, more systems efficient variant of PPO, right? To them, actually, the most important piece was they wanted to get rid of the value model just because it's really annoying to have around.

说话人 1 45:17 
But one thing that's really interesting is, you know, they actually go for this yellow line, the outcome supervision, which is not actually the best performing model in deep seek math. I'll talk about that again at the very end of this section here. So I'll walk through all the different pieces of R1. So I'll start with R1,0, which I think of as the control setting, right? So R1,0 is a very pure form of RL learning. It basically takes essentially the model that is pre trained plus mid trained before doing any RLHF for instruction tuning and then throws it into the math RL loop. And then they try to find out how well does that do. So the details here, how do they do the reinforcement learning? Well, they have a bunch of sort of math ish tasks. The data is not public. They take deep seek v 3 as their kind of base model. And then their rewards, there's two forms of rewards that they use. One of them is an accuracy. So like, did it get the math question correct, right? It's a correct or not reward. It's binary. They have a format reward that basically forces the model to put its cot within like thinking tags, like thinking, start thinking, end of thinking tax, right? And that's important if you want your model to have these like long cots being used.

说话人 1 46:33 
Now the format reward feels like something that doesn't matter. But from many papers and from having talked to many people, apparently it is a pretty critical part of actually getting this whole like reasoning RL thing to work.

说话人 1 46:46 
Once you do this, right, all they're doing is doing RL on top of the base model. Nothing very fancy, but the results are pretty striking. They get performance that is getting pretty close to opening Io one by just doing some RL on top of the model that they already had, right? Without any like, you know, cot fine tuning or anything like this.

说话人 1 47:07 
And there's kind of two things that they know in their paper as being really interesting about R1,0. I mean, I want to talk about this because I think it's important to carefully examine what is happening in R1,0.

说话人 1 47:20 
So the first thing they say is, oh, it's very cool that if you just let the model do RL on this verifiable rewards, the length of the cot just kind of increases like pretty predictably. And you know, in commentary that I don't know if I necessarily agree with, you know, in the paper, they're like, oh, it's learning to solve harder and harder problems by thinking harder and harder.

说话人 1 47:38 
It's like, well, maybe they also, you know, point out it's kind of cool that, you know, they learn, you know, phenomena like backtracking. They call this the aha moment. I think much has been made about this in sort of public discourse that, wow, it's cool that RL training can give models these kind of emergent insights. I'll kind of refer you back to the Doctor Grpo paper, the one that was talking about the correction instead GRPO. And I think they have honestly pretty good and interesting arguments that both of these are not particularly interesting phenomena.

说话人 1 48:09 
Like, first of all, they argue that like the length just goes up because of the biased objective, if not because, you know, it's an inherently interesting object. And second, they argue, well, if you just run deep seek v 3 on a bunch of math questions, it'll also sometimes output things like, aha, I can do this or that, which is maybe not like a deeply new phenomenon that arises from RL. You know, both of these seem like given more recent evidence, kind of credible things that like maybe there's nothing like immerge it and special about R1,0, but it is actually working very well, right? It is a good math model. So R1,0, you can think of as kind of a research setting, right? They're taking a controlled model, they're doing something very controlled on top of it, which is math RL, and they get a good model out, right? But if you're trying to build a really strong model that you're gonna ship to the world, this is not what you're gonna do, right? You're gonna like basically do everything that you can to get the best model that you can, right? So what would you do in that sort of more unrestricted setting? Well, you're gonna maybe insert some supervised fine tuning. You're gonna take, you know, cots from, you know, some undisclosed source, and you're gonna fine tune your deep seek model on that before you do your RL. And after you do that, right, you don't want your model to be kind of like this math savant that can't do anything else. So you're gonna, you know, apply your usual post training pipeline on top of that to make sure that it can do all the other tasks that, you know, people normally want to use these models for. So this is kind of the pipeline differences. And so the key differences both within the pipeline and within the RL is they do SFT initialization to try to get the model to know how to do long cots without starting with RL. They add a language consistency reward in order to make sure that the chains of thought remain in a single language. And then they do, you know, a secondary RLHF stage kind of at the end. So I. I think this makes a lot of sense, right? Every time you want to do something advanced like reinforcement learning, you're probably gonna start by doing a little bit of supervised fine tuning. And so even in sort of reasoning models or long cot models like Deep C car 1, this is the case, right? You start with long cot supervised fine tuning data, and then you're gonna, you know, do RL. I will point out that the description of where they get this data and what this data is remains very vague. Like they don't tell us like what was the cot data derived from? How did they filter it? I don't really, I have any idea based on reading the R1 paper. The claimed benefit of this is that if you cot the model on, sorry, if you sft the model on long, let's say, English cots, then this gives you an interpretability benefit, right? As you do RL, you're not gonna get like weird gibberish. It's gonna kind of keep the model closer to these like more interpretable cots that you started out with. So, and that would be kind of good for users, right? Like as you're, you know, using a math model, it would be nice to see its reasoning as it goes. And an additional thing, right? So, so when they do SFT initialization, they use a ton of data. But one really interesting thing is that for a lot of models, even a tiny amount of SFT on these kinds of like long cot data can be good. One thing that some of my students in collaboration with Percy, we did was basically take a bunch of long cots from Gemini 2.0, flash thinking and, you know, fine tune Quan 2.5 and maybe surprisingly, you know, with just 1,000 examples, you know, you get really high, you know, math benchmark accuracy with just a little bit of long cot fine tuning. So I think both of these are really pointing to the fact that the base model already has a lot of kind of like thinking capabilities that you're just like kind of priming and extracting from the model. And after that, of course, you're gonna do RL, right? So after you've gotten the model set up with SFT, you know, as with kind of the instruction tuning and rlhf pipeline. So you start with SFT and then you do RL to basically get the model to actually optimize the rewards you're looking for. The RL part is basically the same as R1,0, not huge differences, but with a minor difference that you're gonna add a language consistency loss. And they have, I think this note is pretty interesting. This is a minor note, but I'll describe it anyway, where they basically say, you know, they add this like language consistency reward because, you know, during the training process, if they just let the model RL, they find that actually the cot will language mix, like it will switch between languages. And if many of you have kind of seen people playing with reasoning models, I've seen people on the internet post things like, oh, it's kind of weird that like GROK 3 like suddenly switches to Chinese and the cot and this is kind of consistent with those kinds of things that if you aggressively RL a model, like actually, you know, there's natural tendencies for models to language mix rather than staying in a single language. So it actually requires an additional reward to keep it in the single language. And then finally, after you've done RL on like math and other verifiable domains, you basically layer on the usual post training. So you do instruction tuning and then you do sort of the pairwise preference tuning afterwards, right? So they do an SFT step where they combine both reasoning data on non verifiable tasks, like write a proof of something, these are not verifiable. And then so they use their own model as a judge for whether or not they got the answer correct. They have non reasoning data, like sort of, you know, write a nice essay and they use the same SFT data set as what they use for deep sink B3. Now finally for RLHF, they actually still use GRPO for RLHF, which is kind of cool. They use the same RL algorithm for everything. And then they basically just follow the V3 RLHF pipeline. Like there's not really anything different for this post training. How well does it works? Very well. I think many of you probably experienced this as well, right? Like R1 was in many ways a shock because it matches the 0,1 performance, you know, really kind of across the board on a very simple recipe, right? Like as I describe this, I don't think any of you found any of it particularly surprising, but the outcomes kind of speak for themselves. You know, you've got sort of on the English tasks, basically, you know, tide or matching 0,1 on across the board, really slightly worse on code models, but very close really across all these different tasks. The final thing that the R1 paper showed is that you can take these big models and you can distill them into other models. So you can take your big deep seek R1 and you can take those chains of thought, like in their case, they take almost a million chains of thought and then they fine tune quen with those change of thought and they actually get big boosts in sort of math performance relative to the base model, which is only getting something like 50. Percent performance on Amy for the 30 two b model. So they get 25+ percent boost on this task, which is pretty surprising. Cool. And then finally, there's two, I think, interesting and good observations from R1. And I think scientifically, maybe this was the biggest contribution of R1 in a way. So I think R1 maybe had three contributions scientifically, right? One of it was it showed that outcome based rewards with GRPO works, right? It's like the positive proof. And then R1 also had two other kind of like negative results kind of contributions, and they are sort of contained in the very last part of the R1 report. And they basically say, okay, like we try two things, like pretty extensively, we tried PRMs and we also tried MCTs. And neither of those really helped us at all on like replicating something like 0,1, right? And so to get into a little bit of detail, right, prms are basically process reward models. Those are kind of like systems that can give you intermediate rewards on a proof, right? So when your model is giving a chain of thought, a PRM would be able to say, oh, you went wrong at this step in the middle, right? And obviously that is much richer and very powerful form of feedback. An RL algorithm can make really good use of a PRM, but unfortunately, it's also very difficult to get a PRM in the first place. And so, you know, R1UM and the deep seek math people, like they had gone down this road of doing PRMs for a while and they kind of concluded this doesn't work quite as well as outcome based rewards. And thus far, I think outcome based rewards remain the way that in which you would build these models. The second thing that I think hasn't really panned out is search based methods. I think lots of people were interested in search based approaches to reasoning. Thus far at least it hasn't really panned out in the same way that RL on outcome based rewards has, that remains, I think, kind of the strongest kind of baseline and system in this universe. Okay, so any questions about R1, like kind of their setup or any of the other findings? Yes.

说话人 2 57:05 
Yes. So this PR.

说话人 1 57:08 
Now, GRPO and PRM are kind of two different. Oh yeah, yeah, that's right. Yes. Okay, good. Sorry, I said I was gonna mention it, but I didn't. So that is totally on me. That's right. Exactly.

说话人 1 57:18 
And so I think, especially for the PRMs, I think it's really interesting and telling that in deep seek math, you know, they were very much convinced by sort of the strength of the PRMs, which is this blue line with PS. And then in R1, they, you know, concluded that actually this approach that had worked for deep seek math was not really gonna work for R1 and they had gone with outcome based rewards. Yeah, thank you for reminding me. I was about to forget my promise there. Yes.

说话人 2 57:51 
I guess these I'll understand or is absolutely actually know how it protects performance.

说话人 1 57:56 
Yeah, so yeah, it's in this note here. They basically say if you ablate away the language consistency experiment, it results in degradation of the model's performance. But they're gonna put it in any way because they prefer to have cots that are more readable to humans. It's an interesting tradeoff. There have been lots of research about whether cots are faithful, and they're not truly faithful. We kind of know that. But maybe it's better to have a slightly more faithful cot than to have the extra, I don't know, half percentage point performance on Amy. Yes.

说话人 2 58:31 
At the end. But if you if you what you care about is that someone can read it like.

说话人 1 58:38 
So I mean, I'm sure they can do like translations or other kinds of like post processing to make the cots better. And in some ways, you might think of OpenAI and these other vendors efforts to like summarize the cops as being very similar, right? Like because the Rossio keys are probably much messier and then they probably like sort of rationalize it away. That is one way and I think that is an effective way to get interpretability. But I do think if you're interested in somehow, if you aesthetically believe that the raw cot is very important for monitoring and in, you know, closer interpretability, then I do think you do want something like this, right? Cool.

说话人 1 59:17 
Okay, so now we're gonna move on to Kimi K 1.5. And why do we study this one? If you look at the time sensor, when R1 at K 1.5 are released, it's kind of, you know, contemporaneous and it achieves very similar results. It does so using outcome based rewards in RL, it doesn't use the same algorithm. It has kind of different details and different interesting insights. And so we can learn about kind of what's the same, what's different, and maybe, you know, what parts are maybe important in this process. So to, you know, just show you the headline result before we start, just to get you to believe that this is a paper that's, you know, worth being discussed here, right, Kimi, capable?

说话人 1 01:00:00 
1.5 is the dark blue bars here. You can see open AIS 0 one as the next highest bar. So they're, you know, beating or matching O1 across the board on a bunch of important tasks. And they do basically things similar to R1. So they do SFT, they do RL, they have a different RL algorithm, but they do RL. They also describe their dataset construction in a little bit more detail. And this actually gets used in Quin 3 later, so it's worth discussing. So let me talk through data. As Percy said earlier, maybe data is the most important thing in your whole pipeline. And so we should always pay attention when people talk about their kind of data curation, you know, strategies in like a large scale training paper.

说话人 1 01:00:45 
And so Kimi 1.5 does several things to try to curate their dataset. So what they first do is they try to balance across different domains. Like they have a automated, I'm guessing LM based tagging system to categorize basically math questions by different domains and disciplines. And then they kind of balance across these to try to get diversity across different domains. They exclude, even though these are verifiable, multiple choice and true false questions, because they argue that these are too easy to hack or to randomly guess. So they're only looking for verifiable answers that are kind of short and can be evaluated by things like Rex or LLM. And this is maybe the most interesting piece of the curation here. What they do is they take the model that doesn't do any reasoning, you know, their SFT model, and they have this model generate 10 answers and the pass rate is used in order to determine whether to include that example or not. And I think the exact thing they use for their latest later selection strategies, they only select examples that fail best of 8. So if they can get anyone out of a correct, then it's excluded for being too easy.

说话人 1 01:02:06 
SFT data similar to R1, very little description. Who knows where they got that from? They just say they do some prompt engineering. So clearly, it was distilled from something else, but we don't really know what they distilled it off of. So I'll talk about the RL algorithm. The Kimi one's kind of interesting. It's a different variation. It's actually maybe closer to DPO in a way, but you end up with a, you know, algorithm that I think you'll very much recognize. So you can kind of think of this in a very interesting way as conversion evolution of RL algorithms.

说话人 1 01:02:38 
So you start once again at the very top. This is our, you know, classic goal. You know, we're sampling from a dataset, we're sampling from our policy. We want to maximize rewards. We don't want to be too far from our base policy.

说话人 1 01:02:51 
So this is the KL regularizer. You know, if you remember our DPO derivation, you make a non parametric Assumption, you say pi star is the optimal policy is an arbitrary function. That means that the reward can be written as, you know, the log normalizer plus the ratio of policies, right? This is the exact same thing we did in BPO. And now, you know, in DPO, what we did was we took these rewards and we plugged them into the kind of the Bradley Terry preference function.

说话人 1 01:03:19 
We don't have this here, right? We don't, we're not doing pairwise preferences. So we don't actually take that step. Instead, we write down this equation and we say, we know that for the optimal policy, we're going to have Equality here. And so all we're gonna do is we're gonna make this a difference and we're gonna add a squared loss on top right. We're gonna try to drive the left and the right sides close by just adding a squared loss. It's a reasonable thing to do. People have done things like this before and this gives us our loss. It's basically trying to drive the right side and the left side of something that should be an Equality for the optimal policy close together. And well, this is a little bit of an exotic looking object or maybe exotic looking initially, but if you take the gradient, it just looks a lot like GRPO. You know, you've got your gradients of your policy, right? This is your policy gradient stuff. And you've got a baseline reward. And actually, what's the baseline reward? I'm just going to average the R within my batch, right? So here, this is actually doing something different. This is the normalizing constant that think over the batch. But you know, we're doing essentially similar kinds of baseline as GRPO. And we've got a slightly different like square of the log loss regularization to keep my policy close rather than doing clipping, right? So zooming out what's happening here, very similar to GRPO in that this first part is a baseline loss, but there's no standard deviation thing happening. The second part is analogous to the clipping that happens in GRPO, but instead of doing clipping, we're explicitly regularizing the policy, right? So we've got the same ingredients just in slightly different form. And so hopefully, you can see, you know that as long as you kind of have this policy gradient thing in the right baseline and something that looks like regularization, you can get a working.

说话人 1 01:05:00 
Our algorithm. The other thing that the Kimmy folks do, and in some ways, I think this is more forward looking or they got this more right than the R1 folks, which is that they realize that, you know, if you're shipping a reasoning model, what you really care about is inference cost. And if you care about inference cost, you had better try to control the length of your cots, right? If you have really long thinking chains, that's gonna cost you a ton of money or it cost your users a ton of money. And so instead of celebrating the really long cots, the Kimmy folks say we want to really compress the cops as much as possible while keeping performance high. And so they have this length reward thing here where what they're doing is for kind of each batch, they're looking at the maximum and the minimum lengths. And what you have is Lambda, where Lambda is roughly like, you know, where are you in the range of lengths within your back, right? So if you're at plus point five, you know, you're really short and negative point five, you are really long, right? And the reward is basically going to be lambda whenever you get the answer to be correct. So if your answer is correct, you're gonna incentivize yourself to be really short at the very shortest end of this range. Whereas if your answers are incorrect, you're in this bottom part of this length reward, which means that you're incentivizing the cot links to be roughly shorter than the center of the range of the rollouts.

说话人 1 01:06:26 
This is a somewhat funky loss to me to be honest. But you can kind of understand the dynamics of this loss, which is you're incentivizing sort of correct answers to be as short as possible, an incorrect answers are incentivized to be kind of average ish, right? And so you don't have as strong of an optimization pressure to push down incorrect answers, to be short.

说话人 1 01:06:48 
And one final note on this length stuff is the Kimi folks realize that if you add this reward early on in training, it stalls RL because, you know, it basically forces the model to say, I'm in a local minimum. I don't get any of my answers correct the best I can do is to have my cots really short. And then you just can't get out of that sort of local minimum. And so they actually only turn this on later on during training. So they kind of initially do a bit of unconstrained URL, and then they add this length forward in. And they also have additional cool details. Who knows how much of this stuff is necessary or important, but they actually have a whole curriculum set up. They basically have assigned difficulty labels. So the dataset just kind of like top down, like they just manually or via LMS kind of annotate the difficulty labels and then they go from easy to hard in that order. And then they also, as they're going on sample problems proportional to one minus success rate. So if you're 100% succeeding, you just never sample that question ever again. And for the rewards, they basically for code, they take problems with ground truth solutions and they generate a bunch of test cases. And for math, they basically use actually a reward model that's used to compare like ground truth human written answers to the LM output. So instead of using something like a Reg X or using SIMP I, which is what other people have done, the Kimmy folks actually use a model to do equivalence checking. It's kind of surprising to do this in a verifiable reward case, but, you know, they don't seem to have a problem. Their reward models are very accurate because it's really all it's doing is advanced string matching.

说话人 1 01:08:26 
One of the things that's really cool about the Kimi paper is that they also talk about kind of the infra issues that arise doing RL. I think, I don't think I've seen any of the other RL reasoning papers actually talk about systems almost at all. So it's nice to see that they're kind of talking about this. What their structure is, what their layout of this is. You'll have to deal with this in A5, like I think a very mini version of this as you implement things like rollouts in RL. But one thing I'll note, right, is, you know, why is RL so hard to make efficient? In many ways, it's harder then normal pre training to have your GPUs like fully utilized during RL. And the reason I think is because there's rollouts involve, right? So you have to be generating sequences. And whenever you're generating sequences, not only are you kind of, you know, slow in the sense of inference is slow, but also you have this other issue, which is that you have to switch from RL to inference and back, and you have to be passing sort of data back to the RL worker and the RL worker has to pass model weights to the inference server and vice versa. So you've got all this sort of message passing that can happen. And finally, this is unique to long cot models, but if you have really long cots, batches can become very uneven. So you have to handle that cleverly somehow.

说话人 1 01:09:49 
And so the Kimmy folks do this, you know, relatively nice but fairly standard thing of having different workers that are assigned to do the RL updates, having different. Current workers assigned to do inference and they have basically kind of message passing to be able to pass the weights to the inference worker. And the inference worker can basically make datasets for the RL worker. And you know, they have almost the same kinds of setups that you will have. They use VLM for inference, and they also do very, you know, advanced stuff where they have to have VLM with dummy weights and they have to kill it because of the complexities of doing kind of this passing of weights from one worker to another.

说话人 1 01:10:32 
Finally, I think one thing that I think is interesting that the Kimi people show in their results is they have like per iteration results where they show like as RL proceeds, like how does, you know, performance scale and they show really nice scaling, which is in the blue. And they also show the growth of the length of the responses. And much like in Doctor Grpo, because I don't think their RL algorithm is inherently sort of balance or sorry, bias towards longer responses. Most of the sort of responses actually sort of plateau out at a sort of target length, like we saw with Doctor Grpo, rather than grow unboundedly like we did with the vanilla grpl. Cool. Any questions on Kenny before we finally conclude with quentry?

说话人 2 01:11:23 
So back to like the RL seller in email, each part of key model. Yeah, I was wondering like, so I guess like that when you're like doing the rollouts, you're in like training, like the rollout, the inference is done by VRL app, right? Yes. And as an updated parameter, that would mean that like, you know, as they update the volume is they also need to think that to the vom influence through Mike Worker. And that's what this is for her. And this one.

说话人 1 01:11:52 
Yeah, so I think, you know, the most annoying part, at least with current libraries of this process is essentially that step of taking RL weights and putting it into vll. There is an experimental API that is supposed to allow you to use NCCL collective calls to shove a set of weights into VLM. But at least we were thinking about having you use that for the assignment, actually. But it has too many undocumented parameters for it to be a little bit mature. And so maybe next year's iteration, this will actually be, you know, a fairly mature technology. But right now, I think a lot of people do things like start a VLM with dummy weights. The weights then get loaded somehow into memory with sort of hacks on top of the yellow one. In each iteration, they often tear down VLM to make sure they can free the GPU memory fully. Yeah, there's a lot of like, I think, you know, the RL for LMS, I think is fairly new still. So I think the infrastructural support remains a little bit immature, but I think in maybe a year, things will actually just get much nicer. Question back there. You have.

说话人 2 01:12:56 
A lot of like accuracy rewards and like rewards and for other words, how are you supposed to combine them.

说话人 1 01:13:03 
Together? Yeah, so you how you combine the different rewards. I think this is one of the RL, not quite black magic, but really, you know, like RL magic things of like, you just tune weights like in all cases, all the rewards are just added together. But with weights and how are the weights determined empirically in order to maximize the downstream performance? Especially for things like format rewards, you can almost think of them as like shaping or like surrogate rewards. Like you don't necessarily really care about the formatting reward. It's more of a means to an end to get a good long cot within the tags that will get you the answer.

说话人 1 01:13:40 
Cool. Okay, so the final one I want to talk about is Quen 3. And thankfully, Quen 3 released their report before the end of the class. I get the include it. And you know, I think this is the most recent and modern of the RL for reasoning kind of models to have come out. And so we can kind of see how they've like built upon the previous works, like where they've changed things and they have actually pretty interesting scaling and data results that are new and unique. So the overall picture is very similar to what R1 and Kimi have done, right? So quen basically take their base models, they'll do a long cot SFT stage. So that's this first stage over here. They'll do their reasoning RL, they'll do something funky that I'll talk about later called Thinking Mode Fusion, and then they'll do rlhfrl. And then that's the model that they ship. Of course, they then distill that in various ways, but we can kind of forget about that for the moment, right? So we've already seen this in R1, right? RLHF comes after reasoning and then distillation comes, yeah, after. And we already know a lot of the playbooks, so I can actually go through this fairly fast. Much like Kimmy, they basically curate the data by filtering for difficulty using best event. So if your base model that hasn't been RL can already answer it, if you sample like n times, then you can just. Just get rid of it, right? They also do some like decontamination things where they remove things that are too similar to validation data. And then they also manually filter essentially their SFT set. So like for their initial SFT data for long cots, they like manually filter it for whether they are guessing or whether they actually got it right.

说话人 1 01:15:19 
The one thing that empirically I think is really interesting about the Qun 3 RL results is that they are actually doing this RL only on 3,995 examples, you know, which is a very small number of examples to be doing this on. And they get pretty good gains out of the RL process. And so you can view this as, you know, RL on verified rewards as being very efficient. You could also think of this as being analogous to many sample efficiency results in the past, like people have shown that you can instruction tuna model with very few samples or that you can distill long cot with very few samples. But that doesn't necessarily mean that it doesn't continue to scale, right? We don't really know why. What this does show is that even with very few examples, you can sometimes do RL, which is surprising and cool. So what is the two new sort of quan specific stuff that they do? The thing that they do is this thing called thinking mode fusion, which I think is kind of interesting. And where I think the field or the various trends are going is in controlling inference, right? So what they do is they want to have both a thinking model and a non thinking model in the same, you know, single set of parameters. So what do they do? Well, you know, after they train a model with RL, they have a model that can do thinking. And now they're gonna fine tune it again to do one of two things. They're gonna fine tune the model with some data that has a think tag and then it's gonna do the normal cot thing. And you can get this data from yourself, right? Like the original thinking model can generate this, or you can have a no think tag, in which case it should immediately kind of emit an answer. And, you know, in this case, they're gonna have to sort of supervise, fine tune the model to know what no think means and to immediately try to emit the answer.

说话人 1 01:17:07 
And one kind of interesting side effect of this is that they found that if they do this training, where they train the model to have a think and a no think tag, then what they can do is they can sort of, if the model continues to think and you want to terminate the thinking process, you can kind of actually like terminate the thinking process at a special string. That's like considering the limited time of the user, I have to give a solution by thinking directly now and then end think tag and then it sort of accurately gives the answer. So this gives them a control  by which they can sort of more precisely control a maximum number of thinking tokens. And this gives them pretty clean test time scaling from a single model. And so they can set like a maximum thinking budget. And of course, the sort of, you know, very maximum out to infinity is just the original thinking model, but they can kind of early terminate to the left and taking a graceful degradations in model performance. That's, you know, not too bad at the very sort of beginning. You can like half the thinking tokens and still get some pretty good performance. And so, you know, you can look at when 3 also does a nice ablation where they give you the performance at the Reasoning RL stage, at the Thinking Mode Fusion stage, and at the General RL stage. And one thing that's very interesting to me here is that if you look at the first two sets of rows, the general task and the instruction following ones reasoning, RL helps, thinking mode, fusion helps, and of course rlhf continues to help here as well, right? Like kind of everything helps in this regime. But if you look at kind of math or stem performance, you know, in the thinking case, general RL hurts performance and in the non thinking case, it helps performance.

说话人 1 01:18:51 
So there really is seems to be at least some of a tradeoff in, you know, do I optimize for general purpose instruction following up here or do I optimize for like math encoding kind of down here, right? And so those are kind of interesting sort of properties that are emerging, and it'll be cool to see sort of more future models kind of sidestep these tradeoffs somehow. Okay, cool.

说话人 1 01:19:17 
So to put that all together, our sort of initial motivation was to say RL is very powerful. We've kind of figured out that we can do RL with language models in the rlhf domain, but you can't just like, he'll climb on noisy pairwise preferences forever, right? So one solution is to pick domains in which you can't do reward hacking and then just, you know, go for it, right? RL and narrow domains is one good solution and GRPO is one very simple algorithm. And hopefully you all kind of have a sense of like, okay, I can just do policy gradients with some good baselines and that will enable RL on all of these kinds of verifiable reward domains. And then finally, there's lots of successful recipes in the wild, and you hope. Definitely now you've seen, you know, what's in common, what's different, what implementation tricks matter. Cool. Thanks a lot and I will see you all next week.

