2025年7月14日 下午 10:13|1小时 20分钟 13秒

关键词:
same thing、single thing、different kernel、same time、CPU time、compute time、time computations、last thing、performance things、more time、basic things、benchmark things、Triton time、few things、execution time、profiler thing、GPU times、last time

文字记录:
说话人 1 00:00 
Make writing high performance code for GPUs. So part of Assignment 2 is going to be, you're going to have to, you know, do a bunch of profiling. You will have to write your own Triton kernel for flash attention too. You will need to sort of make all of this stuff very high performance. And so in this lecture, we're gonna kind of drill down a little bit and we're gonna try to, you know, write some high performance code for standard components in a language model.

说话人 1 00:28 
So the plan for this lecture is we're gonna just do a brief amount of review about GPU stuff, just to make sure you have once again the basic components of the GPUs that we need to understand in order to follow the rest of the lecture. And then I'm gonna show you a bunch of sort of really basic things about benchmarking and profiling, which will be helpful for both the assignment and in general, if you wanna write high performance pipe towards your deep learning code. And then we're going to basically write some kernels. We're going to write Kuda kernels in sort of C++. We will then do the same thing in Triton. And then lastly, we're gonna, you know, do the easy but very good thing of using Pi Torch's existing JIT compiler to have it optimized for us.

说话人 1 01:14 
And then we'll compare all of those and profile and benchmark things. And throughout, we're gonna really dig in deep. We're gonna go down all the way to the PTX.

说话人 1 01:22 
So pretty close to the machine code to understand what, you know, the GPU is actually doing under the hood when we write all of this code. And then hopefully we'll have time and I think we will finish by writing sort of a fast Titan implementation of Softmax at the very end. So Assignment 1 has come to a close. There's still a leaderboard. You can still submit and update things there. Some of you may be using late days, so please finish up assignment 1 and then assignment 2 is now out.

说话人 1 01:53 
And as I said before, there's gonna be, you know, a bunch of systems stuff that you're gonna need to do. There's fun parts that you can do now involving GPU kernels. And then next week we're gonna talk about parallelism. And that's going to be the other half of the assignment, writing fast parallel code like data parallelism and so on. So we will get to that next week.

说话人 1 02:16 
Alright, so now remember how GPUs work, right? So when we have something like an A100 or an H100, we're gonna have a whole bunch of SMS streaming multi processors within each SM is a large number of units that can do computation. We have in 32 ones or FP32 ones. And then each SM is going to launch a large number of threads, right?

说话人 1 02:38 
And we have the memory hierarchy, which is that we have DRAM or global memory, which is big and slow. And then we've got caches that are much faster. And in fact, you know, you see here there's a thing called the register file. This is very fast memory that each thread can access, and we're going to be making heavy use of these registers as we write high performance code for GPUs today. So the basic structure for the execution model is going to be we're going to have a collection of thread blocks, and a block is going to be scheduled on a single SM, right? So this is kind of the atomic unit that we're going to be thinking about, especially when we write code and things like Triton.

说话人 1 03:16 
And then within each block, there's going to be a whole bunch of threads. And the threads are actually going to be the ones doing the computation. And so if you have a vector and you're going to be operating over elements of that vector, right, you're going to write code where each thread is going to go in and maybe operate over a few elements of that vector at once, right? And all the threads together will sort of process the vector completely. So why do we have these things called thread blocks, right?

说话人 1 03:40 
Why not just have threads and your big global context? Well, threadblocks can communicate with each other. They're shared memory kind of within the SM. That's pretty fast, right? So when you need to do something like matrix multiplication, you're going to need to pass information from thread to thread and within a thread block.

说话人 1 03:56 
That's very fast. Across thread blocks or across these groups, it's going to be very expensive. So you, any data that you need, you're going to want to keep within the same thread block or within the same sort of pile. And that's going to keep things very fast. And that's going to be as fast as sort of an L1 cache.

说话人 1 04:12 
And that's a great, you know, place to be. And so you can use this to synchronize across threads, but you can't, you know, for example, synchronize across blocks. You can't really control what's gonna happen. And remember the thing that I mentioned last week, there's this thing called waves, right? Waves aren't sort of an inherent thing that you normally think about, but for performance, it is an important component. So when we actually run these things, the threads are grouped into consecutive blocks of 32 threads, and that's a wave. And that gets executed kind of all at once in an SM. And so one thing that we would like to do is to make sure all the waves have an equal amount of computation.

说话人 1 04:51 
We can't always do that, but you know, if we can, we would like to do that, right? So we want to make the number of thread blocks, ideally divide the number of SMS and to make sure that. Each wave has an equal amount of work, so we're gonna ideally have a lot of more thread blocks than SMS. So we're gonna try to make that happen as we write high performance code.

说话人 1 05:11 
Okay, and then the last concept and maybe amongst the most important concepts here is arithmetic intensity. We would like to keep arithmetic intensity high. We would like to have more flops than we have bytes of memory movement.

说话人 1 05:24 
And this is because, you know, if you remember the scaling plot from last lecture, our compute scaling is much faster than memory scaling. So a lot of the time computations are going to end up being memory bound that we're not actually getting all of the work done, right? So as a general rule, you know, matrix multiplication is compute bound. If we kind of do it cleverly, everything else is going to be memory bound. And we're going to try to cleverly reduce the amount of things that are memory bound or how badly things are memory bound.

说话人 1 05:53 
So that's our very brief sort of review of GPUs. Hopefully everyone remembers this. You still have a fresh sort of memory of the execution model, feel free to stop me and ask questions if any of you, you know, have sort of lingering doubts or questions about how this is all gonna work. Yes. What was the function of, sorry, a war is essentially a group of threads that get executed together.

说话人 1 06:19 
And the reason why warps exist is that they reduce the amount of control machinery that's needed because you're executing all these threads at the same time. You don't need a control thing for each thread. You need them for blocks of 32, right? And so you see, for example, there's a lot more compute units than there are sort of warp schedulers. And so you're able to do a lot more parallel work without worrying about control. And this is one of the tradeoffs with CPUs, right? CPUs, a lot more sort of silicon area dedicated control and branch prediction and things like this. Whereas for GPUs, much more emphasis on computation with simpler controls.

说话人 1 06:56 
Okay, so now we're going to get into sort of newer content now. And I think if there's one high level thing to remember, it's if you want to write high performance code, you should remember to benchmark and profile your code. And that seems very obvious, but you know, I've seen a lot of things where, you know, students or people go in and they're like, well, I think this is the bottlenecks. I'm gonna spend three hours optimizing it. And it turns out it wasn't the bottleneck at all.

说话人 1 07:21 
I'm sure it was fun, but you know, it was kind of time that was mis allocated. And so if you actually use high performance or a very detailed profiler, you can kind of see exactly where your, you know, bottlenecks are and exactly what the machine is doing. And once you have that, you can go and spend your efforts in sort of the most important parts of your code execution. And so that's the high level thing I want to get across. Because some of the details about, you know, GPU execution and, you know, how you write a Softmax kernel, that's gonna kind of change.

说话人 1 07:48 
And maybe you even want to just rely on the torch compile, you know, auto jet thing. But the fact that you should profile isn't really going to change no matter what the tools are. So I want you to sort of internalize that idea that you should be always profiling if you want to be writing on high performance code. And really, you know, there's a limit to the theory. I think systems is part of this course that you can reason about pretty well.

说话人 1 08:11 
Architecture is somewhat hard to reason about. And you can, you know, really think about sort of the roofline model and so on, but you know, how fast does your matrix multiply? Well, maybe that depends on the library version and your hardware, like which things are bottlenecking for what reason. There's all sorts of, you know, micro code things that you don't really fully know. And so you have to, in the end, have to do end to end benchmarking whenever you're developing these things.

说话人 1 08:34 
Okay, so I'm gonna have an example computation. This is the simplest thing, you know, that we can run compared to all the things that you all are doing in your assignment one. But I'm going to run a very simple MLP. It's going to have 128 dimensions. It's going to have 16 layers, it's gonna have some batch size, and it's gonna have five steps.

说话人 1 08:51 
I'm gonna just do forwards and backwards for five different steps here. And just to make the code clear, it's something like this, right? I'm gonna define an MLP model and we'll sort of, I'll show you that in a moment here. And then I'll define, you know, a random Gaussian input. And then I'll run it for five steps.

说话人 1 09:07 
In that last case, where I compute some forward and then I compute a backwards, and then I return sort of the result, which is just the mean of the output of my MLP, right? Not there's not even losses. It's so simple. It's just you run the MLP forward and I just average pull at the end.

说话人 1 09:22 
And then the MLP is just kind of the simplest thing you can also imagine here. It's just a bunch of linear layers stacked on top of each other, which is this bit. And then, you know, I've got a galliu in between, right? So this is just galu, linear galu, so on and so forth. Everything is nice and square, right?

说话人 1 09:39 
So hopefully this is a very simple MLP that you all feel pretty comfortable with. And then let's go back. Yes. Oh, sorry. I wanna go back up to here. Okay, good.

说话人 1 09:53 
And so now I have this, you know, MLP code that I want to run. And now I'm going to do two things. I'm going to benchmark. So I'm going to do some timings. So I'm.

说话人 1 10:00 
Want to know how long does this function take to run. And then I'll do profiling, which is to go inside the function and ask, you know, where am I spending all of my time? So let's start with benchmarking, right? So benchmarking is just the measurement of walkclock time of performing these operations. And I'm only looking for the end to end execution time of, in this case, my MLP function.

说话人 1 10:23 
And, you know, there are some subtleties to this. Like you're sitting there and you're like, why am I being told how to invoke, I don't know the time it function, but you do have to be a little bit careful about how you measure times. And I think, you know, if you're not paying attention, you will run into these pitfalls when you do assignment too. And so what are we doing this for? We're gonna compare implementations later. We're gonna compare our Triton to our handwritten C++ to Pytorch's implementation and torch compile.

说话人 1 10:50 
And we want to know, was it worth it to write that Cooter kernel? And we'd also like to understand when I make my matrix multiplies bigger, how much slower does it get, right? So we'd like to do some empirical benchmarking of them.

说话人 1 11:00 
So throughout this lecture, I'm going to be using this benchmark function. And that's going to be sort of a wrapper function. I'll step through it. Benchmark is going to do the following things, right? It's going to have a function that I want to benchmark, which is run.

说话人 1 11:13 
And then I'm going to do some number of warm up iterations and then I'll do some number of trials, right? And you might wonder, okay, so like, what's this warm up thing that we're doing here? Well, one thing that's really important is, you know, when you do, when you first run your Pi Torch code and let's say dispatches something to the GPU. It might look very fast and transparent to you, but that very first time something is executed in the background, machine code is being compiled. You know that code instruction might be being sent to the GPU. There's all sorts of things that happen to sort of initialize your code.

说话人 1 11:46 
And so you always want to do some warm up iteration to make sure that you're not measuring sort of the startup speed. Instead, you want to measure kind of the steady state speed, right? If you're running thousands and thousands of iterations, you know, what you're interested in is that part, not necessarily, you know, how fast can you, you know, do on the fly compilation of your Kuda code, right? So that's why we have warm up and you should always have a bit of warm up.

说话人 1 12:11 
And then another thing that's really important, and I'll get to this once we get to the profiler, is you want to call this thing called Torch CUDA Synchronize. Like what is that? Well, the GPU and the CPU are basically two independent compute units in your computer, right? And they can basically run kind of independently. And so the execution model is going to be this Python code that I have here.

说话人 1 12:33 
This lives on the CPU, right? And when I run something, it's gonna dispatch a bunch of CUDA kernels right to the GPU. It says, please run these things for me, right? And the GPU will go off and execute those things and the CP will actually go on and keep running, right? It doesn't wait for those coude executions to stop.

说话人 1 12:49 
And so that's great for writing high performance code, but you should hopefully see the immediate problem if you want to do benchmarking, right? If you're benchmarking and you've got this model where the GPU runs off in the side and your CPU is doing something different. You're actually not measuring the GPU execution time, right? So torch could asynchronize, basically says, all right, let's make sure that the GPU and CPU are in the same state and there's sort of no queued things running and that we're kind of at the same point in terms of the code that's being executed. And now, so the GPU and CPU are kind of in the same state and I'm gonna time it for real, right?

说话人 1 13:22 
And I'm gonna time something for some number of times and I'm gonna run the computation, which in this case is the sleep command. I'm going to do it three times. And since I'm trying to sleep for 50 milliseconds, that's the time that I'm gonna kind of get at the end, right? So I do time that time three times. And of course here, right, I'm also calling Torch Kuda Dot Synchronize at the end of run to make sure that the GPU and CPU states are the same.

说话人 1 13:47 
So, right, so if the CPU is running ahead, it's going to wait for the GPU execution to actually finish here and vice versa. And so now I sort of finished and then I'm gonna average cuz, you know, each single measurement might be, you know, fluctuating because of things like thermal properties of the GPU. And so you wanna take multiple replicates, take them in and return. That's our benchmarking code, right? Very simple.

说话人 1 14:09 
But remember kind of the two important pieces here, right? Always do a warm up. Make sure to call Kuda synchronize. If you do those, very simple. If forget to do those, you'll get pretty crazy numbers.

说话人 1 14:20 
Like you'll get that your big matrix multiply finished instantly, which is definitely not true, right? Okay, so now we can do some benchmarking of matrix multiplies. I'm going to walk through some of these. They're just putting numbers of things that we already know, but I wanna, you know, just walk through it and make sure we're on the same page here. Right?

说话人 1 14:37 
So I ran this on the class H1 hundreds. I have GPUs. I'm going to do matrix multiplies over these sizes, and then I'm gonna go and collect a whole bunch of matrix multiply timings for each of these dimensions, stepping through kind of this benchmark result. And so we kind of see, you know, as we expect, right?

说话人 1 15:00 
Super linear scaling of our runtimes as we increase the matrix size, of course, at the smallest sizes like 1024 and 2048, we actually see that the time zone grow at all because there's constant factor overhead in just doing these matrix multiplies like these numbers have to get shipped from the CPU to the GPU. You know, there's overhead in like launching the kernel. And so it's not the case that, you know, it's super linear all the way to zero. But once the matrices get big enough, we see exactly the kind of scaling that we expect to see with our matrix multiplies, right? Okay, so hopefully straightforward.

说话人 1 15:35 
Now let's try to benchmark our MLP. So what are we gonna do? We're gonna make our MLP bigger. We're gonna have 256 dimensions. We're gonna four layers, backsizes 256, take two steps.

说话人 1 15:46 
And so what's the time that it takes to do that? Well, it's gonna take 6.2 seconds to do that. And now I could do some basic things. I can scale the number of steps from two to five, and I can benchmark all of those and I'll get 2,3,4, and then 5 steps. And unlike in the matrix multiply case, right, if I'm scaling the number of steps, so the number forward and backward passes on my MLP, right?

说话人 1 16:13 
What do I expect the runtime to behave like? Well, I expect sort of linear scaling, right? And that's kind of what we see. Either about 5 seconds per MLP execution, and we see it's about n times 5 for the runtime of kind of the end to end object here. Okay, let me see if I can reset the.

说话人 1 16:33 
I think that's being monitored here. Oh no, I can't. Okay, I'm gonna zoom out a little bit. Sorry about that. Okay, now we can also scale the number of layers from 2,3,4 to five. And what does that give us?

说话人 1 16:45 
Well, it gives us, you know, increasing runtimes once again, linear in the number of layers, right? This time, once again, one layer takes about five seconds, a little bit less than that. And so we get about four times, actually four times the number of layers and linear scaling sort of shows up. Again, unsurprising, right? So both steps and layers obviously have linear relationships with the runtime. And that is exactly kind of what we end up seeing at the end here.

说话人 1 17:13 
I'm going to skip the batch size thing because this is getting a little bit unwieldy in terms of the amount of things that are being tracked here. All right, so that's the end of this benchmarking bit. We can kind of make this nice function that does a little bit of warm up this Kuda synchronize, and we can measure the runtime of anything that we want. And this is good. And you should do this all the time in your code, right?

说话人 1 17:33 
You can measure how long it takes for your new fancy architecture to run. But then I think if you want to fix some problems, benchmarking is a very coarse grain tool. It tells you that your code is slow, but it doesn't tell you where the time is being spent. And so what we would like to do is instead do profiling.

说话人 1 17:53 
And so this is gonna be a much more fine grained object that we're gonna want to do. And so profiling is really nice because it not only helps you see where the time is being spent, which functions, but you know, when you look at what you're calling, usually you interact with the Pytorch interface, right? Like the parts of Pytorch that you call. But beneath Pytorch, there's this whole universe of CUDA stuff that's being called. And when you run a profiler, you can actually see all the way to the low level calls, what is actually being called. And so you can get a much nicer intuition for how the program is actually being executed on the hardware.

说话人 1 18:28 
And so we'll step through profiling a few simple functions and then get a little bit of intuition about what is happening. And so one of the things that is nice is that if you want basic profiling, Pytorch has a very nice kind of built in profiler that you can use. And this will allow you to not leave the Python Pytorch world and get some fairly reasonable looking outputs. And so I've profiled some functions here and you can kind of see the output of this as well. And so, you know, I've taken the sleep example from before.

说话人 1 19:02 
And here is, you know, the sleep function. And when we profile the sleep function, the profile function looks something like this.

说话人 1 19:09 
You know, I have a warm up again, I have Torch Kuda synchronize, and then I call the profiler and I'm tracking both CPU and the GPU times. And then, you know, I run something and then I synchronize again and I print out the average table across all the time. Okay, so I go back. So now I'm going to profile the sleep function. And if we look at, you know, what's happening, what happens here? Well, hundred percent of the time is being spent on something called Kuda device synchronized because there's no GPU work being done. This is just kind of a know up, you know, it's kind of a silly thing to be profiling.

说话人 1 19:45 
And so now let's look at something kind of non trivial, right? So let's look at this basic operation here of adding two matrices, right? So I define an ad function that takes an a and B and add them together and. And this is a helper function that instantiates two random Gaussian matrices and then invokes, you know, whatever is in the operation argument. So this is adding to 2048 size matrices together.

说话人 1 20:11 
Okay, so now I'm gonna profile this and I'm gonna call the profiler and I'll get back something that looks like this block over here, right? So this is what I get back. And I'm gonna have to zoom back out because this is not gonna be alright. Okay. Is this visible from the back?

说话人 1 20:28 
Can someone give me a thumbs up if it's visible from the back and. Okay, good. Or thumbs down if it's not. All right. So when we call the ad function in Python, right, this is kind of all that we interact with this ad function A+B, right? That's all we think about.

说话人 1 20:43 
But actually underneath here, underneath the iceberg, so to speak, there's a lot more that happens. So this gets dispatched to the GPU. And first there's this thing called A10, which is the C sort of interface for Pytorch. And so this wrapper gets called and it says, okay, I'm going to add some numbers, right? This is what's being called.

说话人 1 21:01 
That's the outer wrapper. And then that dispatches to a particular kernel called vectorized element wise kernel for comma at native CUDA function ad. Right. And this is the thing that's actually doing the adding. And then there's this also other thing called Kuda launch kernel that's taking some time.

说话人 1 21:19 
And this is actually, you know, the CPU is taking the command and sending it over to the GPU. That's the kernel launch. And that takes some time. And then finally, you know, the Kuda device synchronizes. We're waiting for the GPU to finish and send things back to us. And that also takes some time, right?

说话人 1 21:33 
The mere act of having a synchronization barrier is gonna cost us some time. And so we basically have, you know, the time total in the end here, 1.4 milliseconds on the CPU and 17 microseconds on the CUDA, right? So this is really fast on the GPU, slower on the CPU. And if we're looking at the CPU time that's being spent, which is the self CPU time, we see that kind of the C++ interface or the C interface is actually the thing that's costing us a whole bunch of CPU time, and they're sort of overhead to doing anything where we're sending stuff over to the GPU. So that's the ad function and we see, you know, what's happening under the hood.

说话人 1 22:13 
Same story here. If I want to do a matrix multiply, so I'm doing, you know, a multiplied by B. So this is a matrix multiply of a and B.

说话人 1 22:19 
You know, I'm doing 2048 matrices once again. And then I do profiling. Now this time I see, you know, a 10 map mole. So this is saying like this is the lower level interface to do matrix multiplies. And this is going to dispatch the Cutlass, which is Nvidia sort of high performance matrix multiply CUDA library. And then it's dispatching to a very particular cutless kernel, which is going to have some tile size. The names are truncated here. I'll show you a more detailed version in a minute. You know, this is basically pointing towards a very particular set of like tile sizes and the number of blocks and so on. And so this thing is parameterized and that's actually doing the matrix multiply.

说话人 1 23:00 
And once again, we see the same two things at the bottom here, you know, the kernel launch and the synchronization of COODA devices. And you can sort of see once again the CPU time, COODA time split. And we're spending way more time in Kuda because, you know, matrix multiplies do take more time than just adding two vectors.

说话人 1 23:17 
Okay, any questions so far? I can pause for a moment here. I think I've just been going sort of very quickly and on my own through the profiler. So if anyone has questions, I can stop for a moment. If not, I can keep going. Okay. Oh, yes, in this case.

说话人 2 23:35 
Are that is greater than our CPU, but we did have a barrier that like sent to for the CPU tooling credit synchronize. And so by that shouldn't receive you time always be at least the same amount.

说话人 1 23:51 
Yeah, I don't think this calculation is vital. Cool. Oh, yes, sorry, there's two questions. Is it any particular.

说话人 2 23:59 
Reason why, like when we switch from adding to manual machine, I went down.

说话人 1 24:04 
Is there a reason why when we go from adding to map mode of CPU time goes down that I am not sure, to be entirely honest. Yes.

说话人 2 24:14 
And the story behind this.

说话人 1 24:17 
Compared to like providing it, is there overhead in the profiler that can distort things compared to running it in the real world? Yes, there is overhead in the profiler, like the barriers will do that. I'll show you a more advanced profiler from Nvidia and you can add things like annotations that will also slightly distort the timings by, but not by much. The really large scale things that you see aren't going to be really distorted by the profile. And so if you're looking at like micro timings, yes, probably. But a lot of the things that we care about in the class know.

说话人 2 24:50 
Yes, just to make sure things correctly. So is that like for the ad case, is the idea CPU?

说话人 1 24:59 
No, I. Guys, right, the millisecond time. No, that's right. So this is the percentage of time, as you can see, that the actual millisecond time that a 10 ad was actually executing in some capacity on the sea.

说话人 1 25:16 
I don't think it's easy. My percent of what the CPU is good. That will sell. Yes, right. This is the kind of CPU's active, not percentage utilization. If that's, yeah, so this is not like the total amount of CPU flops or something. This is the total percentage of time that the CPU is doing something. Yes. Okay, cool.

说话人 1 25:37 
Alright, here's another example of a map mall. So this is a different dimensionality, right? So this is a, I'm multiplying 128 dimensional matrix here. So 128 by 1,28, much smaller. And you'll actually see that now it's actually directly executing sort of this different command that's executing XMMA. GMM is you a matrix multiply type and this is float 32. You can kind of see from the naming of this kernel what's actually happening here, which is that this is a tiled matrix multiply of some kind. And it's not sort of going through cut less, it's executing this particular command directly. And so for a small matrix multiply, you know, you see that is dispatching to a different kernel now. So you can kind of see kind of the complexity of matrix multiply when we're operating at this high level abstraction. We just think of matrix multiplies a single thing, right? We call like a at B and we're done.

说话人 1 26:35 
But underneath the hood, depending on the dimensionality that you have, depending on the hardware that you have, it will actually dispatch to very different matrix multiply sort of primitives under the hood. And that will actually manifest in very different sort of performance characteristics. And so one fun tip is torch compile, which I will talk about later, actually has an option to sort of micro benchmark the matrix multiply performance on your hardware. And then it will actually then pick the highest performing matrix multiply sub routines for your model, which, you know, in the past I found, you know, gives you like 10% speed up for free. It's very cool that like optimizing for these things actually gives you free gains out in the real world. Okay, so that's another map, more example. And so the cool thing about the profiler compared to the just the raw benchmarking is we can now kind of see which Kuda kernels are being called. We can see that, you know, different sizes of matrices lead to different kuda kernels. And we see, you know, cutless 80 simkees SDM, right, is a is a diff is this cutless linear algebra library. And it tells us things like the tile size.

说话人 1 27:47 
So far, these operations are very boring in a way like matrix multiplies and ads. They're basically 1 to 1. You have a, you know, operation on the CPU side, it translates to a GPU operation, and it just gets shipped over, right? So there's just a single operation in all of these that does anything on the GPU. So I want to look at some more complicated operations, two more of these that have sort of more compound behavior. So what I want to do now is I want to do, I want to look at this operation called Torch C disk. And this is computing, you know, for two sets of matrices, the pairwise Euclidean distance between two sets of vectors, right? So this is going to be a big distance matrix computation between A's and BS that I want. So that's C disk. And so this is obviously a much more complicated operation. If you want to compute Euclidean distances, you're going to need to compute dot products, you're going to compute square roots. And we're gonna see that once we compute C disk. So now here is the profiled output of C disk. So we see that this torch, you know, Python command does map in the C interface to some sort of lower level C disk. So this is a 10C disk, which then maps to a 10 Euclidean disk. And then this will decompose into a whole bunch of things like ATM, mammal, ATM, pal, and then some, because these are all primitives that you're gonna need in order to actually to compute the Euclidean distances between all of your vectors. And when you for each one of these like matrix multiplies in concatenation and taking the powers, you have a corresponding CUDA command that is being called here. You know, we have gemm, which become familiar with.

说话人 1 29:33 
So this is a matrix multiply. It's taking 78% of our compute or our compute time on the GPU. We've got, you know, copies and sort of concatenation of arrays. This takes 6% of the execution time. And then this sort of vectorized element wise kernel, which is taking the power, takes 5% of the GPU time and 3% goes to the sum. So now we get this very nice. Nice low level breakdown of where, you know, my GPU is spending all of its time. And from this, you know, I can get some sense of where maybe I should spend my time optimizing. You know, maybe I think I can optimize my matrix multiply. That would be great because that's 70+ percent of the time spent in the GPU. The final example, the final two example, sorry, that I want to talk about is galliu and soft Mac. So these will be our running. Oh, sorry, there's a question. Okay, so I will maybe answer that question in a few minutes cuz there's a cooler profiler that shows you a much nicer picture. And so I can gesticulate here, but I think it'll be better to show that with pictures. Okay, so I'm gonna talk about now the Galu and the Softmax. So the galliu is going to be our running example throughout the class. So this is a non linearity. If you remember, it's the Gaussian error unit, Gao Shen error linear unit. And that's gonna be a product of a can H and a exponential, if I remember right. And so we're gonna have, you know, all sorts of operations. So we're gonna add a and B, and then we're gonna call Galu, sort of simulating the linear plus non linear structure that we might have in our MLP. And so we see once again, basically the same sort of mapping. We see a 10 ad corresponding to A plus b. And then we have the Kuda equivalent. And then we have actually a galu function implemented in Kuda, which is all the way down here. And that takes about 33% of the compute. Okay, fairly reasonable. And then we have one second, the Softmax, I won't go through all of these in sort of gory detail since, you know, they all start to look the same after a while. But the thing to really point out that I think is cool is that a lot of these really core primitives like Softmax and gelu, there's just kernels written for them, right? So it's not like the GPU is executing the basic primitives. There's sort of a fused operator that computes all of this. So there's no back and forth between CPU and GPU for all piece. So, okay, I mentioned before that I was going to sort of answer this question on what the CPU was doing. And so let's think about something a little more sophisticated, right? I had that MLP example that I started with for benchmarking. And I would, let's say, like to optimize that MLP, make it run really fast. So how can we do that? Well, ideally, we would sort of profile this in a nice sort of fine grained way. So if we use the torch profiler, this is kind of what we would get. If you remember the MLP, there's, you know, stack linear layers, there's a forward and a backward. And you see roughly, you know, there's this backward thing that's happening. There is a matrix multiply, there's linear, and then there's accumulate grad operation for the backward. And here is the matrix multiply kernel and then there's only 10 things that can fit here. So I think this gets cut off at a certain point. But this is nice. It does tell you that most of the time is being spent in the map moles. But you do kind of wonder like where does all the rest of the time go and why does only 31% of my times stay here? And where is the 60% here? It's a 8,10 mm, but there's no corresponding kernel, right? This is a little bit mysterious. And for something that's very complex module, this is not a very good visualization. And so for that, I think we have to actually get out a real sort of grown up profiler. And you will have to, you know, or we will ask you to look at this thing, which is Nvidia's Insight Systems. And this is the kind of Nvidia's sort of detailed way of looking at GPU behavior and performance. And so we will actually kind of see exactly what is happening as we run this MLP. So actually in the back, can you see, I don't know, this tiny text over here? Thumbs up. Okay. All right. If you can see it, then I'm not gonna zoom in, but it does seem small even from here. All right, so basically, if we look here, we see several different things. We see CUDA, HW over here, and then we see threads. And so this top half, this Kuda part, this is what the GPU is kind of doing. And then in this threads part, we see kind of what the CPU is doing. And I can also pull up the code. I think, yes, the code here. When I profile it, I've added a few annotations. Okay, this one I need to zoom in for sure. Let's. Excellent. All right. So I've annotated the code with this set of things that says, oh, let's see, nvtx, which basically annotates my code with annotate with markers. So when the profiler comes in here, it will know that this piece of code belongs to a block called define model. And for example, this part that says step range push and range pop this range. Range here from line 77 to line 55 should be annotated with something that says step underscore step. Okay, so I've added all these annotations in my code before calling my profiler. And so let's go back here. So now if we go to this lined us as mvtx, we can kind of see define model, which is the thing that I wrapped my model construction call. And then I see step 0, step 1, step 2, step 3, step 4, step 5. So each step is now nicely annotated in this profiler. And we can kind of see all of the things that the model is doing as it goes along. And I'll start on this side. One thing we see is that this piece of code, it doesn't do very much work. It takes only 14 seconds. So actually most of the time for the profiler is spent on overhead. So the part up until roughly here is, you know, things like just loading the libraries and that takes a long time. It takes apparently 7.5 seconds to just initialize everything. And then at least on the GPU, at 7.5 seconds or so into the program, it starts actually building the model. And you see here on the memory footprint, you know, this is the place where now memory is being sort of allocated. And on the GPU memory, the memory usage starts to grow. Right now, the model is now constructed at this point. And then step 0 is where sort of the action starts to happen. And so you were asking earlier what's happening between the CPU and sort of GPU. And so how the execution model of this works is here is sort of step 0 on the CPU and I'm starting right here. And here's the forward pass. And this is layer 0. So let's just kind of think through what's happening. As I said before, when you first encounter or when you first call a piece of code in Pi Torch, it doesn't just directly execute. It will actually do things like, you know, on the fly, compile things. And so, you know, this thing like runtime triggered module loading is sort of overhead work that's being done in order to just initialize the layer and the computation and move sort of various bits of code into the GPU. So this takes a long time. And then after this layer 0 is done. Now if I look at sort of any slice here, let's sort of zoom in to selection, we'll see that each of these layers is really quick. And what happens here is when I highlight this layer 1 over here on the CPU side, notice that's not where layer 1 is on the CPU side, right? So as I said before, the CPU and GP are kind of two different execution devices. So I start at layer 0. I'm done with layer 0. I start layer 1. Now the CPU is actually just sending all of the sort of CUDA commands, the Kuda kernels. It's launching all the Kuda kernels already to the GPU at this point, right? So when the CPU is saying I'm doing layer 1, what it's actually doing is it's queuing commands into the GPU. It says, now run this thing next, right? And so the CPU is running way ahead of the GPU. And by the time Layer 1 starts executing on the GPU, actually we're already at layer 9 on the CPU, right? The CPU is running way ahead and there's basically a queue that the CPU maintains where it's sending a fixed number of kernel, couta kernels to the GPU. And so once you hit that Q depth, it's gonna sort of stop running ahead. But until that point, it's just gonna keep going and going as far as it can, right? And in this case, this does become, I'm going to zoom out again. Undo the zoom. There we go. In this case, this kind of gets a little extreme because if I zoom out once more, notice how, you know, in these steps, I'm running way ahead. Like the Step 0 is here, step 2 is here. This was step 1, which basically took no time at all. Step two is here. So it's the CPU is basically running one entire step forward and backward ahead of the GPU. One interesting thing that you might do is if you're writing, you know, various code for training a language model, one normal thing that you might do is let's go back to the code. I might do something like print, you know, my losses in between iterations. This seems like it should have no effect on what the GPU is doing, right? You're like, oh, it's a print statement. How much could it do if you think about it for a moment, this will have big impacts on the execution layout on the GPU. Because in order to print this statement, right, this print statement happens on the CPU and the CPU needs to get the loss. That means it needs to wait for the CPU to compute that loss. And so let's look at what happens. So here, you know, as I said, you know, step 4 on the CPU happens way before the GPU equivalent. Now let's switch back. Now this is the version that I profiled where it has the print statement, right? And then now I sort of zoom into selection here. Now see how step 1 and step 2 are basically kind of synchronized now, right? Because I have to wait for the long. Us to get computed. And you look at this and you say, oh, but it's still a little offset, right? Like step 2, step 1 isn't exactly aligned with each other. So now let's kind of zoom back in and see, okay, what happened to step 1 on the CPU? Well, basically the endpoint of step 1 on the CPU is also kind of where the optimizer step starts, right? So by the time that Ford is done, sorry, this could have stream synchronizes things. So this could stream synchronize command on the CPU. This is basically saying, I'm just waiting for the GPU because I can't run ahead. I'm waiting for this loss to be computed and to be sent back to me, right? So this is kind of a dummy operation where it's saying CPU weights. Well, the backward step is done. So now I can print the loss. I've printed the loss. Okay, now the CPU can start running ahead. And it does run ahead and start sending step 2 stuff now and then. Well, once this hits here, it sort of run out of commands. It's waiting for the loss again. Could a synchronize wait? Backward step is done. Now I can print the loss. Now I run ahead again, right? So in this case, you know, the GPU is still essentially full utilization in both cases. But in extreme cases where let's say you're printing tons of stuff all the time, actually you're gonna introduce a CPU bottleneck, right? Because the GPU has to, the CPU has to keep waiting for the GPU and it can't launch the kernels sort of ahead of time. So that's kind of a really cool thing that you can see with the profiler, sort of the CPU versus GPU. And there are actually different devices that communicate to each other. It's not at the single unified object. And you wouldn't see that unless you started to look at some of these like more advanced profilers. Any question about that sort of set of things? Cool. Okay. And the other thing that I want to kind of show you is, you know, the profiler thing that I was playing with before. You can also generate very similar views in NCIS as well, where you sort of select some range of things that you wanna. Let's do a warm up. I said we should, so we should exclude the first couple steps. So we'll start a step 3 and we'll measure some steps sort of in this range. We could take the kernels. This is what's doing the computation. And you can see that there's actually many different kinds of matrix multiply. This is one matrix multiply kernel. This is a different matrix multiply kernel. There's a different sort of like vectorized element kernel. And all of these are taking different amounts of computation. And we can take this and we can say, oh, show me in the events view all of the things that are happening. And I can also see sort of the stats view, all of the time that it takes. Wait, let's see. We want the average time that we want. Sorry, scooter kernel execution summary. Yeah, we want the total duration of the kernels. And so we can see which kernels are taking the most time and aggregate across these views. So this is actually a very powerful tool that can give you both like the aggregate view of what's slow and what's fast, as well as individual kernels that are being launched and when they're launched and where the CPU commands for that came from. And I guess one final side note here is this is one of the reasons why, you know, it doesn't matter that we're programming in Python and Python's not a very high performance language, right? Cuz the CPU is never the bottleneck because the CPU can run ahead and sort of queue commands into the GPU. And so this sort of detaching or like this disconnecting aspect between the GPU and the CPU is one of the key reasons why we can use this nice high level programming language and yet still get sort of full utilization out of sort of our GPUs. Cool. Okay. Any questions before I sort of switch back to this? Because I'm going to leave NCIS sort of forever for this lecture, this point. Cool. Yeah, but you'll get to play with it in assignment too. And I think you'll appreciate it cuz it gives you like a really interesting view into what your hardware is actually doing to make these like language models train. Okay, that was benchmarking and profiling. Now you have all the tools you need to be able to do sort of performance things. And now we're gonna write some kernels in the remaining time. So remember kernel fusion, right? So this was the image that I showed you in lecture, right? There's a little factory. Every time I need to do an operation, I need to ship it from the warehouse to the factory and back. And so if I, you know, naively do a bunch of operations in sequence without thinking about it, I'm paying for a lot of sort of shipping cost back and forth from the warehouse. What I should do is have one factory that does all the operations at once, so I do not pay for this cost multiple times, right? That's very important. So now we're gonna do GELU and we're gonna write a kernel for GELU. And I'm gonna write that kernel in several different ways. And we're gonna look at the performance impacts of doing that. And. So we have the pipe torch implementation of galu and that looks just like this torch and n functional galu. And I invoke approximate equals 10 h because I want this to exactly match the naive thing that I'm gonna do next. So this is not gonna be, you know, actually multiplying by the CDF of the Gaussian. It's gonna be some approximation to that that's easier to compute. Okay, so that's the Pytorch value. And now I'm going to do the dumb thing, right? You're going to look at this code and say, this is going to be low performance. I'm gonna go in and in Pytorch I'm gonna write galu as 0.5 times x times 1+10 H square root pi over Q times x plus 0.044715 times xq, right? Magic formula. But this is a good approximation to the value. You can look it up or convince yourself this is true. But if you do this, you see that there's a lot of operations that happen, right? There's like a 10 h, there's an X cube, there's multiplication by a constant in addition and multiplication by 0.5 and x. If this involves, you know, multiple different CUDA kernels, this is probably going to be slow, right? That should be our intuition at this point from fusion. So let's see if that's true. Okay, so these two are the same. You can see at the top left, they compute the exact same numbers. And you know, we can systematically check this on random gautions. And now let's sort of benchmark the two. Okay, so the manual time is 8.1 seconds for a really big value. And Pythwatch time is 1.1, right? Millisecond, sorry. And the fuse version is gonna be significantly faster. In fact, 8 times faster. Wow, you know, big difference from writing a simple kernel. Of course, your map moles are probably still going to be the bottleneck. But it would be really cool if we could go from that 8 milliseconds to that one millisecond, right? That would feel very satisfying. So we're gonna try to get close to that 1.1 millisecond in the next few parts of the lecture. So now let's look at what's happening under the hood. I don't need to look at NCIS because all I really want to know is some very high level stuff for the manual galu, you know, kind of just like I said, it's gonna do a whole bunch of operations. It's gonna do a bunch of multiplications. It's vectorized, but it's a bunch of, you know, Kuda kernels being launched here. And notice on the right, this Kuda kernel gets called three times because we have a whole bunch of multiplications floating around here.

说话人 1 47:23 
We've also got, you know, addition. We've got a 10 h. And each one of these is probably kind of slow. And in the end, you know, we're incurring fairly large overhead doing this.

说话人 1 47:34 
Now let's do the same thing, sorry, with the Pytorch Galu. And this is really great. There's a single Kuda kernel launch. It happens once and it just processes the whole thing. This is what we like to see. And of course, this is very fast because it's just a single Kuda kernel, right? So this is really nice. And we would like to, you know, somehow get to the Kuda kernel. And so the first thing you might think of, depending on how much you know about writing GPU efficient code is, all right, the Pytorch people must have written this in the lowest level language possible. So we're gonna do the same thing. We're gonna go to not the lowest level possible, but we're gonna go to the C++ API and we're gonna write the CUDA kernel in C++, right? So let's open it up and write our own CUDA kernel. So how is that gonna work? Okay, so we have gone in and sort of created a C plus version of the whole thing. So Kuda, you know, when we say Kuda is actually the C++ API for interfacing with and programming GPUs and just like sort of the logical model of a GPU that we describe, you know, we're gonna write some sort of function F. And then when we sort of invoke this Kuda kernel, it's gonna automatically call F on all of the elements of a vector or a matrix. And then we will get to parallel compute everything that we want as nomenclature, we're going to have a grid, which is a collection of thread blocks. So think of this as I have a task, I'm going to cut it up into pieces and there's gonna be a number of blocks. This is the, you know, in the 2D grid, for example, there's gonna be sort of a row coordinate, and then there's gonna be a column coordinate. And this will be very useful if you're working with matrices. And then there will be the size of each of these blocks. Like, you know, how big are these in terms of the number of thread blocks. So this is the dimension of the box. And then there's a collection of threads within these blocks. And this is the coordinate that, for example, one thread block lives in. And then each thread is within each block, right? So there's sort of hierarchical structure here. There's a grid and then there's a thread inside a grid. And then we're gonna basically each function is gonna take in three things. It's gonna take the block index, like which thread block do I belong to? Which, what's kind of the block dimensions, and then what is the index that I am, like my thread index. And with these, I can kind of know which coordinate. That I am in the matrix or the vector, and then I can sort of decide what logic that I want.

说话人 1 50:05 
One sort of last thing before we go through the actual C++ code is, you know, whenever you're trying to debug CUDA, you want to launch with Kuda launch blocking equals 1. This will allow you to actually debug your Kuda kernel. It will give you sort of error messages back at a cost in terms of the runtime. If you don't do that, you are going to have a bad time if you're writing Kuda code and needing debug.

说话人 1 50:31 
So, okay, here is my Galu code and let's go through it kind of piece by piece and then I'll talk about what all the pieces are doing. This will probably take the longest out of the things that we're going to walk through, other than the machine code. And once you understand this, you should be able to understand all the other pieces. So we'll go through this a little slowly. So there's two parts of this code. So the first part, this Galliu kernel piece up here, this is the actual kernel. This does the computation, right? This goes gonna get sent to the GPU, it's gonna do the computation and then it will return the results.

说话人 1 51:06 
This piece, the galu function here, this is a wrapper, right? This is lives on the CPU. It's gonna orchestrate the launch of the kernel, which is actually going to go out and live in the GPU, right? So maybe we can start with kind of this sort of wrapper piece, this galu function first, right? So we're always going to check two things basically in the Triton or the Kuda code, we're always gonna check, oh, sorry, there's a question back there. Okay, sorry, that's my bad. Okay, that is an easy fix, but I needed to know that you can't see. Okay, good. Right? Is this good? Okay, excellent. Okay, so we're gonna start with the galley function. And there's two things that we're always gonna need to do. The first one is to make sure that X lives in like the GPU device, like a CUDA tensor of some kind, right? If it's not, well, that's going to be a problem. We're not going to be able to do anything on the GPU. The second thing, which is maybe less obvious, is that we want to check to make sure X is contiguous. What that means is it lives in a continuous block of memory because when we index into X, we're going to do a whole bunch of indexing arithmetic, and we're going to assume that X lives in a block of memory, right? And if it doesn't, it's just gonna be, you know, basically impossible to do this with any level of generality.

说话人 1 52:26 
And so when we compute the galu, right, we take in an input X and we're gonna output a y, right? And so we need to allocate output. So torch tensor y equals torch empty like X. This is just saying, well, give me sort of an output tensor space or a pointer to an output tensor that is just like the dimension of X.

说话人 1 52:48 
And notice that I'm not calling zeros. This will save on extra operations. I don't need to zero out these wise because I'm gonna write into them anyway, right? So this is a minor, but you might as well do it optimization. And then basically in all the code that we write, we're gonna need to figure out the grid, right?

说话人 1 53:06 
So what's the total number of elements that I have? What's the size of each block? The number of threads that I have in each block, and then how many blocks total do I have.

说话人 1 53:16 
And when I need to figure out the number of blocks, I'm going to, you know, call C Div, which is going to be essentially take the ratio of num elements to block size and then take the ceiling, right? Because I need to round up to make sure that very last set of elements that sort of isn't divisible by box size still gets computed, right? So I take the ceiling rather than the floor. And then this is all very simple bookkeeping stuff. And then I say, all right, launch the kernel. You know, the galu kernel gets launched. And this sort of angle brackets are saying this is kind of the, with the given number of blocks in the size of each block.

说话人 1 53:51 
And this is going to be passed into sort of the kernel command. And then I'm going to pass in the pointers to x's and y's, right? I'm not actually going to pass the values of x is and wise and the total number of elements. And I need this to compute sort of essentially the boundary conditions of my kernel.

说话人 1 54:09 
So now let's go to the actual kernel itself, right? So I have global void galu kernel and I get in pointers for in and out. And I have number of elements, items, and this keyword global.

说话人 1 54:21 
The website. Sorry, the rendering here has mangled it a bit, a little bit, but you should think of this as underscore global. And this is a keyword that distinguishes it as a kuda kernel function.

说话人 1 54:33 
And so what am I doing? Well, you know, this thread is actually supposed to operate on a single element I, right? But I don't get I as input. Like the code doesn't actually tell me you're in a vector in coordinate I. So I need to compute where I am and how am I going to do that?

说话人 1 54:51 
It's going to be, I take my block index, right? I only have one dimension, so it's block index dot x. So just the first coordinate and then multiply it by the size of. Each block, the block of dim dot x. And this tells me, you know, basically the starting point within my current block.

说话人 1 55:07 
And then now I add in thread IDX. So you know, I know where the start of my current block is. And I add in the offset to where I am within the block and that gives me my global coordinate I, right? So some bookkeeping computation just to get the coordinates here.

说话人 1 55:21 
And then this is important too. You see this pattern basically in all the Kuda code that people write. There's no kind of out of bounds checking naturally. And so what you do is I have my coordinate and I'm gonna check to make sure that, you know, I am supposed to be processing something that's inbounds. And some of the threads at the very end of your block, they're gonna be processing stuff that's out of bounds in memory, and you do not want it to touch those. And so you basically condition it on I less than num elements and you do nothing if you're outside of that.

说话人 1 55:50 
Sorry. Yes, sorry. That's, this is just the extension that you sort of write the Kuda code in. It's to distinguish it from, you know, just your standard C code. Okay, so is this just a file name thing? Is this dot CU? There's nothing particularly special about it.

说话人 1 56:11 
Okay, and then, so now, you know, within here, we're gonna just do our computation, right? It's just gonna be, I'm gonna write out, I have my input in, I'm going to index into the I th element and I compute my galu just like I did before and I assign it to out of I and then I'm done, right? That's all that I need to do. And since this is all pointer stuff, I don't really need to worry too much about what is kind of actually happening here. So that's basically it.

说话人 1 56:38 
I can then take my sort of Kuda gelu code that I have and then I can load this sort of C++ code in line and then I can just have it compile into a module all within Python. It's all very nice and convenient. You don't really have to go out onto the command line and do things. And so now we have Kuda Galu defined. So this is nice. And basically it's a compilation of this and I can call it from within Python and we'll use the C bindings to call this guy. Okay, we're done calling kuda gelu. I have my, you know, I can check that the manual galu and the kuda galu are the same. And now let's benchmark the two. So I have the time that it takes to run Pytorch and, you know, just like last time, it's about 1.1 milliseconds. And manual time, remember, is 8.1 milliseconds. So a drum roll, what is our Couta time? Well, we've gotten it down to 1.8, right? Not quite as good as Pi Torch's implementation, but you know, we're getting pretty close to Pi torch time, right? We've gone from 8 milliseconds to 1.8 milliseconds, which is not bad because that C code wasn't that hard to write. And so now we also do some profiling and we can kind of see what is happening here now. And you know, it's called the Galu kernel, right? This is the code that got shipped off to the GPU. And then it's calling empty. Like this is the initialization and then empty stride it, right? And then could a launch kernel and could a device synchronize? And that's basically all that's happening.

说话人 1 58:11 
And notice how, you know, once again, this is a single Kuda kernel eats up 100% of the GPU time, kind of like what we want it, right? So there's some further optimization we can do, but this has really already solved the problem of, you know, kernel fusion. We fused all the operators together. So pretty good.

说话人 1 58:30 
These kinds of element wise operations are easy to write in Kuda. Like if you have a new kind of, I don't know, non linearity, you could easily write a Kuda kernel for it yourself if you really wanted to. But more interesting operations are going to require reading multiple values, like doing reductions. Those are going to get a little more complicated. Flash detention will be a little bit more complicated, but not too much. So when you have to do it in the assignment. Okay, any questions on the simple C++ Q to kernel? Yes. So, you.

说话人 2 59:03 
Know, to check the beginning. Is it, is that for error? Is it like Calson's lower kernel?

说话人 1 59:09 
So the question was, what happens if it's not contiguous? At least in the code that we wrote, it will just throw an error. Cuz it's an assert. You could potentially write code to handle it, but there's almost no reason for memory to be fragmented because it will allocate continuously. And you won't reallocate like the middle of a memory unless you're doing something like really tricky. And so you should, you should really, unless you're doing something pretty advanced, expect to have continuous memory.

说话人 2 59:37 
Do like a transpose of some operation that makes every, not us, right? So like when you're putting at a higher level, careful with diversity, I can force.

说话人 1 59:47 
You my coverage. So the question was like, if you're transposing, then you're no longer gonna be continuous. You're gonna have like a, you know, jump between all the elements and the index if you're sort of row traversing. Something that's sort of column stored. Yeah, so I think transpose or like views or like essentially shuffling dimensions is like the one exception to this, but that's handleable. And like the outer, like sort of the wrapper part, right, you can basically pass it something that is continuously indexed and for a lot of the matrices, you won't really care. So yes, right. So what would happen if you chose a different block size, the sort of GPU related sort of concerns would kick in, sort of like, do you have enough blocks for to saturate your SMS and do you have enough work within each block? And those are like kind of the two things that could matter here. But I think my guess is that for block sizes that are relatively large, like 1024, it probably won't matter past a certain point cuz we're not doing anything advanced. It's all entry wise off operations for this. Like very simple example.

说话人 2 01:00:56 
Is the reason that our non ug version was so slow because this queue asks like do small operation and since each groups fantastic understanding.

说话人 1 01:01:08 
So the question was like, why was our non couta kernel sort of manual thing so slow? It's not that it's sending things back from GPU to CPU per se, like X is gonna live in the GPU, we allocate it in GPU, like we'll do like as device like CUDA, but it's gonna basically not be in the SM the whole time, right? So once we do like x squared, right, that's a, you know, akuda kernel. And so that multiplication operation will read the sort of vector from the global memory into the SMS, do the computation, it'll write it back. And so this is all in the sort of DRAM to SM communication cost rather than the CPU, the GPU communication cost. Of course, if you write like as device CPU, then you'll hit get the, you know, CPU transfer cost in addition to the DRAM transfer cost.

说话人 1 01:01:58 
Okay, so now you've seen that and like, okay, so that was not too painful, but it would be really nice if we had nicer sort of Python abstractions for writing CUDA kernels. And this is what Triton is. And Triton is quite nice. It like has this very nice middle ground where you don't have to manage literally everything about the GPU. So Triton is sort of a domain specific language developed by OpenAI in 2021. And it makes GPU programming much more accessible. So like you write everything kind of in Python and you don't really think about the threads anymore. You think about thread blocks and Triton manages a lot of stuff that is annoying but can be automatically optimized so it can manage a coalescing of memory. So remember that, you know, from DRAM, you get four sort of adjacent values at once, what's something called burst mode. So you really want to make sure that you know, your memory retrievals are sort of grouped into adjacent sort of four element or more sort of calls at once. So it will handle those automatically. It will group those. It will do shared memory management when you need to sort of manage which sort of memory that you're writing to within the SM with multiple threads from within each SM, you know, you might need to stop or start threads all managed automatically, but scheduling across SMS or what different SMS do, that's manual.

说话人 1 01:03:29 
So like the kind of the programming model is that you're gonna think kind of at the SM centric level and the compiler will handle a lot more of the lower level details. And trying is quite nice because it can outperform by quite a bit a lot of Pythor implementations. So it's kind of like going all the way to writing CUDA, but you're still in the very familiar Python land. And I think a very underappreciated advantage is sort of as it's written here, it's all in Python. You can step through it, you can kind of debug it fairly nicely.

说话人 1 01:04:00 
And so let's step through a Triton kernel. Like once again, we're gonna write yell you and we're gonna do it in Triton. So this I've, you know, put the code to be as similar structure as possible to our other code, right? So this is sort of the CPU side code, so to speak. This is the wrapper Triton Galu code. It takes in X, which is a torch sensor. And I've got my 2 asserts at the top. And I'm going to allocate an output tensor y using empty light once again. And it has the same exact sort of coordinate computation sort of components. And even the kernel launch looks very similar. I've got this num blocks annotation and then my block sizes is, you know, at the end here, not in part of this brackets, but basically I'm passing the same information to my kernel and now try and galu kernel is this code over here. And this is going to do the same thing as what we were doing before, but now it's nicely written in Python. And, you know, the mental. Model here is the inputs are going to be at X pointer, y pointer is the output vector, sort of the starting coordinate. And the block size is how big, you know, each of my blocks are. And num elements is gonna be sort of the very end of my array. So now I need to get this set of lines 5,5,7 to 561. This is doing the computation of my index, right? I did I equals, you know, some formula before. This is doing the same calculation over here. I'm calculating where is the start of my current block. Well, that's my block ID times the size of the block. That gets me, let's say I live in Block 1. It'll get me this point right here at the middle. And then afterwards, I need to know where do I live within my block. Well, that's gonna be kind of the offset. But now notice one difference. I don't get in an offset because I'm not programming threads, right? I'm programming blocks. And so what does that mean? Well, my offsets are actually a vector, not a single value. Because this is basically going to be, I'm going to do vectorized operation where the vectorized operation is going to be handled by different threats. So here my offsets are the start of the block plus a vector, this range of block size sort of offsets. So my offsets are all of these coordinates within block 1 at once. Of course, if I'm at the very end, I might go off the edge. And so I need a mask to handle anything that lives off the boundary of my vector. Now I'm gonna load in a sort of single vectorized operation everything at once. So x pointer plus offsets, these are sort of the values that I'm responsible for masked up and it's loaded into X, which is my sort of internal values, my internal sort of temporary vector that I need. And with this temporary vector, I'm going to do exactly the old galu computation. There's no 10H, so I compute that manually. But this formula, you can convince yourself is the same as what we have here. And then y is going to be the formula computed up here. Now, once I'm done, I need to write it back into my output sort of buffer or my output vector. And so I compute sort of my targets. So this is y pointer plus offsets. I take my values, my temporary values, y, and then I store it, right? So this is very similar to what came before, but this one is the vectorized version. I get to operate on an entire block at once. And so instead of kind of thinking at the perspective of a thread, I'm thinking from the perspective of a block, but not too different, right? This is all fairly similar stuff. So now I've written my Triton value and all right, I will do this fairly quickly. Alright, so one last thing, I will only point out a few things here because I don't wanna get like so in the weeds that you all like get up and leave. But the one last cool thing that we can do is Triton, of course, compiles into low level sort of almost machine code for the GPU. And we can look at, you know, this very low level called PTX code after the Triton compiler sort of goes over it. And it's actually kind of cool. You can kind of see how the GPU like actually works at the threat thread level. So this is the Triton Galu kernel. It was generated by the compiler and at first it's going to do some of the really basic stuff. So what's it doing here? It's saying, well, I'm going to need to store some values, right? I'm going to need to store intermediate computations. B means actually sort of untyped, sort of basically like bytes. So I need bytes that are sort of 32 bit size. I need floats from doing computations called f. And I need another set of registers that are 64 bits. And, you know, that's another set of registers. And so I have all of these sort of registers that I need for temporary computations. And then starting here, I'm going to start computing basically my coordinates. So sorry, this part is loading the various arguments to the function. So things like the x pointer and the y pointer get loaded here. I starting here, I start computing the coordinate offsets of my Triton sort of kernel. And then once I get down here, this LD Global, this is the code that's used to load the values from x pointer back into my temporary registers. So it's basically saying load R2,R3,R4,R5 using the memory position in RD one. And notice how it's loading four things at once because it's cleverly handling coalescing, right? We know we can get four values for free. We should, you know, operate on all four of these values at once cuz we get them. And then you do the same thing again for you do the same thing again here and then you start to get basically the floating point operations mall f dot 32, which basically goes through and does the 10 h computations. I'm not going to explain all of the different pieces, but, you know, here it's doing, it's multiplying by a constant. It does X to the cube by multiplying the same numbers multiple times. And then it's going to compute here, you know, 2 to the X, but we want E to the X. And so it multiplies by log 2 to get the exponentiated base. You can really see all of the different like literal step by step operations that the GPU does in order to get you the final result. And so I'll skip all over to the end. This is all floating point computations that it needs to do. And then at the very end, it stores the values that it has, R38 through R41 into R D4, which is the memory position of our output, right? So this is kind of like what's actually happening at the low level. And we see that each thread is operating on four values at a time and its temporary storage is the registers, which is the really high speed storage that it has very locally. So we can see, you know, this is gonna, you know, just looking at it be probably pretty fast code, right? Okay, so that was the PTX and we can, you know, go through and see what it's doing for all sorts of things. But now let's go back and actually benchmark things. So we got manual galu, 8.1 seconds, Pytorch time, 1.1 seconds, Couta time, 1.84 seconds, Triton time, 1.848 seconds. So we didn't get any faster, but it was much easier to write Triton code, right? We wrote it in Python. We thought about blocks. We could do vectorized additions. If you're doing more sophisticated stuff, you know it. Basically, Triton will handle a lot of the memory stuff for you. And so it's actually pretty good. And then profiling, once again, we see single kernel launch that consumes all of the GPU. So that's great. And that gets, you know, Triton kernels. The last thing, at least in this sort of, whoops, one second here, that I want to talk about is torch compile. Of course, writing Kuda kernels is cool and it makes you feel really good, but maybe we don't need to do that, right? Like the things that we were doing here were very simple. We were just taking these like, you know, X cubed and like exponentiation operations and we're just shoving them all into a single CUDA kernel. And so maybe we can just do that without, you know, doing much. And so, you know, we've had the several different ways that we've showed you. Now the last one I want to talk about is this thing called Torch Compile, which will take, you know, a non optimized Pi Torch code and it will write more optimized code. And so here it's gonna attempt to automatically do optimizations like kernel fusion and this compiled value is gonna be, you know, equivalent in the actual outputs that it generates. But now let's look at the runtimes, right? So we've got some runtime variation, but basically the same kind of numbers, right? 8.1 seconds, manual, 1.1 seconds Pytorch, 1.8 seconds Kuda, and then 1.47 seconds on torch compile, right? So the punchline here is modern jet compilers are pretty good. It can do optimizations like operation fusion without you having to do very much at all. And if you look under the hood, you can kind of see that there's basically, once again, one thing that happens, this is a sort of fused ad multiplied 10H Triton code. So it's generating Triton under the hood that basically is doing similar kinds of things as a Triton code, but it's actually slightly more optimized than what we did. And so it's getting slightly better performance than even our code. So torch compile is quite nice. Yes.

说话人 2 01:13:49 
First of all, do better. Like you just need to like try to implement their price conversion and see if I'm gonna move like it gets to slash and transfer, right?

说话人 1 01:14:00 
So the question was like, when do you know that I guess maybe the better way to phrase that question is when do you know you can do better than torch compile, right, is sort of the relevant question. And I think for simple stuff like simple operator fusion, or the other thing that it's very good at is optimizing matrix multiplies. So torch compile, as I said before, can do things like if it knows the shape of the matrices, can figure out which kernels to dispat. It is very good at those things. I doubt that you can get much better than that.

说话人 1 01:14:31 
But there are things like if you've seen flash attention 1,2 and 3, those are pretty non trivial optimizations. Like these days, Torch compile and like Jack's like SLA compiler can do those. But that's because we know in hindsight that those are the right optimizations to do. I think some of those things are a little bit non trivial to figure out, like flash attention 3 has additional sort of hardware level optimizations that leverage, you know, the H100 hardware that's not obvious to do with a jet compiler. And so there are some things that I think are quite hard with torch compile that I think you could do better. But in general, like I think the point here is, you know, you shouldn't go home and say, I'm gonna kuda kernel, like I'm gonna write kuda kernels for every single part of my language model, you know, that's probably not a good use of your time. But if you're writing a new architecture with some complicated piece and you're not getting utilization but you think you can, that's maybe the time to really bust out the Triton. Okay. So we're basically at time, but we can quickly go through one last example of Triton. Maybe this will be useful for you in assignment too of doing soft Mac. So one difference is until now we were doing just basic element wise operations and that's really easy because you just operate on each element and they sort of know sort of complexity to those kinds of things. So now let's do Softmax, which is, it has a reduction operation where you have to add across all of the elements. So how do we do that? Well, what we want to do is we want to normalize across each row of the matrix. And you know what we would like to do is we'd like to make this fast. So a naive version of this is going to be pretty slow. And now we're gonna write the Triton kernel. So if I want it to be lazy, the easiest way to do this is, again, actually, you can think for a moment about what the easiest way to do this.

说话人 1 01:16:23 
Now, let's say you want to write a Softmax. You're going to normalize each row of a matrix. And imagine these matrices are pretty small, so you're just writing a kernel for small matrices, right? So if you're doing this, what's the right kind of block design? Well, maybe what we should do is our grid should actually just be rows. So each SM is going to handle a single row. That's kind of the optimal thing to do because if we can fit a whole row into an SM, then we just sum across that row in the SM and then we divide, right? That's great. And so that's gonna be the simple design for our very, you know, naive Softmax kernel here.

说话人 1 01:16:59 
So all we're going to do is that we're going to make the block size basically, sorry, we're going to make each block a row. And so the block size should be number of columns plus, you know, a little bit of buffer to sort of be able to fit all the columns.

说话人 1 01:17:13 
So this is Triton next tower of 2 of n. And that's a nice way of padding out your columns. And then I'm going to make each block of rows and the number of boxes exactly the number of rows. And then I have like Triton Softmax Kernel, which is written in kind of the way that you expect. So now we have a matrix rather than a vector. So we have X pointers, we have y pointers, we need the strides of the matrices. And then we can basically figure out what row index I'm in. I can get the column offsets. This is gonna be the same kind of code as before. In fact, getting the row offsets simpler because each row is a block.

说话人 1 01:17:52 
And then now I'm gonna do basically the same kind of stuff. I'm gonna load in each row into my sort of SMS sort of local memory. And then I'm gonna do computation exactly in a way that looks like a Softmax. I have my row, I subtract my max, I take the exponent, I sum it, and then I divide, which is going to give me my Softmax normalized row and I write it back to global memory, right? No complexity at all. Whenever your computations fit nicely in sm, writing Triton code looks very similar to writing just normal Python code, just with a little bit of load and store and keeping track of where the blocks are.

说话人 1 01:18:27 
Right? So life is pretty simple. Let's go back. Wait, where were we? To the Triton. Here we go. And then we can kind of see how fast all of our different pieces of code are. So I'll zoom out again, just make sure. Okay, so manual time takes 3.7 seconds. Our compile time is 1.3 seconds for torch compile, the Pytorch time is 1.5 seconds and the Triton time is 1.9 seconds. It's still a little bit slow. Torch compile can actually do better than sort of the native Pi torch implementation, especially when it knows about the shapes and sizes of certain operations.

说话人 1 01:19:06 
So finally, we can look in the profiler. The manual Softmax is kind of a disaster. Here you see all sorts of crazy operations happening all over the place. Let me clear this if we go back up here. Okay. Yeah, we see all sorts of operations happening.

说话人 1 01:19:20 
You know, we have X, we have Max, we have some, because we've implemented things naively and we've got memory reads and writes everywhere. The compiled Softmax is just gonna be sort of one few Softmax operation that goes quite fast. And then we've got Pytork Softmax, which is also one could call CUDA kernel call. And same thing with our Triton Softmax. We have our nice Triton Softmax kernel that is a single fuse kernel for everything. Okay, I won't go through the PTX code for this.

说话人 1 01:19:51 
I think, you know, we're kind of at time and I don't want to drag you through that low level again. But hopefully, this has given you a flavor of lower level. CPU programming for the purpose of making language models go fast. And hopefully you'll have fun doing assignment too. Next.

