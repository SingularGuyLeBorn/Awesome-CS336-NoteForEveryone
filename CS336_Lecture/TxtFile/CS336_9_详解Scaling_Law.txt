2025年7月14日 下午 10:14|1小时 5分钟 11秒
关键词:
model scale、data scaling、data model、different scales、different models、model parameters、optimal scaling、new models、compute scale、model training、different data、different compute scales、log scale、scaling law、different scaling law、predictable scaling、scaling analysis、other models
文字记录:
说话人 1 00:00 
About scaling laws. Originally, I think we were going to talk about inference, but I'll take a few minutes to start on scaling laws, and then we'll kind of figure out where we'll go from there. Okay, so the whole point of scaling laws is kind of, well, to begin with, I want you to put yourself into the following scenario, right? So you have a very rich friend and he or she has given you 10,000, actually, let's say hundred thousand, H1, hundreds per month. And you have to build, you know, the best open source LM that you can, right? So this is a somewhat hard task and we've given you some of the tools that you need to make progress on this question. So you know, you can put together your infra team and your systems people and you can put together, distribute a training framework in the next assignment. After that, you're gonna put together a great pre training data set and then you kind of know all about, you know, architectures and so on. So you kind of know you have all the pieces. And so we can turn the crank and we can run the big model.
说话人 1 00:56
And in the first couple lectures, we talked about, you know, all the other various decisions you might make along this journey, right? Like what's the architecture? What's the hyper parameters? Like how are you going to do all these things? Well, you know, I think in some ways the answer I gave you from those early lectures was just pick what other people have done, right? Like just follow llama or whatever other models. But in a way, that's a very boring answer because that doesn't let you push the frontiers, right? Like if you're in like a big frontier lab and you're gonna build the best model, you don't wanna just copy other people. You wanna innovate, right? So how do we innovate and get these optimized solutions in the first place? So that's kind of gonna be the point of scaling laws.
说话人 1 01:37
What we want to do is we want to build simple predictive laws for the behavior of language models and scaling laws are basically this whole idea of being able to take small models, scale them up and be able to do that in order to improve your engineering, right? So one way of thinking about this is the old and pleasant way of doing deep learning is just, you know, train a bunch of big models, tune your hyper parameters so that your big models are good, right? That's just gonna cost tons and tons of compute. Like you can't really easily do that. And so I think the new optimism and if you're sort of following a lot of this, these developments on scaling, you know, you kind of think of this as, all right, we're gonna train a bunch of small models. We're gonna learn a lot of things from those small models, and then we're gonna extrapolate them back up to two bigger models, right? So we're gonna take our smallest models at, you know, the left side of this sort of compute scale here. And I'm gonna learn a lot about what to do and then I'm gonna nail it in one go when I build my big model.
说话人 1 02:37
And the first place I want to start with is just kind of the history and the background of scaling laws. And I wanna contextualize this because I think when people talk about scaling laws, often this is done in like very like messenaic, like AGI terms. So like scaling laws just tell you that, you know, these amazing things are log linear forever and we will achieve, you know, super intelligent or something. But I think scaling laws are actually much more grounded and have a lot of interesting history. And so I'm gonna start there to sort of try to convince you that scaling laws aren't necessarily just fitting lines on log plots, although that is a very big part of what we're gonna do. And then I'm gonna do basically very easy steps. I'm gonna try to convince you that at least for data scaling laws are very natural thing to think about and expect. So as a person, that's kind of brought up in statistical machine learning, you know, my starting point is gonna be statistical machine learning, right? Like what is scaling laws? You know, in some ways, scaling laws are telling us as we increase the amount of data or we change the model size, we expect certain behaviors out of the model, right? And if you go back to something like machine learning 101, and if you remember your like VC dimensions and rod mocker complexities and so on, in some ways that's the theory version of exactly this. So, you know, I have on the top, you know, generalization bound for how the generalization bound for the excess risk of learning amongst a finite set of k hypotheses. And we see that should scale as a 1 over square root of m, right? In some ways, that's a theoretical version of a scaling law where we're making predictions about how fast our error should decay as a function of that.
说话人 1 04:17
On the bottom, we might have something a little bit more exotic if we're doing generative modeling and we're our generative model is a really flexible non parametric class. What we might do instead is we might, you know, fit some sort of smooth density. So in this case, you know, well, our prediction is that the L2 sort of error of estimating a density is going to be up or bounded by some polynomial and to the beta over 2 beta plus 1, right? This is what some people might call non parametric. So, you know, theorets have been thinking for a very long time about how sample size especially should relate to error, right? This is a very classic problem that people have thought about in machine learning theory, but these are upper bounds, not actual realize loss values and really scaling. Sorry, in some sense, the leap from thinking about kind of the theoretical side of how should data and model size relate to performance and going to the empirical side of saying, actually our bounds are bad, but maybe we can actually fit these things empirically. And this is a fun trivia fact, you know, or arguable trivia fact.
说话人 1 05:20
What is the first scaling lost paper? And actually not many papers cite this one, but I think probably the right first scaling Law Papers is a paper from 1993, new rips from Bell Labs. And you might recognize some of these names. These are, you know, kind of theorists and some of the people that have done really classic work in machine learning theory like, you know, Vapnik and Karina Cortez and other. And I take an excerpts because, you know, I was reading this paper actually just, you know, preparing this lecture earlier. And it just struck me how you know, ahead of its time in many ways, this paper was, right? It's saying, you know, training classifiers on large databases is very computationally demanding and we need to figure out which ones are good before actually training them. And so what we're going to do is we're going to propose a new predictive method that predicts how good a model is going to be without actually training the whole thing, right? And that, look, that sounds a lot like scaling loss. And you'll see this later, but you know, they have a functional form that's basically like, oh, the test error of a model is, you know, expressible as some irreducible error plus a polynomial indicator. And you're like, Han, that looks a lot like a modern scaling law. And they even do the thing where they, you know, train a bunch of small models, they fit their curves and they're like, oh, we can accurately predict the behavior of the model further out. So as with many things, I guess, you know, scaling laws partially, you know, thought about Abel Labs way back when. And of course, there's others that I think, you know, have thought about related ideas in scaling, not just scaling models, but also really the modern mindset, I think, of thinking about scaling. There's another paper that often gets mentioned in sort of the history of scaling laws on Banco and Brill, who was studying sort of how does the performance of a certain kind of NLP system scale with the amount of data? And they have, you know, what looks like, you know, very often a modern scaling law, you know, log access data on the x axis, performance on the y axis. And, you know, they're basically arguing, well, look, we can get really dramatic performance improvements just by scaling up data. It's very predictable. And, you know, maybe we should consider the tradeoff spent between, you know, spending time and money on algorithm development versus just collecting more data. And you're like, that sounds a lot like what a lot of this pre training stuff is thinking about.
说话人 1 07:36
And then finally, you know, one of the things that I think people have thought about recently and in the past is, you know, is this thing really predictable? What are the right functional forms? And as early as like 2012, you know, people are really thinking about, all right, like, are these things actually predictable? You know, is power law, like for example, power 3 and power 4, are those really the right functional forms for predicting the behavior of models? And of course, all of this, you know, just to remind you, right, is thinking about the behavior of models on the y axis, the capabilities as a function of the amount of data that you have on the X axis, right? So that's the relationship that I think has been really classically studied, what you might call data scaling in all these cases. And if you're interested in like kind of the earliest like large scaled neural Scaling law paper, that would probably be Hesnus at all in 2017. I believe they were at Baidu when they did this work. They showed that for a range of tasks, machine translation, speech, and I think like some vision tasks, they showed that essentially error rates fall as a power law. And they even have this nice plot that I really like to refer to. And when people are discussing scaling laws, that really your expectation should be that there's three different regions in the behavior of a model, right? Initially, you start out a best guess. You then enter into a region where you're kind of predictably scaling the model. That's the power law region. And then there's another asymptotic region where you're approaching essentially the irreducible error of your model class. And I'll kind of highlight that. I think, you know, there's been in the last few years a lot of talk of new phenomena, things like, oh, emerging capabilities or like scaling compute being a new thing or systems being really important. But had you been reading sort of hessis in 2017 carefully, you would have seen essentially all of these things. You know, they say actually, you know, it's really hard to do predictions by scaling law when models are at random performance because suddenly you can leave the random region. They talk about computational limits. Actually, you know, if we can scale, it means actually scaling by compute is really important. And then finally, they even say things like, you know, maybe we should do things like quantization, because if we have predictable scaling, then that means we should be willing to pay for model accuracy with compute, right?
说话人 1 09:53
These are all very modern ideas, but I think a lot of the early scaling law papers, I think kind of understood fair. Really intuitively because, you know, once you see these spots, you kind of see that actually with predictable resource investment, you get predictable capabilities improvements, right? So that's in some sense, so the core, not quite history, but I think context that has really shaped scaling loss. All right. Any questions so far on kind of a context? This is mainly just kind of data scaling, but I wanted to make sure we go over it carefully.
说话人 2 10:28
Yes, be natural for like wave like scaling. I was wondering like what like, is there cases where there isn't.
说话人 1 10:36
Scaling where that doesn't get better? Yeah, so the question was, you know, it's natural or maybe it may be arguably natural to expect scaling. Are there cases where we don't get scaling or we get different kinds of scaling. And I think one way of thinking about this is if you're measuring kind of training loss or like, you know, held out versions of training loss, then I think scaling is very natural, right? Like all of classical statistical theory says, you know, things should converge and when they converge, eventually they will get better, right? At some sort of very asymptotic sense.
说话人 1 11:08
But we do see non scaling behavior. There was a really interesting competition a few years back called like the Inverse scaling prize, where they were looking for things that like scale inversely as models got better. And a lot of these are very niche things. Like, you know, models tend to copy better. And so if you want to like suppress copying behavior, it becomes really hard for really strong models, for example. But I think one sort of like thing that ties a lot of that together is, you know, if you go really far out of distribution where the behavior is not well specified by the data, then you can get all sorts of behaviors like no scaling at all or inverse scaling or what have you, right? So in some sense, you can think of this as like the extension of the classic like deep learning robustness problems. Cool.
说话人 1 11:53
Okay, so now I'm going to talk about scaling behaviors of LLMs, like just essentially going through several kinds of empirical results. I'm going to walk you through data scaling in particular, and some examples just to convince you that this is a very natural object to expect. And then we'll talk about model size, which is a, you know, different kind of a thing. So scaling laws, I think, are fairly well established, and they seem to appear very often in kind of many variables, right?
说话人 1 12:26
You see scaling and compute on the x axis. These are all taken from capital and scaling wallpaper, which I'll refer to extensively in this lecture. So the x axis here is log compute, y axis here is log test loss. And on the right, you see similar kinds of scaling, both for dataset size. So this is the amount of data and parameters.
说话人 1 12:46
One subtlety I'll mention here as I sort of talk through this is, you know, when we scale things like data set size or parameters, we're always assuming that the other variable in this case, if you're scaling dataset size, the model size is much bigger than you can saturate with the dataset size, right? Because obviously if you have way more data than, you know, parameters, eventually you're gonna sort of ask some tote out, right? So in all of these, we're trying to avoid the asymptotic regime. They hold in also pretty non standard settings. They'll hold for downstream tasks. They'll hold out of distribution, which is what's being shown here from the Caplin paper.
说话人 1 13:18
And so, you know, in some ways, power law relationships seem to appear more often then we might initially expect, especially for these OD or other variables. So I want to talk through data scaling laws first, because I think they're the most intuitive, like at the very least, I think the theory for that, it's fairly clear. And so to be precise, when I say something like data scaling, what I mean is just some sort of simple formula that maps dataset size, which I'm gonna refer to as N, to our excess error, right? Excess error is the error beyond the irreducible regime. And, you know, if you recall that figure I refer to in hesnist, what we are going to expect is monotonic logistic looking curves. And really our interest is primarily going to be in the power law region to the irreducible error region. Like, of course, it's very interesting to also ask questions about what happens in the small data regions as we leave random guessing. But that's much harder to reason about. Whereas I think this right tail, actually, I can hopefully convince you that this part is actually a very natural thing to expect power loss scaling. So, okay, right.
说话人 1 14:31
So the first empirical observation that we have, right, and this is kind of the thing that I'm gonna convince you as natural, is when we plot on the x axis dataset size and on the y axis test loss, then on the log plot, model performance is linear, right? You might call this scale free or you might call it power law. These are more sort of physics oriented terminology and sort of this was established, you know, by many people. But you might refer to Kaplan to see many examples of this.
说话人 1 15:05
So I think, you know, as sort of the previous question sort of brought up, right, we kind of expect error to be monitored. We train on more data, error goes down fairly obvious. The part that is less obvious is the precise functional form of this scaling, right? So when I say it's a power law, it's linear in log space. And then, so what is the implication of that, right? If something is linear in log, that means that there's a polynomial relationship between your x axis and your y axis, right? And why is polynomial decay natural? Well, I'm gonna walk you through two examples and both of those are gonna result and some fairly natural polynomial decay. I'm gonna start with the simplest possible example, right? Like this is just gonna be, you know, even stats 101 rather than, you know, machine learning 1:01.
说话人 1 15:52
So what I want to do is I want to estimate the mean of a data set, right? And estimating the mean is a task of estimating a parameter, right? I can ask for what's the scaling law, what's the error of my mean estimation task as a function of data, right? So I can write that down. Well, you know, my input comes from a Gaussian and the task is to estimate the average. I've written those out in the blue box above. And what's there? Well, by sort of very standard arguments, right, the average is gonna be also distributed as a Gaussian with the standard deviation divided by n. So I'm gonna get, you know, Sigma squared over n is my estimation error, right? This is the expected squared error of my estimate. And if you look at, this is Paul O'milann, and just to really drive the point home, you know, you take the log of both sides of this log of the error on the left and log of sort of n on the right hand side, you know, I get exactly log of error is equal to negative log n plus 2 log Sigma, right? So this is exactly the kind of thing we expect. And we expect the slope of 1 if we were to fit a scaling law for mean estimation.
说话人 1 16:55
So now, you know, equipped with this new knowledge, you might say, alright, I'm gonna go around and I'm gonna look at what the rates are for estimating different things and that will tell me about what I should expect for data scaling. And so you might say, oh, what I expect is 1 over N. You might expect one over square root of n for agnostic learning and so on and so forth. So we should expect to see some like pretty nice round numbers on the slope here, right, of a log. I should expect to see like 1 or point five.
说话人 1 17:21
What do we actually find empirically when we look across these papers, right? Just to sort of call them out in hessness for machine translation, we see the negative 0.13. For speech, we see negative 0.3. And for language modeling, we see an exponent of negative 0.095. Those are all much slower than the one over n or 1 over square root of n rates that you might expect when you're just fitting simple functions. So why might this be? Okay, this will be the last math slide of this lecture. And then we can go to just fitting lines on log lock plus the rest of the time, but this will hopefully drive the point home of why we might see these particular slopes.
说话人 1 18:05
So we know that neural nets aren't just estimating the mean, right? Or it's not even fitting a linear regression, right? They can fit arbitrary functions, right? So let's turn that into an example and let's work through that example.
说话人 1 18:15
So my input is x 1 through xn. So I have n samples and I'm going to put them uniformly in the 2D unit box. And I want to estimate some random, not random, some arbitrary regression function y equals F, right? And I'll assume F is smooth and so on. If you really want to be precise, right? But there's some regularity conditions here. A simple approach to estimating a regression function F is just to cut the 2D space up into small boxes. And within each box, I can measure the average of the y values, right? Like a very simple non parametric aggressor. It's just cut the space up and then to estimate what's gonna happen.
说话人 1 18:51
Now, informally, if we pick, you know, I'm gonna have square root m boxes. Now, each box is gonna get square root of n samples. And now my error is gonna be 1 over square root of n. And if you sort of follow this logic through the more dimensions, you'll see that in d dimensions, this is going to be error is equal to n to the negative one over d.
说话人 1 19:08
And then sort of my overall scaling, if I were to take log plots of the whole thing, is I expect the slope of negative 1 over d, right? And so why do I walk you through this example, right? I walk you through this example because if you have flexible function classes, what people call non parametric function classes, you expect dimension dependence and therefore the slope of the scaling law to actually move sort of much more slowly. And in some sense, the slope is telling you almost precisely kind of the intrinsic dimensionality or the ease of learning this task. And people have argued this more formally or sort of more literally. There's been several sort of theory slashing empirical papers arguing that really the reason why we get these sort of exotic or non standard rates of learning is that it is closely connected to the intrinsic dimensionality of the data. And the sort of, for example, the plots of these. Predictions, the dash lines and these purple circles are somewhat close, although, you know, you don't want to read too much into this because estimation of intrinsic dimension is an extremely difficult problem and as difficult as modeling the data overall.
说话人 3 20:16
Oh, yes. I mean, I guess it's related to point me at the end since much months ago, but like, yeah, how do you generate data that has an underlying intrinsic dimension at all from a simulation perspective?
说话人 1 20:30
Yeah, so the results here, well, if you want, for example, to generate data, that's actually not too hard. You could like write down a function that takes in like five variables, right? And then that would be as long as all five of those variables, like don't, you know, cancel each other. That's a five dimensional surface and you can add a little bit of noise and you're good to go. The difficulty here is that they're actually doing things like, you know, training on seafar and then they're having, you know, different, they're trying to estimate the intrinsic dimensionality of CPR. That's a, you know, much harder task. Okay. And data scaling laws are quite useful. You know, I was going at this from a, let me explain to you, scaling loss perspective, but you can actually use scaling loss to do many interesting things, right? You can make engineering decisions of various kinds using data scaling laws. And people do in fact do this. For example, you know, you might say, well, how does dataset composition affect performance, not just data set size? Well, if you're changing the test set, you know, Kaplan, it all has a really nice figure showing actually data composition only affects the offset, not the slope. And what that would mean is it says if you want to pick a really good dataset, you don't have to necessarily train your models at a huge scale. You can scale them down and do your data selection experiments on much smaller models. And the shape of the expected sort of, as we mix sort of different data, we might expect certain kinds of sort of shapes. And you can use regression and other kinds of techniques to try to figure out, for example, optimal data mixing using scaling laws. And people have written several papers on this topic, although, you know, as with all data selection sort of research, a lot of this seems fairly tricky to execute reliably.
说话人 1 22:20
There's other also interesting questions that you might ask, right? There's a lot of discussion these days about, you know, are we running out of data, right, on the internet? And so once you start asking those questions, the other interesting and important question is, well, can we just keep training on the same data we have? What's the diminishing returns property of that? Right? And so there's interesting work extending scaling laws to multi epoch training, basically arguing that there's a sort of effective sample size. And after about four epochs, you know, you have rapidly diminishing returns as you repeat more and more data. And by modifying sort of the usual scaling law, you can basically get a version where you have amount of effective data and unique tokens that sort of diminish out as you increase the amount of repetition.
说话人 1 23:09
Finally, I think one interesting sort of combination of these two ideas is if you're thinking about sort of data selection in the large data regime, right? Like imagine you're going to be training on trillions and trillions of tokens right now. What would be better? Would it be better to repeat high quality sources like, you know, Wikipedia and perhaps your secret pirated books 10 times or would it be better to include new data, right? The fact that you can either repeat data or you can include more data right now has multiple sort of axes on which you can sort of optimize your data mixture. And there's also been some interesting data scaling work, this one from CMU folks on essentially trading off between repeating data versus picking lower quality data that's new. And so all of this really is a really natural extension of what I sort of already taught you, which is if you assume that there's a predictive power law relationship, right, and that this power law relationship hold sort of on a per mixture basis, then you can fit these sort of scaling lock extrapolations and then get an estimate of how good your data is going to be at scale. So that's the starting point, which is data scaling, right? And hopefully I've convinced you at this point, both sort of empirically and conceptually, that it's natural to have, you know, log, log linear relationships between data and error. This relationship seems to hold very robustly across domains, across different kinds of models. And you could kind of have a nice clean theoretical understanding of what is happening here. And once you do this, you can use this for all sorts of purposes like picking optimal data mixtures or whatever else.
说话人 3 24:52
Yes, model size picks on the data scaling box.
说话人 1 24:57
Yeah, so as I was kind of saying. Back in, well, not this slide, but back in this slide, when we think about kind of the data size scaling, the model is always picked to be really large. So the data is not saturating your model, right? And you want to kind of avoid being in this irreducible error regime. So the model is always picked to be large enough that you're in the power law region whenever you're only varying data.
说话人 2 25:30
For like all of them, that one really big model size or like this, each point under different size model.
说话人 1 25:36
Yeah, for example, for this plot in particular, it's for it's like one big models. Okay, when you're looking at, for example, compute scaling on this axis, then data and model scale jointly at some like, you know, pre ordained ratio. Any other question on? Alright, so now I think we get to move from data scaling to, in my opinion, slightly more mysterious kinds of scaling. And we're gonna talk about model scaling next. And I think this is a more practical engineering set of questions that we're now gonna try to answer. So you're in charge of, you know, building and shipping a really large language model. And there's a lot of interesting ideas out there, right? Like you could train the latest, you know, state space model. You could train a transformer, you could use Adam, you could use SGD, right? People invent all sorts of new tricks. Which ones are worth scaling up and which ones are not. You could also take, you know, your limited compute resources and spend them on different things. You can train models for longer or you could train bigger models, right?
说话人 1 26:45
For given flop, you can trade between these two and you could also do things like go and collect more data versus get more GPS. There's a lot of different sort of things that you can do and the scaling laws allow you to have a pretty simple procedure to just answer all of these questions, right? So I'll go through the classic sort of Kaplan scaling mock paper if you're interested in these topics, I encourage you to read it. It's just kind of a gold mine of all of these kinds of observations. Some of it is old, but it's, I think, still unmatched in the thoroughness of all of the things that I really studied in a fairly nice unified setting. So architecture wise, you might start by asking like Transformers versus LSTMs, right? Which one's better? Well, you know, the brute force way might be that, you know, scale up LSTMs and up to like GPT three level and then, you know, you can figure out whether it's good or not. The scaling law way is much simpler, right? You basically train a bunch of Alice TMS and transformers across many different compute thresholds or compute levels. And then you kind of see what happens as you scale them up. And I think the trends here are fairly clear, right? Like no matter how many layers you have on your LCMS, there's a pretty big gap, right? Pretty big constant factor gap between transformers and LSTMs, right? And remember, this is a log scale.
说话人 1 28:00
So this is kind of saying something like, you know, I don't know what the exact numbers are, but imagine this is like 15 times less efficient, right? Then no matter where you are on this plot, you know, the LSTM is, let's say, 15 times less compute efficient than a transformer, right? So there's a constant factor compute penalty to using LSTMs, at least in this form. You know, you could zoom out and say, well, there's a lot more architectures, you know, which ones are, you know, really good and worth doing and sort of some of the classic papers. This one is by Ek and others at Google have done exactly this kind of scaling work where they took a bunch of architectures on the right here and they basically scaled them up. So the x axis is the amount of compute, the red line is basically each architecture, and the green line is the transformer baseline, right? And they ask like, oh, can any of these alternative architectures match or out, you know, outscale the transformer, right? And what do they end up? Well, actually, the only thing that seems like really strongly and reliably meet the transformer is, you know, gated linear units and mixture of experts. And once you know it, that's exactly the kind of stuff that people are doing today, right? And so this is kind of the scaling law version of that same idea of saying, like, how would you have come to the conclusion that we should be doing Switch Transformers and Glu and for example, not the performer, right? And the scaling law provides some clear evidence of why you might want to do that.
说话人 1 29:29
Optimizer choice, I think, follows a similar thing. This one's from hessness. You know, they compare SGD and Adam. They find very similar to before this kind of constant factor gap, right, in compute, in this case, dataset size. But of course, that translates to compute in the effectiveness of Adam versus SGD. You know, RHN in this case is recurrent highway nets. You can sort of ignore the details here. You kind of see the point of how you would do this analysis rather than the specific results that are shown here.
说话人 1 30:02
You know, in the beginning, I also said something like, oh, you know, depth versus width, like what should the aspect ratios be? That was one of the hyper parameter topics we talked about. And we see sort of similar sort of analysis, but in scaling law form from Kaplan. I think this one's intriguing to me at least, because, you know, we might think that deeper layers get dramatically better, right? That there's like clear separation between the number of layers. But we see at least here that, you know, there's actually a lot of sort of slop. One layer is really bad, but a lot of the other sort of layer choices sort of remain pretty stable.
说话人 1 30:35
And hopefully this is reminiscent of kind of that slide I showed back in the architecture lecture where I said, well, you know, the aspect ratio, the ratio of width of depth, you know, roughly something like 4 to 16 or something was a pretty natural number. But there's a really wide basin in which you are approximately optimal. And the scaling law analysis also backs that up.
说话人 1 30:56
One important subtlety that I would do wanna point out, and this one bites people every now and then, is that not all parameters are equal. Like often you want to do, you know, parameter scaling analysis, but if you were to say, count embedding parameters as part of your model, well, you get like a pretty different scaling law. You get this, you know, kind of weird looking thing that like slightly bends over here.
说话人 1 31:20
Whereas if you only consider the non embedding parameter, you see that much cleaner result that I showed you before, right? So embedding layer parameters don't really behave the same and they don't show the same kinds of sort of log linear scaling as the non embedding parameters when you account for them. And there's sort of related work on saying like not all parameters are the same on recent papers on scaling mixtures of experts where they're also sort of trying to figure out like what does it mean to be a parameter when you have such sparsely activated parameters? And in those kinds of papers, they sort of try to derive essentially things like equivalent number of dense parameters in order to sort of try to normalize the number of parameters in Moe.
说话人 1 32:06
I've showed you this plot earlier in the hyper parameter selection, but hopefully now actually you see the full context, not just the original sort of the hyper parameter choice question.
说话人 1 32:17
We know that in many cases, I'll go back, let's say to like here, often what we'll see is scaling lock curves that look like the following. You'll often see that the slope of the curves remain very similar, they're non crossing and that there are sort of constant factor offsets between these curves. And whenever this is true, what you can then do is you can take a slice at a particular level of compute or a particular set of hyper parameters and analyze the hyper parameter tradeoffs very carefully, assuming or, and sort of be sort of safe and sort of scaling that up. And so when you go to Kaplan paper, you'll see exactly these kinds of analyses being done, especially I think the center one, the aspect ratio plot is definitely worth looking at. You know, they're not just sort of scaling up and down models. They're actually taking different slices. So different sized models, 50 million, 2,70 million, 1.5 billion, and they're looking at how the aspect ratio changes the loss. And they kind of see that, oh, actually the shape of the curve, not just the scaling slopes, actually remain similar. And this means that, you know, I can pick an aspect ratio between 10 to 100 and anything in between will work fine at all of these different scales, right?
说话人 1 33:29
And so this is, I think, important to think about. I think initially when you're trained in sort of deep learning, you know, model training, you think about hyper parameter tuning, but you want to be sort of scale aware in how you're tuning your hyper parameters. And that's a really big difference in mindset, I think, between kind of the scaling law style approach and sort of maybe what you've been trained or what you've, you know, naturally think about in terms of, oh, let's just tune these models at a small scale, right? And so the same is being done kind of for feed forward ratio and for attention head dimension. You know, you're varying various aspects of scale and you're trying to see whether sort of the minima remains similar.
说话人 1 34:09
Okay, another important thing, next, actually maybe not next lecture, but next lecture I'm going to talk about sort of practical case studies almost of how people have scaled up models. And we'll actually see that batch size and learning rate are actually two really tricky things that you have to deal with carefully when you scale models up, right? So when you scale models up, you're gonna have to maybe think about, you know, the optimal learning rate will be different across model scales. And if you're doing that, then actually also maybe the optimal batch size might end up varying as well, right? Because those are often co linked. And so we need to think about what the right way of scaling batch sizes and how batch size interactions scale and also learning rates. So I'll talk about those.
说话人 1 34:53
Oh, for the next couple slides. So batch slice from the system's lecture, hopefully you remember it has diminishing returns past a certain. Point. So up until a certain point, so you know when the batch size is smaller than the noise scale, we're on the left hand side here, right? Increasing batch size is almost equivalent to taking more gradient steps. So that's, you know, roughly saying if I double my batch size, it's as good as taking two gradient steps. And that's a really good place to be, right?
说话人 1 35:21
Because now you've got the system's power of being able to paralyze across the batch, right? While having the optimization efficiency of taking two steps, but past a certain point, you're gonna have ineffective scaling, right?
说话人 1 35:33
Where now your sort of noise scale and your batch size are the same and the additional samples in your batch that you're taking, you know, they're not reducing useful noise. It's getting dominated by kind of the curvature of the bias term, so to speak, of the curvature of your optimization landscape. And one really useful thing to think about, useful sort of analysis object is this notion of a critical batch size. And the critical batch size you can think of at is kind of this threshold point where we go from perfect scaling to dip strong diminishing returns, right? And you can sort of analyze this in theory and sort of OpenAI papers on critical batch sizes do this. But you could also analyze this empirically. And this is another thing that's been studied sort of, you know, in the scaling law kind of way, you can kind of see, you can estimate the point at which sort of progress slows. So you can estimate empirically what the critical batch size point tradeoff points are. And you can also basically train bigger and better models. And one really interesting thing is as you try to, you know, improve the loss, so you're going left side here. So you're making losses better and better, your critical batch size ends up getting smaller, right? So the smaller the loss target, the bigger the overall batch size that you can be. And so one of the things that this leads to is, for example, if you look at the llama 3 training report, you'll actually see, for example, that they'll like increase the batch size after a certain point, or they'll do things like increase the batch size as they train because as their loss target gets smaller, your batch sizes can in turn get bigger.
说话人 1 37:18
So as we increase both compute and model size, like what's the right thing to do? Once again, we can do kind of a scaling analysis. This is from Kaplan and you can try to figure out, you know, as we increase the amount of compute, what is the optimal backsize? And what we kind of see is that, you know, as we increase the amount of compute, we can actually have reasonable sort of parallelism. The number of total steps can stay the same, at least within this compute threshold, the number of total steps can stay the same while sort of getting the batches bigger and bigger. And if you fix the amount of batches, of course, the number of steps is gonna go up and up.
说话人 1 37:55
So this is good news, hopefully, for data parallel processing. So that's the batch size story. The thing you can, you should maybe remember, because I think critical batch size are kind of a messy concept, is that a, there's a sort of diminishing returns point, the critical batch size. That's one thing. The second one is that it does seem to follow a pretty predictable scaling, often as a function of your target loss. And given that you can figure out, you know, what is the right tradeoffs that I can make in terms of systems efficiency and my optimization progress.
说话人 1 38:29
As I said before, the other aspect of this, right, is, you know, you've got your batch size and then you've got your learning rate. And those two are fairly closely linked for each other. And I'm gonna talk about new P at much more extensive length in the next part of the scaling lecture.
说话人 1 38:46
But this is kind of a really important, I think, broader idea. So you could do one of two things. And I think this figure will allow me to talk about both of these. So let's look at this left plot first. What's labeled standard practice, right? So when you train a transformer, right, what you're basically gonna see is something like this left thing here, this standard practice. So the optimal learning rate is gonna be at different points and the wider the model, right, as you increase your model size and your MLPs get wider and wider and wider, the optimal learning rate is going to be pretty small. And as you make your model smaller and smaller, right, your losses, of course, are going to go up because your model is less, you know, less expressive, but also the optimum learning rate is gonna also go up, right?
说话人 1 39:28
And often, you know, people say there's a rule of thumb. It's like one over the width is the right rate at which you should scale the learning rate. More advanced people will actually fit. Basically take these curves, find the minimum, and then fit a scaling law on the optimum learning rate. And so there we can see that this is a predictable decay in learning rate. And maybe we can fit a scaling.
说话人 1 39:49
I'll talk about this more in the next set of lectures. But an alternative one that I think many people have started to adopt and I think is a really interesting thing to think about. Is that you can actually re parameterize the model. And in particular, you know, you can do things like scale the initialization or scale the learning rates of different layers based on the width. You can scale the variance of the initialization based on the width of the model, as well as multiply the output in the forward paths of different layers of the model. And if you do this in a way that you know, is dependent on sort of the width of the model, you end up with a parameterization in the model whose learning rate is supposed to be more stable, or at least, you know, in the original paper, exactly stable across scale, right? So you tune your learning rate once and you don't have to do anything else. That optimum directly transfer.
说话人 1 40:46
Actually, you tune it here on the smallest one and that directly transfers to the very largest scale, right? And this is the idea called new P. There have been sort of this original paper that I'm showing you is called with newp. There's been other variants, meta with the release of law before clinks to how have invented something called meta P, which I'm not quite sure what it is yet, but you can sort of see that a lot of labs I kind of thinking about this, right? Because if you're gonna have to, you know, rely on predicting what the optimum learning rate is, then you have to do all sorts of tricky scaling off fits. And maybe this is very unstable, but if you can reparameterize your model, then, well, maybe you don't have to do any sort of retuning at all. Of course, that's, you know, way more optimistic than what happens in practice. But hopefully this gives you a sense of, you know, why this is really cool and really interesting, right?
说话人 1 41:34
Scale aware initializations. Cool. Any questions? Up until this point, I feel like I've sort of gone through a whole bunch of scaling architecture and parameter stuff. So I mean, I'll stop for a moment here in case anyone has any questions.
说话人 2 41:51
I really get the intuition behind like, if we wanna lower loss target, we wanna increase the batch size. Are you maximum?
说话人 1 42:00
Yeah, like, so when you have a lower loss target. So what you want to do, right, is the smaller the loss target, the kind of more sensitive things are. And in the same way that like you're gonna be lowering your learning rate, right, you wanna also increase your batch size in order to deny, right? Like the more sensitive the target that you have, the sort of more precise your gradients potentially have to be. One way of thinking about is like, you know, as you're cooling down in your learning rate is going up, maybe your batch size should increase as well. Cuz the learning rate and batch sizes sort of affect each other inversely. 2 special iPhone is only true for NLP, not true for you or for computer vision. I'm not sure there is a sort of related opening I scaling paper for sort of multi modal models, but I'm not I don't remember what that says about critical backsides for those.
说话人 1 43:02
The noise scale. Yeah, the noise scale, at least in sort of this figure, if that's what you're asking about, this is a kind of theoretical analysis. It's basically about the gradient noise that you expect from random sampling within the batch. So this is not like a, you know, precisely empirically measured quantity of these things. Oh, alright. So one thing I'll caution, and I think this is a big caution for a lot of scaling law works, is that scaling laws are very nicely behaved for log losses, right? So we train on, you know, next token prediction cross entropies when your scaling law targets are those cross entropies, the very easy works very well. But if you're trying to do downstream tasks, right, you're trying to like directly scale on benchmarks, behavior is much less predictable.
说话人 1 43:56
So here on the left side, this is from Ek's paper comparing lots of different sort of like hyper parameters and architectures. You see that the number of parameters, which in this case is a circuit for compute, and the negative log perplexity is, you know, very nicely linearly correlated.
说话人 1 44:13
And what this is basically saying is, well, it doesn't matter what your like depth or width or like precise setting of the hyper parameters are the only thing that really matters is your total compute expense nature, right?
说话人 1 44:23
This is a very simple and nice story. But then you take these models, this was back in 2023. So, you know, people were still kind of doing super glue accuracy. And, you know, you basically say like, okay, but what's the downstream performance of these models? And while now we don't see a very nice linear relationship anymore, right? We see this like totally different thing where certain models are much better than others and certain architectures are better than others. And so you might not expect exactly this kind of scaling property.
说话人 1 44:53
And we've seen variants of this story play out in many different places if you follow the literature on state space models. That's one thing that we've seen, you know, in state space models. You know, we see really nice predictable scaling, like the ones on the left, but often for certain capabilities like in context learning or for QA, people have shown that, you know, these models maybe do less well. So it's important to not take this like perplexity scaling as the same thing as downstream scaling. And you want to be a little bit cautious whenever you're doing these kinds of analysis. Okay.
说话人 1 45:30
So maybe this is not surprising to some of you, but hopefully you know this is surprising and convincing, which is that, you know, if we want to make lots of engineering decisions like hyper parameter choices, architecture decisions, we can do a lot of that before training, right? Like we can train these models at small scale across several orders of magnitude compute, and then you scale that up in order to try to predict the behavior of models, right? So the scaling law based design procedure, it's pretty simple. You train a few smaller models and these smaller models should span a couple orders of magnitude compute, you establish a scaling law of some kind. So you know, see that at least on the models that you train that there is a clear log, log linear relationship. And then based on this prediction, you can set optimal hype in many cases. In fact, you know, these scaling laws won't really vary too much. Their slopes will actually be the same, in which case, sort of the corollary to this is you can just train a few smaller models and you know the results of those small models will transfer surprisingly well to larger models in many of these cases, but not all of them. Learning rate being an important exception, for example. Okay, so that's how you do things like hyper parameter selection and architecture selection.
说话人 1 46:42
Now I want to talk about one very important use of scaling laws, one that's had kind of an outsized influence on, you know, how we pick sizes of models, how we think about data efficiency and so on of these models. So I think back in the earlier days when people were beginning to scale up these models, there's a really core question that you need to ask, right? Do we need more data or do we need bigger models? In some sense, you know, back in 2021 to 2023 or something, you know, data was way more abundant than compute, right? So we didn't need to worry about, you know, the total data limitations. And so the one limiting resource is compute, right? Your total number of flops for your training budget. That's kind of the limiting resource. And you can then spend that resource in many different ways. You can spend it on training on lots of data with a small model, or you can train one giant model on very little data, right? And both of those extremes seem very wasteful, right? Like if you have a teeny tiny model pumping in tons and tons of data, doesn't seem use one. In reverse, if you have a giant model with like 10 tokens, also doesn't seem very useful.
说话人 1 47:48
And so this was sort of a core question for many people. And so there, you know, simultaneously several authors sort of proposed sort of joint data model scaling laws to try to answer this question. And so what are those, right? I've been talking about scaling laws in essentially one variable exclusively up until this point, and that one variable has varied. It has sometimes been parameters or data or compute, but we've not looked at joint scaling, right? And so data model scaling laws are things that look like this.
说话人 1 48:18
These two sort of equations here are both like functionally equivalent to first order and describe the tradeoff between the amount of data and the amount of models. So the top one from Rosenfeld, you know, is basically saying there is a part of the error, one part of it that decays polynomial data. There's a part of the error that decays polynomially in the model size. And then there's an irreducible error term that cannot be removed even if I scale both the data size and the model to infinity, right? Same effect with Kaplan, but here they're sort of thinking about irreducible error rather than reducible error. So there's no constant from here. So this seems kind of arbitrary because I don't think there's any sort of, you know, top down reason why this has to be the correct functional form. But this provides surprisingly good fits to the joint error that you see in data and model.
说话人 1 49:11
So this is from, I believe, Rosenfeld. They show this nice 3D plot of this is the amount of data. This is the amount, this is the size of the model, and this is the loss on the y axis. And the surface that's being fit is their functional form. The dots are their runs. It might be a little hard to see from the back, but the surface fits the dots almost exactly. And despite the fact that this functional form is kind of ad hoc, like it's pulled out of a hack, it is surprisingly accurate. This one's from Rosenfeld as well, where they basically say, okay, I'm only gonna train on essentially the small half, right? Models that are small and data that is small, right? So on this sort of left bottom, and I'm gonna extrapolate to models that are sort of both large and trained with more models. And how good is that fit of like joint extrapolation? Well, quite good, right? So if you look at the error, my sort of real values are on the x axis, my predictions of the error on the y axis, and they're sort of almost exactly right, both on like sort of image net and on wiki text. So this seems pretty good.
说话人 1 50:22
And so, you know, for a fixed compute budget now, what can we do if we go back to, for example, capital and we see similar things being done here, we see sort of joint scaling of compute and data. So in this case, parameters are on the x axis. The colors represent compute. And so there's sort of a third axis of data that's being implicitly varied in order to vary the total amount of computation as you go shift. And on these curves, the parameters are being varied while the compute is being held constant. And so the amount of data is gonna vary. So Chinchilla, I think many of you have hopefully heard of, is probably the reference in solving this problem, right? So both Rosenfeld and Kaplan came up with kind of this joint scaling functional form. And then, you know, both of them sort of noticed that it was possible to use these functional forms to optimize the tradeoff between compute and data in various ways. But for various reasons, it's, you know, basically it's hard to fit these sort of functional forms precisely. And the details, like the sort of learning rate, shapes being different are important.
说话人 1 51:34
And so Kaplan sort of had one estimate that was quite far off from what was later, in some sense, validated to be optimal. And so the Chinchilla paper by a bunch of Google authors sort of was an attempt to really empirically try to nail down what is the right tradeoff between the amount of tokens and the model size, assuming that your goal is to get the best model for the smallest amount of training flops, right? So they have three different approaches, approach 1,2, and 3 for basically fitting different curves and making scaling predictions. These blue dots are the models that they trained and the basically the lines are predicting different optimal parameter sizes for different flops. And hopefully most of you kind of know the Chinchilla ratio. That's something like, you know, 20 tokens per parameter. And that comes from exactly this, right? Like if you take each of these points and you multiply it by 20, you're gonna get roughly the, or sorry, multiply it by 20, you'll get the token count. And so if you multiply the parameters by that, you'll get the flops. The difference between sort of the Kaplan results, which were basically estimating one set of token to parameter ratios, and sort of the Chinchilla ones.
说话人 1 52:47
One of the reasons is because of learning rate schedules, right? We know that we train models with cosine learning rates, right? So cosine learning rates are gonna look something like this, right? It goes up and then it comes back down and then it's gonna cool down all the way to a minimum learning rate at your bottom. But you can't, one thing about cosine learning rates that sort of trips everyone up all the time is you can't truncate them early, right? For cosine learning rate, you have to sort of go all the way to the end in order to get a valid model, right, you have to get a cooldown phase all the way to the end. If I truncate a model in the middle, this is not the same as starting a model from scratch and training it with a cosine learning rate somewhere in the middle. And this was one of the sort of contributing factors.
说话人 1 53:29
There were others as well, leading to the capital estimates being pretty far off from the later sort of more improved estimates provided by the Chinchilla paper. So what do the change of authors actually do? Well, they have three different methods of trying to estimate the optimum tradeoff between tokens to models. And each of these methods are gonna sort of provide different scaling coefficients, right? Scaling coefficients for the model size and scaling coefficients for the data size. And kind of surprisingly in this case, they're getting 0.5 on both of these for methods 1 and 2. And method 3 is providing pretty different or slightly different estimates. They're about off by point o three, but we'll talk about that a little bit later. Kaplan and all you see is way off than any of the three estimates, right? So we'll go over each of these methods.
说话人 1 54:25
Each of these makes sense. They make sort of different assumptions about scaling, but they end up with very similar estimates at the very end here. So method 1 on Chinchilla is to basically take the minimum over curves. And so what does that mean? Well, you basically overlay all of the different training curves that you have. So you can see here on the x axis is different flops on the y axis is sort of the training loss.
说话人 1 54:56
And I have models trained at many different sizes, and if. Of course, you know, each of these sizes are gonna be trained with different amount of tokens. And so they're gonna reach a different sort of total flop as I sort of go through training right now, what I'm going to do is I'm going to look at the lower envelope, right? The set of sort of points or checkpoints that proved to be optimal under any compute budget. And I can take these models and I can look at, okay, what were the actual parameter sizes of these models? And you can see that sort of the total compute on the x axis here and the number of parameters as well as the corresponding tokens all forms a relatively nice scaling law.
说话人 1 55:35
And so this is kind of the minimum envelope method. It's basically saying, I expect the minimum training loss where I optimize over all the model sizes to actually be optimum in flops and sort of to call back to some earlier papers, right? If you look back at the earlier sort of Caplin paper and other scaling laws, you see exactly this already being done. You see, you know, different models being trained with different sort of parameters and different compute scales. And we're taking sort of the minimum across these. And we've already seen that the minimum forms of scaling law. So this is building on this observation that the minimum across many different training curves across compute will should form a power line. So under that Assumption, you can get fairly nice fits. And this gives, you know, one estimate that is quite consistent with others of point 5, point five.
说话人 1 56:31
Now, the other one, this, I think if you were to pick a single canonical way to do the Chinchilla analysis, this would probably be the one. And in some ways, I think this is the most conceptually straightforward one, which is the isoflop analysis.
说话人 1 56:46
So to do the ISO flop analysis, what you do is you pick a bunch of compute scales. So each of these colors is a different amount of Q. And what I'm going to do is for each of these compute scales, I can essentially have models with smaller parameters trained with more data or more parameters trained with less data, right? So I'm gonna sweep over my sort of model sizes for each of these swaps.
说话人 1 57:08
And then I can look at the minimum of each of these curves. I can either pick the minimum point explicitly, sort of non parametrically, or I could fit quadratics onto each of these and get the minimum point of the quadratic.
说话人 1 57:20
But in either case, sort of the argument is fairly simple. The argument is I should be the case that this minimum itself follows a predictable scaling law, and thus I can extract from it sort of the optimum sort of parameters per flop. So that's the minimum points across all of these. And I can also extract the optimal number of tokens per flop. I can read that out by sort of dividing my flops budget by the number of parameters, right? So I can get those simultaneously. And you can see that once again, this gives very clean sort of results that are consistent with method 1, right? So we can compare that with before. This says for the eventual Chinchilla model budget, you want 63 billion parameters. This one's a 67 billion parameters. The two are quite close. Okay, the last one honestly is just a little bit messier.
说话人 1 58:12
And this goes back to kind of that Rosen felt paper. If you have a functional form like this one, right? Like this from Rosenfeld, a very natural instinct is to say, I'm just gonna train a bunch of models, varying both n and n, right? And I'm just gonna do curve fitting. I'm gonna fit this curve onto whatever I get a thing I get out of my models, right? So I'm gonna train a bunch of models and fit that 3D shape. And we know from Rosenfeld, it's reasonable to some extent to fit these, right? So you've got all these dots, which are the models. I fitted a curve. That's all this sort of heat map color that you see on the left. And then you can sort of back out what the implied ISO flop should look like from these dash lines. But if you look at this, you know, hopefully you see that the scaling law fits and like sort of the curve fits here are just not quite as good as the fits in the other plots, right. And, you know, if you look at the coefficients, the Chinchilla method 3 just gives way different estimates in terms of the model size and total token count than the others, right?
说话人 1 59:17
And actually, this was a mystery to me for a long time. I think some of my students were like, why is method 3 so different? And I said, I don't know, maybe scaling laws are just sometimes noisy. I don't know how many of you know this, but this is a really fun trivia effect, or not trivia fact, fun piece of trivia, let's say.
说话人 1 59:35
So last year, some folks at Epoch AI, I don't know what motivated them to do this, were curious enough about this result that they went and tried to replicate method 3. And you know, the it was very difficult to replicate it because you don't have the original data for all of these training runs. So they actually like went to the extreme of actually looking at the plots and using sort of a. Forensic tool to extract the values of the points from the plots. And based on that, they could actually replicate the original result. And kind of the funny thing is they showed that actually the curve fitting was the bad part. Like their data and their approach was good, but actually when they fit the curve, they didn't necessarily do it right. And so the original fit had residuals. If you're familiar with regression, you know your residual should be zero mean centered because otherwise you should be, you know, offsetting your predictions to make it zero center. Their residuals are non zero and then they, you know, fit it better. And then when they did fit it better, well, actually their optimal estimate, you know, almost exactly matched methods 1 and 2.
说话人 1 01:00:41
And so this is one of those funny cases where actually, you know, the original authors had both the idea and the data right, but because of a minor issue in curfeiting, they kind of had it wrong. And the replication actually makes it more correct than before. Usually replications sort of disprove things. But in this case, actually the replication just show that the original result was correct all along, which is, I think, a pretty cool result. Okay, so the final thing I want to talk about with kind of this set of Chinchilla results is, you know, we're talking about training optimal scaling. So you have a fixed flops budget. I want the best possible model possible.
说话人 1 01:01:21
But really, I think that the story has really shifted when sort of Chinchilla was written and the Kaplan paper was written, you know, LLMs were not really a product yet. And so really the name of the game was everyone wanted the most biggest, flashiest, most intelligent model, but they didn't care about the inference cost of actually deploying these systems. But nowadays, you know, what we really care about is inference cost, right? Cuz these systems are actually products. They generate revenue. You know, you have a cost associated with the revenue. And so we've seen over time that actually the tokens per parameter has steadily grown, right? Like GPT three was 2 tokens per parameter. Chinchilla moved us to 20 tokens per parameter and forbid people played around with sort of 20 tokens per parameter stuff. But then, you know, very quickly people realize actually what we care about is, you know, really good intelligence at really small parameter sizes. And so people have really started to scale up the number of tokens per parameter very rapidly. And I think I saw yesterday that, for example, the most recent models were trained on 30 trillion tokens, right? You know, people are really pushing the limits on the tokens to parameter ratio because really you would much rather pay the upfront cost meant to pay the ongoing operating cost of in running inference on a really big expensive model.
说话人 1 01:02:42
Cool. Last thing, you know, that is kind of a fun side thing that I wanna end with is to say, you know, these results are pretty robust and easy to replicate.
说话人 1 01:02:54
A few years back, one of my students, Ishaan, was really interested in, you know, really pushing diffusion models for text forward. And so one of the things that we had to do was to say, this is a whole new kind of model. We don't know what the optimal token to parameter ratio is. We don't know if this thing even, you know, reliably scales the totally different kind of generative model. What do we do? Well, turns out, you know, if you just fit the same kind of playbook of saying, oh, we're gonna do, you know, isoflop analysis for auto aggressive models. We get almost exactly the Chinchilla thing without too much effort. You know, you do the same kind of analysis on diffusion models. Wow, we see, you know, very similar kinds of curves, even though it's a pretty different generative model entirely. And then if you plot sort of the minimum across of these, well, you see very predictable scaling for both separated by a constant offset, right? Like I don't bring this up to say, you know, because I want to particularly push to you diffusion models, but just as a really random sort of case study or example to say, you know, these scaling laws don't necessarily need to be these very cherry picked examples. They seem to happen pretty naturally as you're sort of working on new models or working on new environments.
说话人 1 01:04:03
So, okay, you know, this is, you know, the, to put together this last part, right? Log linearity is not just about sort of one dimensional things where we think about data, they extend to sort of model parameters, they extend to total compute. And so that lets us, you know, make all sorts of hyper parameter in other decisions. That's kind of this first part. And they're also letting us make really smart resource tradeoffs, right? They let us make tradeoffs between sort of big models versus more data. And we saw that in kind of this Chinchilla analysis. And, you know, it's kind of remarkable how cleanly things like the ISO FOP analysis turnout.
说话人 1 01:04:43
So, all right, that's all I got for basic data scale or basic scaling laws. We did a recap or of Kaplan as well as Chinchilla today. And hopefully now you're on board with this idea of data scaling, model scaling, and using scaling loss to sort of optimize all. The aspects of your model, without actually going all the way to the large scale training runs. Thanks, and I'll see you all Thursday.
