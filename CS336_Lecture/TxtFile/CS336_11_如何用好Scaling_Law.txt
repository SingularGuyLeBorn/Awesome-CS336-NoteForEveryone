2025年7月14日 下午 10:15|1小时 17分钟 36秒
关键词:
model size scaling、different scales、different models、large scale models、data scaling、scaling loss、scaling laws、optimum scaling、big models、parameter models、small scale、different model sizes、predictable scaling、fit scaling、d scaling、model ratios、model initialization、width scaling
文字记录:
说话人 1 00:00 
Laws as part of their design process, right? So motivation from last time and today is what's the best practice for scaling a large model? We want to have large language models with nice hyper parameters and good architecture choices. And you know, I've already told you about Chinchilla and using scaling laws to validate some of this. But I think, you know, you should have rightfully skeptical questions about scaling lots, right? Like it's curve fitting on a log plot. Is it really as good as, you know, I said it was last lecture. So does Chinchilla's approach to scaling laws actually work? You're finding this out in your assignments. You know, if you fit an Isoflop, is that really telling you about the right token tradeoffs? Can you use this stuff to really set optimal learning rates? And should we be picking sort of particular architectures or parameterizations to scale nicely?
说话人 1 00:53
So the last paper or the newest paper we talked about with like lots of detailed scaling studies in last lecture was the Deep Mind Chinchilla paper, right? And after that, ChatGPT happened and kind of the competitive landscape of large language model building really changed and people just stopped publishing anything about, you know, data and scaling and all these things, right? It was sort of very secretive. I've talked to, you know, people at some of the frontier labs before and ask them, oh, you know, like, what are you guys doing for scaling? And they're like, no, we will not tell you anything about what we do for scaling. And so we have to sort of rely on other sources for how scaling happens in practice. And there have been several, you know, competently executed large scale models that have done scaling. And so, you know, last year in this lecture, I covered 3 risk GPT, deep seek, LLM and mini CPM. And as a nice side note, you know, last year I had to really strongly justify why I was covering these like Chinese models, so to speak.
说话人 1 01:56
But this year, thankfully, hopefully you're all already excited to hear about Deep Seek rather than me trying to convince you that this is the right thing to listen to. In the year since then, I've looked at a lot of the models that have come out. I'm actually the hall in terms of new scaling law insights and papers is actually much sparser. I'll briefly mention some results from Llama 3, which came out at the later end of last year. Hunyam Large, which is a Moe model from China, and then Minimax 0,1, which is a linear time sort of hybrid attention model from a long context model that came out this year. And all three of those have some scaling studies, but really nothing quite as extensive as deep seek or mini CPM, which have really been the gold standard, I think for modern scaling law studies.
说话人 1 02:46
So that's one part of what I want to talk about today. I want to make sure you guys have an understanding of, you know, what scaling looks like in a real, you know, semi production model. And the other thing I want to talk about, which is an important deep dive, I think, is the mup method that I mentioned last time. So MUP is this approach, just as a recap of last lecture is, you know, when we train these models, as we make them bigger, we need to change certain hyper parameters, right?
说话人 1 03:17
On the left hand side of this plot here, you see that as you make models wider, in this case like MLP, you make them wider, the optimum learning rate sort of shifts downward. So you need smaller learning rates for these bigger models. And that's a really big problem potentially because then you need to hyper parameter tune your learning rates at the very large scale, right? And that's gonna be very computationally expensive. It's gonna be a huge problem. If, on the other hand, if we could sort of parameterize our model differently so that, you know, the learning rate that's optimal just stayed the same forever across all the scales. You know, that's great. That's really simplified our search process, right? We would like all of our hyper parameters and really choices in general to remain stable across scales, right? That's the ideal.
说话人 1 04:01
And new piece is a very interesting class of approaches. And, you know, it teaches us some pretty interesting sort of ways of thinking about the problem. So I'm gonna actually go through some of the details in the math. In the year since last time I called this, there are a couple of very nice tutorials on up that came out. So I'm gonna follow those cuz they have math that's pretty easy to follow. And then I'll talk about some work that has come out doing sort of third party validation and evaluation of up style.
说话人 1 04:31
So, okay, the focus of the first part of this lecture, which is the case study, is gonna be on three models. I talked about three additional more modern models, but actually the details in those are much more sparse. And I think the lessons you learn are primarily from these three papers here. So that's gonna be my focus for the first part of this lecture.
说话人 1 04:50
So I'm gonna talk about Cerebris, GPT, Mini CPM, and Deep Seek. And each one of these will have actually a pretty different mix of scaling strategies, and it'll also have different things to teach us about.
说话人 1 05:00
How to get scaling right. So we'll get started. Cerebrous GPT is the first of the models and sort of scaling things that I want to talk about. It's a large family of models. It's trained point one to 13 billion parameter models trained with the chinchiller recipe. So roughly the same number of like token to parameter accounts as is optimal. And, you know, they have an interesting core finding. The series risk folks actually are pretty interested in a lot of these like scaling and primectization studies. And they have a really interesting core finding, which is they scale up this MUP thing that I mentioned before. And they find that it makes scaling a lot more stable and a lot more pleasant to deal with, and just to kind of show you the punchline, right, you've got test loss on the pile and you've got sort of the scaling curse here of like the three  GPT in blue.
说话人 1 05:55
This is with standard parameterization. You've got mu p in orange. This is the model that they, you know, also train. We're using the maximum update prioritization and they show that it scales more nicely, if not better, than things like Pythia or GPTJ. So that's nice.
说话人 1 06:12
And the thing that I want to emphasize here is that this is kind of one of the few, if not first public validations of VP. We know that all of or most of the labs that are doing LM scaling pay close attention how they parameterize their networks, their initializations as a function of the scale of the model as well as things like per layer learning rates are things that people pay close attention to you to make scaling much more stable. And so things like new PR pretty important in this space.
说话人 1 06:43
Well, on the 4, for example, the paper for that isn't out and I don't know if they, it will be out, but they talk about a technique they call meta P, which is a variant of this as well. So what they show is that when they train models using sort of standard parameterization, they find that, you know, they have sort of big oscillations kind of around the predicted scaling point. So that's the this dash line, you know, they have kind of oscillations. Do you know the fact that, for example, they have to adjust the learning rate as a function of scale? And so it's hard for them to kind of really get the predicted performance sort of exactly right, which is this dashline using their scaling recipe.
说话人 1 07:21
On the other hand, what they find is, you know, if you have sort of the 3 risk GPT, sorry, mu P scaling, then you got this orange line, which is much closer to the scaling law fit for this new P version. And so their claim here at least is that using this alternative prioritization allows them to get much more predictable scaling, much more nice hyper parameter tuning.
说话人 1 07:45
We're gonna see this in more detail. I'll return to this slide again once I've sort of gone through the mathematical derivation of new P. But in case you're ever interested in implementing this thing, some of the cerebrous GPT folks and in general, the kind of artifacts that the cerebrous research folks are putting out is very helpful for MUP because they have this big table in the appendix that really just tells you exactly the difference between the standard initialization and parameterization or SP and the maximum update version or VP. And you'll see that, you know, I'll just give you kind of the one liner version.
说话人 1 08:21
Basically, every non embedding parameter is initialized with 1 over the width. And then the learning rates per layer are scaled down by 1 over kind of the width, right? So the interesting difference from standard parameterization, even if you're already doing sort of one over width scaling on the initialization, is actually there's per layer learning rates are different and I'm gonna get to that later. I'm gonna do a full derivation of this result. But you can kind of see here this kind of like nice quick reference. And also, if you want to implement this thing, this gives you very easy ways of implementing new P.
说话人 1 08:55
Another interesting thing that we also see in some of the other scaling strategies is that you combine these strategies like new P, which makes hyper parameter selection stable with very aggressive scaling. So what they do here is they scale down their experiments all the way down to 40 million parameters. They do extensive hyper parameter search on this proxy model and then they scale things back up using sort of new P to try to keep hyper parameters as stable as possible. And so this is sort of what they see in their small scale hyper parameter search. Each one of these dots is a model run and there's sort of a hyper parameter associated with each one of these. And then they pick the minimum across these runs, giving them essentially they're sort of hyper parameter grid. This is a very like clean approach to hyper parameter selection. It's unclear whether, you know, this kind of this level of aggressive scaling down is really what you want to do if you want to train these like really large models tools.
说话人 1 09:52
But this is kind of a one strategy that we see also in many CPM and deep seek, like training much smaller Seraga models and then trying to figure out how to. To stably scale them back up, and that's gonna be kind of a theme that we see throughout.
说话人 1 10:06
And yeah, if folks have questions, please stop me. I could you maybe I'll stop here for a moment in case anyone has questions for the 3 risk GPT piece. Although maybe you'll be clearer once I talk about the VP derivation later in the section.
说话人 1 10:22
There's another paper I want to talk about mini CPM or another, you know, artifact, I guess. And for whatever reason, I think mini CPM hasn't been talked about quite as much, especially in sort of like western academic circles. But at least for me, this was one of the first sort of releases or papers I saw coming out of like a Chinese research group where they had done some like really cool in depth, you know, scaling and other kinds of research. It really felt like, you know, stuff coming out of the frontier, right? And to give you an overview of what they do, their goals here is they want to train, you know, relatively small language models, but use a lot of compute to train really good small language models. That's their ostensible goal. And in doing so, they do a lot of careful scaling computations. They also once again use MUPI to stabilize and simplify scaling when they sort of end up scaling these models, not in size, but in terms of the amount of data and to try to convince you that, you know, this is a paper worth following.
说话人 1 11:24
Right. You know, at the time that they were trained, this was a remarkably good 1.2 to 2.4 b models. It beats out, you know, most of the two b models that were out there and it matched many of the modern 7B models, at least modern as of 2024 standards. I mean, now of course, you've got even better 7B models. The arms race is fierce.
说话人 1 11:47
But this should give you a sense that at least, you know, given the amount of compute and technology available, you know, back in mid 2024. This was actually really at the frontier and they did something right to get models of this quality. And so much like 3 risk GPT, they do essentially, they have to have, you know, some strategy to get scaling right, right. So stepping back, right, you're gonna do a really big model run. What do you have to do? You have to pick hyper parameters. You have to make sure those hyper parameters scale nicely. And then you scale up your model, right? So we can do the same thing as the 3 risk GPT folks. We can try to pick hyper parameters at a small scale, hope that they sort of stay stable, and then scale everything up. And the way to do that would be to use something like new P. And this has exactly the same kinds of strategy at play here.
说话人 1 12:34
You see, so for embedding, you don't really do anything. You just scale up by a constant. Whenever you have some sort of like residual connection, like an MLP, you scale it by the square root of the number of layers. You initialize it by sort of fan into our one over the base with and then the learning rates are also scaled by the width of the model. We see basically the same strategy or the same kinds of scaling factors appear as the Cerebras GPT case, right? And they also end up with very similar parameters as serious CPT, the same kinds of scale embeddings, similar learning rates off by a factor of 2 or so. But generally you end up in similar places as these kinds of hypo parameters. And then what you do is once you have this, you're sort of relying on your optimum learning rates to remain stable. So you're just going to keep those roughly fixed. And we know that the aspect ratio is a pretty important thing, so we just fix that after figuring out what the right one is. And then you scale up the overall model size going all the way from, you know, 9 or 30M all the way to half or one, a billion parameter models, right? And so what they have is like a roughly 5 x or maybe a little bit more compute savings going from the smallest models that they've got to the largest sort of pilot run models that they have.
说话人 1 13:56
And now you can use this, and then you can sort of figure out whether you have sort of optimal batch sizes as a function of scale. So, you know, you want to figure out be crit, which is the critical backsize. If you remember correctly, this is the critical batch size is roughly the diminishing returns point, right? So as models get bigger, their losses get lower. As their loss gets lower, you can make use of bigger and bigger batch sizes. So the critical batch size is roughly telling you for the given model size and scale that I'm operating at, what is an appropriate global batch size for me to be training these models with. And so much like the cap on paper, they follow a similar kind of recipe, you know, the plots look different from the cap on paper, but the underlying strategy is kind of the same. What they're trying to figure out is what is the critical batch size for training or the optimum batch size in this case. So for training different models, and they're trying to find relatively predictable scaling relationships between the batch size and, for example, the data size or the loss size and vertical columns here sort of represent a single training curve. And then sort of the quadratics are sort of being fitted to try to identify the minimum. So the red line here is the minimum across all of these points as we go upwards. And this is trying to tell us the optimum batch size for a particular choice of model size and data set size. And then at this point, you know, you can follow the same logic as the CAPPLINE paper for identifying the batch sizes. Basically, you, we reproduce the same kind of plot if you remember the Kaplan paper and the critical backsize discussion from two lectures ago. If not, you can kind of pull up the lecture slides. You'll remember that. Basically the thing that's highly predictable is the loss that you're trying to train through, the terminal loss and the batch size of like the critical batch size point. And so we see that once again, much like in Kaplan, you see a log, log linear relationship here between the target loss or the terminal loss and the batch size that you want, right? And so from this, you know, you can kind of figure out what batch size you're gonna get, right? Because if you have a particular target scale, you can use scaling loss to figure out what is the loss that I expect to get. Once you know the loss that you expect to get, you can use that to back out what batch size you can kind of operate at, right? So there's a fairly clean trend. Polynomially increase the batch size of loss decreases. Now batch sizes do sort of shift around as a function of target loss and thus compute. So we have to fit a scaling loss for that guy, but we already did newp. And so in theory, right, if the approach works at all, what we should now get is we should get that the optimum learning rate here is stable.
说话人 1 16:38
So on this plot, we're seeing essentially different model sizes from sort of small models in the light colors to their biggest models in the dark colors, and you see them sort of varying different learning rates. The big models, they're only running for a little bit for compute reasons. But what you see is a fairly clear trend. And, you know, once again, very consistent with some of the earlier results that we've seen. In Kaplan at all, where you have a relatively wide basin and then sort of sharp increases as your model becomes very unstable, right? But the important thing here is that the minimum remains fixed across relatively large orders of magnitude, right? From your small model to the big model, the minimum or at least tied with the minimum is at the exact same point at roughly 10 to the negative 2 learning rate. And so this is a nice sort of piece of evidence or some validation that properly scaling your model initialization and properly scaling your per layer learning rates allow you to avoid tuning learning rates over and over, or even fitting scaling laws on learning rates in order to try to predict what the optimal learning rate is. Okay.
说话人 1 17:46
And then the final thing is, you know, you might want to figure out essentially model size to data tradeoffs if you're training small models, you're gonna be probably over training your model. So at least you want to justify it yourself why you're training on so many tokens. And so you might want to replicate something like the Chinchilla analysis. So the many CPM people had a really cool or nice innovation. Others have done similar things, but I think they were the first to really popularize this in the LM setting, especially in the context of Chinchilla style scaling is kind of the following. So let's say I want to fit a Chinchilla scaling law. When I do that, what do I need to do? Well, I need to very the number of tokens and I need to vary model sizes, right? And so when I do that, I'm gonna fix a model size and I'm gonna train a model for longer and longer.
说话人 1 18:37
It would be nice if I could sort of early stop and take the checkpoints of this model and have that be sort of the difference or changes to the dataset size, right? Because earlier checkpoint CS data, it would be nice if I could use a single run to collect all of this sort of data scaling things.
说话人 1 18:51
Unfortunately, what I'm showing here, right, is that kind of the cosine learning rates for different data targets are different, right? So if you have a very small amount of data, you have a cosine that goes up very quickly, or sorry, that goes up. The warm up is always the same, but a very fast cooldown, right? You train for a little bit and then you come down very quickly. If you have a lot of data, then you're gonna very slowly come down to the end. And so your learning rates between a small data training run and a big data training run will be different, right? This is a very key point, right? Lots of people get tripped up by this. You cannot use a single run of a cosine learning rate model and try to get early checkpoints and reason about data scaling behavior based on that, right? This is this bites people all the time. And so in order to avoid this, what you would normally need to do is you need to train a model from start to every single endpoint, right? So you have to train it to every single target. And so this kind of takes you to n squared runs, right? Like some of the runs are small, but you have to basically run lots of runs, each one with a target termination point, rather than using a single run and collecting checkpoints. It feels kind of senseless that we have to do this. So the mini CPM folks popularize this idea of a WSD or warm up stable decay learning rate. And so this plot on the left really shows you, you know, what's going on here. Normally, what we would train with is something that looks like this.
说话人 1 20:13
Cosine learning rate shown in yellow here, right? It goes up. There's a warm up period, usually very short to get to your full learning rate. And then there's a cosine that goes all the way down to kind of a termination point and maybe you stay at your minimum learning rate. This is all, of course, optional. You might terminate here as well. You might go all the way to zero, right? And so cosine learning rate looks like this. And the issue here, of course, is that if I have a different target, the cosine is going to be totally different. So everything past the warm up can't be reused.
说话人 1 20:41
Now if you look at this new WSD, which is basically a trapezoid learning rate, what it has is three phases. It's got a warm up face that's the same as a cosine. It's got a stable phase that's flat and then it's got a decay phase that rapidly cools down the model down to its minimum learning rate. And of course, you can have variations of this. You can go up, down and then stay stable at your minimum. You know, you can do any of these variations.
说话人 1 21:03
But I think in general, the simplest form to think about is warm up, stable, decay, terminate, right? Why is this nice? This is nice because you can reuse the stable part, right? So the thing that you do is if you want to do Chinchilla in almost one run, what you do is you sort of warm up, you have a stable run all the way to the end and then you cool down. And then if you want to figure out, oh, how would my model have been if I use less data, you rewind the checkpoints and then you do another cooldown, right? And now you've got a exact warm up stable decay learning rate shape without having done the training from the beginning, right? So this is a very nice thing. The fact that the stable part essentially is flat allows you to do Chinchilla style scaling or data scaling in a single training run or for mostly the cost of a single training run.
说话人 1 21:49
And a lot of people now do this. Okay, so they work very well. Mini CPM, I think, popularize this. And, you know, I think a lot of people have. Since then adopted it, and we see a lot of WSD style schedules in many places. They you see curves that look kind of like this. If you have a cosine learning rate schedule, you'll see essentially relatively predictable smooth decay towards your terminal loss like this yellow line here. If you train with WSD, you'll see much funkier learning curves that look like, you know, the curves that I have here above them, the darker lines, right? So you've got your warm up phase, which doesn't really show up in this training curve, it's so short. Then you've got your stable phase where it sort of goes down normally. And then as soon as you hit your decay phase, like the cooldown part, your loss really rapidly drops off until you hit your sort of either zero or minimum learning rate point, at which point you've got in your terminal loss, right? So these losses may look very disturbing to you, but they are actually pretty normal when you're training with these kinds of sort of rapid cooldown learning curves. And maybe the point to make here is at every single token count, you see that the warm up stable decay curve, you know, the minimum point beats or matches the cosine learning rate. That's not always the case. There can sometimes be cases where cosine works better or WSD works better. But in general, I think a lot of the, I think things that people say here is that the two learning rates are roughly comparable, but WSD has the additional nice advantage that you don't have to worry about your termination point. You can repeatedly cool down to get checkpoints of different data accounts. Okay, cool. Okay.
说话人 1 23:33
And then of course, there's other things that have appeared for trying to estimate in chillikers. Some folks, a collaboration of like udub, formerly UDUB and apple folks had this paper on estimating sort of the Chinchilla penalty. That is when you keep adding more and more data, you know, how much worse is your loss then if you had, you know, scaled according to Chinchilla? So you have your sort of peel line here, which is sort of m equals 20, you know, 20 tokens to parameters. And you can sort of think about, okay, what happens if I train with 320 tokens to parameters? Well, then you have a separate parallel scaling line and then you have another line, which is the circles, which is what if I train with 640 or the darker one is there. And so the thing that they show is actually instead of doing this WSD style thing, another thing you could do is you could try to figure out, okay, how much does my model degrade as a function of sort of higher tokens to parameter ratios? Well, that turns out also to have a fairly predictable shape and you can sort of extrapolate that based on sort of degradation at small training runs. I don't think I've seen large scale training runs using this idea, but it's kind of an additional cool thing to know that essentially you could do Chinchilla in almost one training run by sort of extrapolating the access token penalty at a small scale as well.
说话人 1 24:54
So, okay, going back to mini CPM, now we have the tools that we need. We have the WSD learning rate, which allows us to essentially do one training run. And that one training run allows us to, you know, have both variations, sorry, not that one training run allows us to vary data as we go along. And then we have multiple training runs for different model sizes. That gives us all that we need to do Chinchilla analysis. And they use method 1 and method 3. If you remember what those are, method 1 is you overlay all the learning curves and you take the lower envelope and the lower envelope of all the training curves is supposed to be roughly a power law. And then method 3 is you basically jointly fit this equation 2. You have here, you hypothesize this two variable scaling law and you kind of fit it to all the data that you have in kind of curve fitting, style, fashion. And then that, you know, allows you to solve for the optimum token to data ratio through that fit. So they do both of that. They do see for Chinchilla method 1, fairly clear, although not perfectly linear trends that allow them to essentially go from compute to token ratios. And their primary approach that they use to justify a lot of their design decisions is the method 3. It's the curve fitting. So the kind of contours that you see here is the curve that they fit, the dots that they have here is the small scale runs that they did to fit potential up parameters. And just to, you know, sort of justify what they do, they find very high token to parameter ratios, like so high that I feel like this is an outlier that doesn't really agree very closely with most of the other literature. They argue that llama style architecture should all have a higher ratio because of, you know, improve data quality and improved model efficiency. But they're token to parameter ratio estimates are really high, 192 tokens per parameter, which I don't think I've seen anyone else derive. I think other people have done replications of Chinchilla. I don't think anyone's ever really done or argued 100 for 100. 92 tokens to parameter. Regardless, you know, we have seen that recent models like Lama three have significantly higher data than model ratios. We also don't really see diminishing returns like these models aren't way worse than the equivalent.
说话人 1 27:13
Chen Chala scaled like one of two models. This kind of suggests that with careful optimization and careful tuning, we should be able to go far beyond like the 20 times model size rule of thumb, right? So if there's one thing you take away from this set of sort of, you know, these last two slides, maybe not necessarily that you should trust whatever scaling off fits that mini CPM did, but rather that sort of the Chinchilla analysis isn't really, you know, a strong constraint, right? Like 20 times model size is just a starting point. You should be, you know, feeling free to significantly increase that token to parameter ratio.
说话人 1 27:50
Finally, the curfeits that they get are generally pretty good looking. So this is the scaling lockers for essentially data and model size scaling and perplexities on code in English. They do have some really weird outliers that I don't really understand why they get these, but their sort of fitted scaling laws are generally pretty good as they increase the amount of data on their relatively small model. So this is one example of a, you know, large scale training run scaling recipe. So I'll stop here. Things like WFDR are probably new to you. So, you know, if you have any questions, please feel free to ask. Or any of the other bits, including like the chinchiller replication MVP, and so on.
说话人 2 28:35
Okay, sure. I'm currently the main adaptation of MU keys in terms of initializing.
说话人 1 28:42
The weight of revenues, right? So the question was, the main change in newp was initialization. So there's two things that will happen when you derive and you implement newp. One of them will be the initialization will change. And the other thing will be that the learning rate changes or the learning rate changes per layer, right? And that is probably a more exotic object than many of you are used to. The initialization actually is not that different if you're already using like a standard like Kai Ming style initialization that's already won over the fan in one over, sorry, one over the square root of the fan, which is gonna be already the right thing. Whereas the learning grade normally, like unless you're doing something really exotic, you're using like a global constant learning grade, right everywhere. So that's gonna be a big difference from what you're normally training with. So you can think of that as like the practice practical difference for a lot of the VP implementations. Yes, the ordinary.
说话人 2 29:39
Was kept constant. With the QST, he saw that the curve was very close to the cosine decay.
说话人 1 29:50
Right? Yeah, so you're talking about this curve and you're saying like, oh, when we're in the stable phase of WSD, like when we're up here, the curve remains pretty close. And like, why is that? Well, it's kind of close, but also not really, right? Like if you look at this last curve over here, you know, there's a big gap before we enter the decay phase between cosine and WSD. And I think this is like one of the pretty interesting, you know, mysteries about deep learning optimizers. Clearly, you need a stable phase to kind of get you far from your initialization, but also the cooldown phase is what gets you most of your gains and your losses, right? If you don't cool down, this is a gigantic loss in your losses. So the cooldown is actually really critical. And, you know, a lot of the gains from cosine versus here, like this relative gap, this is all from cooldown, right? And so a lot of the optimizer learning rate design is about this balance between like, how do I keep learning rates high to travel far from my initialization, but still have, you know, good decay on my learning rate to be able to unyield my loss down to a very low value.
说话人 1 30:56
So the other paper I want to talk about is Deep Seek. This is the original Deep Seek LLM paper from 2024. And in many ways, if you read the original Deep Seek Lom paper, you'll kind of, you can know that these are like very serious science people, you know, cuz they do a lot of very careful scaling ablations and they're really trying to get it right when they scale up. And that's kind of an attitude that's shared amongst sort of the players that get scaling, right? So, you know, they have 7 and 67 b parameter models, you know, at the time, very high performance relative to, you know, llama, which is really the primary competitor at the time. And, you know, there at the time, I guess, you know, Llama 2 and Mistral were kind of the big players. Deep Sea comes in and they're able to match the performance, not quite the flashy impact of Deep Seek v 3 coming in and sort of matching open ais gbd 4,0. But you know, for a first time sort of attempt, this is a pretty remarkable result. And so let's kind of dig in.
说话人 1 32:00
And try to understand, all right, like what did deep seek do that allow them to go from essentially zero to, you know, at least for open source state of the art at the time, right? So, and I think deep seek more than, you know, most other players. Maybe the only comparable one being mini CPM is very open about a lot of the experiments they did and the approach they use to choose a lot of these hyper parameters. So immediately we see one difference between deep seek V1 and mini CPM and also series risk GPT, which is that they don't use any new P and they're gonna directly try to estimate both the optimal batch size and the optimum optimal learning rate. So it's like a really direct method, you might call it, and requires kind of a strong belief in scaling laws. So what they do is they, you know, take two relatively small models and they have, they run a grid over different batch sizes, they run a grid over different learning rates, and they get losses across this grid. They do the same thing at a larger scale. And, you know, you can get kind of the optimum batch size and learning rate, right? And so they're saying, alright, well, this is a pretty wide basin, so we don't maybe have to be, you know, too scared about messing this up. And so then what they do is they know that, all right, the choice of learning rate and batch size are both relatively forgiving. But we do want to get the order of magnitude of these things correct, right? So how do we get the order of magnitude of these things correct? Well, you know, what we're going to do is we're going to train a bunch of models with different amounts of, you know, non embedding flops. And we're going to change essentially across a grid the parameters that I had before, both the batch size and the learning rate. And by varying these, you know, we're gonna have the optimum batch size and the optimum learning rate, sorry, across these different scales, right? So you can imagine basically making these grids across many different flop scales and basically marking down a star for each one.
说话人 1 34:07
Perhaps unsurprisingly because it's the scaling law lectures, these things seem to kind of follow a scaling law line, at least for the batch size, things seem more clear. And you can kind of fit a line to here and you can, you know, extrapolate out to the big models that you're going to train, what your optimum batch sizes should kind of look like, right? They do the same thing with learning rate and they sort of fit this line and they say, oh, these are the two learning rates we're gonna use. It might be because the points are being plotted on top of each other, but I find this line to be particularly, not particularly, like somewhat suspicious looking. I mean, I could have probably could have horizontal line and that would have also looked okay. This one, I don't know, even as a scaling line enthusiast, I'm not quite sure I would, you know, bet my life on this one to pick the learning grade, but they did. And, you know, that's how they get the learning grade.
说话人 1 34:58
Now, they also kind of follow best practices at the time. They do a Chinchilla style analysis and they use once again a WSD style learning grade where, you know, they're trying to essentially minimize the amount of repeated work that they do, kind of something a little bit weird or a little bit more non standard where what they're doing is, you know, they do warm up, they do stable and then they do two sets of decay steps, decaying down to zero. So it's like two, you know, decay phases consisting of kind of like 10% plus 10%. And they sort of analyze different choices of that decay phase and they kind of doesn't seem to matter very much. But generally speaking, you know, it's about 20% of the total compute budget is gonna be spent on that cooldown phase. And so they also show once again that it matches cosine learning rates.
说话人 1 35:47
But once again, the advantage here is that we can do shintilla style analysis for very cheap in contrast to a learning rate. Sorry, learning rate fits, you know, Chinchill's style analysis just fits really cleanly. I think this is a broad lesson when you look at lots of people scaling laws. I think the stuff on hyper parameters always looks a little noisy and tenuous, but the ISO flops analysis from all of the players, like always, like very nice.
说话人 1 36:18
And so this is, you know, replication of the Chinchilla result. You see, you know, different compute scales, we see different quadratics, we draw a line through the bottom of the quadratics. We get, you know, exactly the kinds of, of sort of optimum of, sorry, optimum floss for token, an optimum token size as a function of training files, right? So this gives us a very straightforward way of analyzing the token size to model size tradeoffs. And this allows them sort of do everything from scratch, right?
说话人 1 36:47
Of course, I think you know the side commentary, I think it's really nice that they're kind of redoing a lot of this. Like they could have certainly cargo culted Chinchilla and just pick 20 tokens per parameter, but they said no, like let's actually. Actually go and do the scaling law analysis. And like, let's actually make sure that, you know, the token sizes are relatively appropriate for us. Okay.
说话人 1 37:10
And then they have, you know, a fitted scaling law at the very end. This is in some ways not surprising because this is after they fix their scaling strategy, they do predictable scaling. They try to predict what happens on the 7B and the 67B models. You know, it's unsurprising in many ways, but very nice that they're able to extrapolate out from about 10 to the 20 to the 10th of the 24 and actually nail the prediction on the basis of the scaling law, right? So it's a very nice thing to be able to see that we can actually get predictive measures of model capabilities before we actually train them.
说话人 1 37:46
So that's kind of the deep seek part. Anyone have questions for kind of the deep seek strategy and kind of what they did and any of the other pieces? I think most of this, I think WSD was probably the newest thing that I've sort of mentioned today. The other thing that Deep Seek does is directly fitting us, scaling along to the optimum warning rate and batch sizes rather than using something like New P? Yes. Do they have a global learning experience? Yeah. So they're tuning that global learning. cool,okay,cool,yeah. 
说话人 1 38:27
Discount. Yeah, so the question was like, do people redo this kind of analysis for new frontier models? To be honest, I'm not actually sure. And I'm beginning to think that a lot of people maybe don't like exactly replicate some of this because we see that in the newer paper is just increasingly less scaling details, like even from deep seek, for example, like deep seek v 2 and then v 3, we see a lot of emphasis on the new parts of each paper. Like so for Deep Seek V2, we see a lot of emphasis on like MLA and like the architectural improvements and the deep CV three. We see a lot of the systems components being emphasized, like the low bit training, but we don't see, for example, in either of those any additional new scaling loss studies. And so I think my guess is that there's not much new there. Like maybe they're replicating it just to make sure it works, but nothing new to report.
说话人 1 39:18
And I think that will kind of be captured in the next couple slides where I'm gonna talk about scaling laws and papers and models from the last year or so. I did a little brief survey, but actually there's nothing that is at the level of detail of either mini CPM or deep C. Like those are really still, I think, the most detailed open studies into scaling that we have in 2025. Cool.
说话人 1 39:44
Okay. So, you know, Llama 3 was probably one of the bigger model releases in the past year since I last taught this class. And they do have some pretty interesting scaling bits.
说话人 1 39:55
For one, you know, just the question right now of like, do people actually replicate these analyses once they've run them once? Well, kind of, yes. Lama 3, you know, redoes the isoflop style scaling, Chinchilla scaling laws. And they find, you know, roughly that the optimum ratio, if I got the calculation right, is about 39 to 1, right? And I do think this is interesting because, you know, Chinchilla got the 20 to 1 parameter ratio. I think many of us have trained models at the Chinchilla ratio in our research and so on. You know, it's quite clear that the 20 isn't really that stable. Like other people that have fit in, fitting it have been getting generally slightly higher ratios than before. And that might point to things like improved algorithmic efficiency in sort of architectures that learn better from data. It might mean something else like improved data quality. All of those are kind of moving parts. So it's hard to know like what's leading to these slightly different ratios, but the results seem fairly clear that the fits are relatively good and they do get a 40 to 1 ratio.
说话人 1 40:59
The other thing which is close to the data scaling stuff that I mentioned, the early parts of my first scaling lecture. One of the interesting things that the llama 3 folks do is they try to essentially correlate compute into NLS like log loss, and then correlate those nllls back into downstream accuracies, right? And so the thinking that they're trying to do here is they would like to not really scale against log likelihood. That's not really a thing they truly care about, right? They care about improving, I don't know, benchmark numbers on mmlu or Lombada or whatever other benchmarks that they've decided to hill climb on, right? And so if that's the case, then what they're gonna need is to have a conversion factor going from these nllls per character, these, you know, perplexities or equivalent to perplexities, and then map them into accuracies. And so they've done some studies in Lama three, essentially trying to relate these two fitting sigmoids, showing that, you know, if you fit essentially these small models and you fit some Lama 2 models and you fit a sigmoid on the whole thing, you can accurately predict the performance of Lama 3,4, O, 5, b on the basis of those fits. It's interesting, I think they say that they use these kinds of ideas for data selection. But I think it's a, there's not that much details there and it's unclear whether this is like a really core object when Lama three was being trained or whether this was kind of a side scaling thing that was like just of interest to the authors.
说话人 1 42:34
Another recent work that has come out sort of, yeah, another Chinese LM that's nicely executed is Hunyan 1. Hopefully I didn't really butcher the pronunciation there. They are training Moes. And so because they're training Moes, they want to kind of redo the Chinchilla style analysis. They fit once again, they do isoflops analysis, they fit quadratic, they figure out the minimums, and then they're able to get a different token to parameter ratio. So they get a 96 to 1 data to active parameter ratio. These ratios are obviously gonna be quite different because the training Moes, there's lots of differences about the architectures. We don't really expect the same thing as gintilla, right? And so we do actually see in various papers essentially replications of chin shell happen again and again because a lot of these people are very interested in understanding like how far can I push the token to parameter size ratio? We would like to stay on the higher end of that, right? Like have more day. Data, then parameters, because then people actually use our models or our models will be cheap to serve, right? So for all those reasons, people have been replicating Chinchilla. I think this is one of the best replicated results in scaling in many ways.
说话人 1 43:41
The actual 20 to 1 parameter ratio isn't the thing that, you know, consistently replicates. But the fact that you can do ISO flops and fit the minimum and get these like very predictable tradeoffs in flops to minimal optimal act optimum parameters is quite clean and consistent in the replications.
说话人 1 44:05
Okay, the last one, which is honestly a little bit more of an exotic scaling law over the last year is minimax 1, which came out pretty recently. So minimax 1 is a kind of linear time or long context language model release by another sort of Chinese startup. And their interest is, well, what we're gonna do is we're gonna take Softmax attention, which is quadratic. And you know, they have this thing called lightening attention, which is a kind of linear attention or linear, yeah, linear attention layer, which is linear time. And then, you know, they have a hybrid version of this model and they want to figure out like, alright, how much cost am I paying in terms of the performance of the model going from Softmax to linear to hybrid attention. And so they do things like they basically replicate method 1 for Chinchilla, where they're looking at the lower envelope of the loss curves as they train. They look at essentially the implied optimal model size and the implied optimal token count as they go. And roughly the conclusion that they draw from this is that, you know, the Lightning and the hybrid models, you know, roughly perform the same as the Softmax attention. And thus they can, they're, you know, okay to train long context models on the basis of these architectures.
说话人 1 45:23
We've seen these kinds of plots occur very often in research papers, like if you look at the Mamba paper or the Mamba 2 paper or any, or the Deltonite paper or any of these other kinds of linear time complexity R&M tapers, you'll see paths that look a lot like this, where they say, oh, the full attention scaling and my linear attention scaling are basically the same as a function of compute. But this is, you know, I would say like a kind of a rare case of this same plot being produced almost at scale from a major sort of artifact release.
说话人 1 45:59
Okay, so putting all that together, right, I know that was like a bunch of mini case studies that I went through fairly quickly. But I want to sort of step back and recap it a little bit, right? We've seen several common ingredients being used in these scaling recipes. We've seen cerebris, deep sink, mini CPM, and then the few new paper sets. So cerebrous GPT and mini CPM both use MUP as a way to make hyper parameters more stable across scale. And, you know, they essentially, mini CPM especially has a nice WSD schedule, which is a thing they popularize to be able to do Chinchilla style scaling.
说话人 1 46:39
A series risk doesn't bother to replicate, ensure deep seek does a little bit different thing. They assume that most hyper parameters just don't change the scale, but they do a full scaling analysis on batch size and learning rate. And then they use the scaling laws as a way to figure out optimum scaling.
说话人 1 46:54
You know, I've already noted that some of the scaling looks a little bit more suspicious than others, but really this is a way to at least get the order of magnitude hopefully right. They use isoflops analysis, you know, they replicate chenchilla once again to figure out the model sizing and to make sure they're kind of in the right order of menu of the more recent releases, you know, Lama 3 and honeyandu, isoflops analysis only Lama three does a little bit more, but that's basically it. And then minimax does the more interesting thing of basically justifying architecture choices through the lens of a scaling law. But we see, generally speaking, that there's a few different things that get replicated like Chinchilla and learning rate and batch size are really the things that people are really deeply concerned about when they're scaling models up. And they sort of do things like fixed aspect ratio and just scale the total model size up, and that's generally the way that people handle a lot of the moving pieces of scaling up. Okay. Any questions about the case studies pieces? Actually, I'm gonna stay here and just make sure I've covered any questions that people might have.
说话人 1 48:02
Okay, cool. Okay, so the second and kind of last part of this lecture is going to be understanding new P. Hopefully through the case studies you've seen that essentially getting the learning rate right is one of the core concerns that people have. And also the backside.
说话人 1 48:21
But in general, I think we want to have scale and variant hyper parameters. And it is the case that, you know, our choice of initialization and our choice of per layer learning rates are essentially arbitrary, right? Like there's no reason why we have to initialize them one way and not the other. And so if we could manipulate those sort of free variables to get scale and variance in our learning rates, that would just be really wonderful, right? Like that would make our lives way easier and it would make, you know, small scale experiments much more possible.
说话人 1 48:49
So, you know, I'll also talk, you know, first through the math of this, like how it's derived, what's the justification, what are the core conceptual objects behind trying to make models scale predictably? And then I want to talk about a pretty nice pre print by an independent researcher on basically just a bunch of ablations on up like what makes a break, what is it robust to? Does it work on a real, you know, transformer language model? These kinds of questions are explored pretty well in this pre print that I'll talk about at the very end here.
说话人 1 49:21
So, okay, what is new P anyway? I feel like maybe I've jumped the gun for the last two lectures. I've mentioned what this is without really giving you the core conceptual object that is based off of. On the other hand, I think I'm justified in doing this because I think most of the literature doesn't explain up that clearly either. They're just like, yeah, just scale the initialization by 1 over the width and scale the per layer learning rate by 1 over the width. That's new P, but I think the ideas behind up are pretty interesting and worth discussing because I think they speak to some core objects that occur in deep learning in general.
说话人 1 49:57
So I'm going to be basing my slides off this pre print or paper if you're interested in kind of reading about Meep, I would point you to this one. I think this and another blog post called a Practitioner Guides to mute, I think are the two kind of readable descriptions of what the sort of this paradigm is. Okay, so I'm going to base myself off this. The math is, for whatever reason, not exactly the same across these different presentations. I'll, you know, clarify that. I'm basing the math off this one.
说话人 1 50:28
So mute is based off of this, the following relatively simple ideas, right? So there's two things that we think should happen when we're training a neural network, right? So, you know, when we scale a neural network, we're gonna make the, in this case, let's just say only the width of the network bigger, right? And we're gonna fix the layer size or the, sorry, the depth, and I'm gonna make the width bigger as we go. Now, if I do that, as I make the width bigger, I want the activations initialization to remain, you know, big theta of 1, right? I wanted to remain roughly constant, you know, bounded above and below by universal constant, you know, roughly constant. As I make the width bigger, it shouldn't blow up, it shouldn't vanish, right? Seems like a pretty natural thing to want, right? You don't want your activations to get too big. This is per coordinate.
说话人 1 51:15
Now, the second assertion I want is that, you know, I'm gonna initialize my model and I'm gonna take a single gradient step. And when I take that single gradient step, I want to make sure that the change in activation should also be big theta of 1, right? So both of these seems like very natural conditions, right? Because if you violate these, it's gonna mean that, you know, as I make the models bigger, either the initial activations will blow up or vanish, or after one gradient step, my activations will either blow up or vanish, right? Those are both bad conditions, right?
说话人 1 51:45
And as a note, right, I'm talking about individual activations like coordinates. And so if you're thinking about norms, right, of an entire vector of activations, right, that should look like, you know, big theta of square root of NL, right? Because each one of these are gonna be roughly independent. So the norm is gonna look like the square root of the width, the number of elements in my with coordinate. So I can derive, you know, mu p from those two conditions.
说话人 1 52:15
So the first condition, which is that I want my activation to remain stable, imposes sort of constraints on the initialization, right? So I'm gonna walk you through a very simple example, right? So I'm going to consider a deep linear network. So this is H of L. So this is the activations out layer, little L. And that's going to be a function of the weight matrix at layer L and the activations from the previous layer. No non linearities, no fancy stuff, right? Like just, you know, it's all square. Just forget all this complexities if you want complexities, you can go read the pre print. They'll explain in slightly hand way returns why those things don't matter.
说话人 1 52:53
Now, the initialization, I'm gonna pick a Gaussian initialization, right? So it's gonna be zero centered. It's gonna be. A rectangular size of the sizes that depend on the sizes of my activations. And then I'm going to have one hyper parameter, which is the noise scale of this matrix at this layer. Sorry, there should be a little L on the Sigma.
说话人 1 53:18
So now what can we say? Well, I want to understand the size of H of L, you know, at initialization. So how can we do that? Well, one thing we can do is we can consider sort of the limiting behavior of this system, right? I'm going to take the basically little novel and little l minus 1 to infinity. And if I do that, this W is going to concentrate. It's a random Gaushin matrix. If you remember your random matrix theory. Actually, that's not a prerequisite for the course. But you know, if you know some basic random matrix theory, you know that the operator norm of a Gaussian matrix is gonna roughly concentrate to this object, right? It's gonna be Sigma, which is the noise scale times, you know, the square root of both of the coordinates added.
说话人 1 54:00
And importantly, you know, you can write down roughly that this equivalence is true, right? So the activations that layer L, the norm of that is going to be approximately equal to the operator norm of WL times the activation norm of H of l minus 1, right? And this is roughly assuming that W of L is independent of H of l minus 1, which is true at initialization. So I think you can basically make that a right arrow if you'd like.
说话人 1 54:25
Now I'm going to say I'm going to pick a particular choice of Sigma, which is going to be square root NL over square root NL minus 1 times this object. You can simply think of it as this right hand side thing. This is the exact form. This is kind of the more asymptotic form that you can think of this as. But really it's just one over the square root of the fan in of your layer times the minimum of one and sort of the aspect ratio of your model in case your fan in is much larger than your fan now than the sort of kicks in.
说话人 1 54:58
Okay, so let's say that I pick this Sigma. What happens right? Roughly one over the square root of my fan in. So now what happens? I can plug this back in to this formula, the matrix concentration limit, and also this approximation here. And I can sort of inductively prove that every layer is gonna have the right sort of activation size.
说话人 1 55:17
So let's just go through all the layers and assume that up until layer l minus 1, I have this property, right? So that's the inductive Assumption. At layer l minus 1, I have that my activation norm is square root of n l minus 1. Okay, so that's just an Assumption.
说话人 1 55:31
Now, if this is true, then I just plug all these in, right? So I plug in square root of NL minus 1 into this component, into WL operator norm. I plug in the limit. And then for Sigma, I plug in this expression over here. You see that this inverse cancels this. And then you're going to get exactly that HFL, the L2 norm of HFL is equal to square root of n of L.
说话人 1 55:55
So this is the thing that we wanted because before, remember we said we want to make sure that the activations remains a big state of 1, which means that the norm should be square root of NFL. So that's exactly what we get, plus some lower order terms, right?
说话人 1 56:11
So, you know, this is a fairly clear step by step argument that shows you what the right thing to do is for initializations. I want to pick one over the square root of the fan in plus a small correction factor in order to make sure that my activations do not blow up at initialization. I'll pause here for a moment in case someone has questions. I feel like this is actually maybe the first like real math that we've done in the class. So maybe it's a bit of a context switch for people. I did not warn you that I was gonna talk about a bit of math. Okay. Is this all relatively clear for people? One over square root of fan? And yes, I'm gonna assume that everyone's on board with one over a square root of fan.
说话人 1 56:54
Okay, so now we're gonna derive the second part of newp, right? So the first part of newp was about initializations. The second part of newp is gonna be about learning rates, right? And so how are we gonna think about learning rates? Well, to think about learning rates, I'm gonna look at the second condition, A2, which says when I take one gradient step past initialization, what needs to happen is that my activation, sorry, my update size needs to remain constant. It can't blow up, it can't vanish. Okay, so what does that mean? So if I have an update of delta WL on the weights at layer L, what does that come from? Well, that comes from, let's say, what I'm doing, SGD, that's gonna come from this expression. It's gonna be a learning rate times L, which is my loss, the gradient of L, which is my loss, and then the activations transpose. In the case that my batch size is 1, this is a rank 1 object, right? This is a rank 1 update to delta of L, right? And because it's rank 1, you know, there's a nice easy expression. The change of WL times the act. Activation of the previous layer is equal to the norm of the change in WL and the operator norm of this thing times the L2 norm of H of l minus 1. And now, you know, combine this with the fact that the change in activation at layer L is kind of this expression, you can convince yourself that this is true. You can write this out by sort of figuring out what the actual final activation is at layer out after the update. And that in canceling out wlhol, which is a shared term across left and right, then you'll get this expression. You'll get that, you know, what is the update in HFL. This is the object that we want to keep roughly square root of novel, right? The norm of this object. So let's look through each of these terms and look at what the magnitude of this is. The first term here, WL delta h of l minus 1. This, you know, we can assume is going to be controlled from the inductive Assumption because this is exactly the sort of the delta h of L that we have, plus the condition A1 argument, right? Condition A1 basically says, sorry, condition A1 is gonna say that delta of H of L minus 1 is gonna be square ML. And then WL is gonna maintain that more. The more complicated parts is going to be these two arguments, the second and third terms that we have here, delta WL H of l minus 1 and delta WL delta hl minus 1. Sorry, that's quite the mouthful. They all have the same order of magnitude, actually. And the only thing that we need to really figure out is this expression here. What is the product of the previous layers, norm times the operator norm of delta w l, right? Because we don't really know how big the update is gonna be in the weight Matrix W if we knew that. All very straightforward stuff. Okay, and so the remaining argument is actually relatively straightforward, even though this is actually like a, you know, complicated jumble of things. The intuition is actually very clear. The intuition for this is says, okay, what do I really need to figure out? The one thing I really need to figure out is this expression here. How much does the weight at layer L change, right? If I can figure that out, then I can sort of derive all of the relevant quantities and solve for the learning rate, but at a high level, that's our strategy here. And so how can we possibly figure out after one gradient step how much delta W L moves, right? That's really the key question.
说话人 1 01:00:23
Well, there's an additional sort of sneaky Assumption that then shows up here. And the Assumption is something like this. If our learning is well behaved, then after a single gradient step, then the change in the loss delta of L, this quantity has to also be big theta of 1, right? And why is that?
说话人 1 01:00:40
Well, because we don't want are the size of our losses, the update, the decrease in our losses to kind of blow up or go to zero as the width goes to infinity, right? We want essentially our improvement in losses to remain roughly the same order of magnitude no matter how big our models get. That's a stronger Assumption than what we've had before. But assuming that's true, then essentially we can say, okay, the change in the loss is kind of like multiplying the gradient with the change in the weights. This left side is 0 of 1. We know how big this delta of L should look like. So now, sorry, we know how big this delta w L looks like.
说话人 1 01:01:16
Now we can solve for the gradient size. And once we have that, we can plug that in here. We know delta WL, we know the gradient of L, we know the size of H of L from condition A1. And now we can solve for the learning rate. And that's exactly what you get at the bottom here. And the, you know, if you work through the arithmetic, the final result that you get here is that the learning rate for SGD is equal to the fan out over the fan in, right? So lots of steps involved and lots of like substitution and slightly sketchy bigger notation being tune into the equations here. But once we do that, we're going to end up with a very simple formula. Note that this is true for SGD. And those of you that have kind of been paying attention and like kind of staring at this equation are probably, you know, internally complaining, you're like, you have misled us because, you know, in a transformer, well, what's NL over NL minus 1 for like an MLP that actually is like a 4 right? Because you've got a factor of 4 between DFF and d model, right? And so this thing doesn't really change. It's just a constant in most models, right? Unless your aspect ratios are like dramatically changing through your network.
说话人 1 01:02:24
The reason why new p is different from standard parameterization is because this derivation is for SGD, where the prioritizations look very similar between up and SP. If you do the exact same derivation for Adam, you're gonna find that actually you're gonna get slightly different things, which is that it's gonna be one over the fan in rather than the fan out over the fan in. Okay, so here's the recap. I have sort of dragged you through, hopefully willingly, the derivation of the basic, what people call the spectral conditions that define up. But now I will give you the kind of one slide, high level takeaway of that result, right? So, you know, when we want to do something like new P, if we're following the guidelines from before directly, what we will end up with is the following blue box initialization. You know, you set yourself to one over the square root of fan in times a correction factor. That's, you know, one, if your fan in is smaller than your fan out, but you know, square root of the ratio otherwise. And then this is a simple initialization for the scale of your Gaussian for a learning rate. If you're doing SGD, then you set it to fan out or a fan in. But if you're doing atom, that's gonna be slightly different. It's gonna be one over the fan in.
说话人 1 01:03:41
Now, in case you already sort of know the standard like Kaiming initialization and so on, off top of your head, you know, you can mentally compare what this looks like to the standard paradization. So in a standard paradization, if you're doing it right, you should probably be already setting your gautions to initialize to 1 over the square root of fan in. So that's good. That's already perfectly set. But your learning rates are probably being set globally. So constant. This is fine for SGD, not so fine for Adam, where the really big difference between SP and new peak comes in. So, okay, that brings us right back to kind of the serious GPT paper.
说话人 1 01:04:20
Now we have all the context we need to understand all the operations they do. If you look once again at the column over here of new P, the embedding layer is special. It doesn't really do essentially like any scaling because embeddings are 1 hot. So their norms don't scale linearly with the number of vocab elements. But ignoring that, basically you see that all of the layers get scaled down by 1 over the width. You know, that's the initialization rule. And then the learning rate rules are scaled by 1 over the width as well, right? So this is once again the learning rate rule for Adam, right? So if you're using atom, that's exactly the right thing to do. And that's also exactly what they do in series risk GPT. So hopefully that's clear and hopefully this gives you a sense of, you know, both the interestingness of manipulating per layer learning rates to get more predictable scaling, and also maybe an appreciation of this idea of trying to control activations and updates as a function of model with, right. Like, you know, I'll pause for a moment there and just mention, right, that's like a very successful idea from physics, right? Lots of physicists think about ideas like renormalization as I take limits of certain things, I want things to remain stable. I want them to not blow up or go to zero. This is an exact application of that idea. That's kind of an interesting use of that. Okay. Any questions about, I don't know, BP derivation or service GPT or any of the other things. Yes.
说话人 2 01:05:46
There's no Assumption about any architecture, right? So can you transformer or a needed model?
说话人 1 01:05:51
Yeah, so that is part of the subtlety. The question was, you know, what's the architecture assumptions? Well, I mean, technically, there's an even stronger Assumption here. Oh, there's an even stronger Assumption here, which is that I'm assuming things are deep linear network, right? I'm just multiplying matrices repeatedly. You know, this is the kind of philliest network that you can have. Basically, there are arguments for why adding non linearities are fine. There are arguments for how you would take the same arguments and apply them to the attention layer, their arguments for why much more complex things are needed for a gated linear unit. So each one of those architecture pieces needs a careful analysis in order to have a, you know, corresponding object.
说话人 2 01:06:33
Yes, visa, like, and something else for this department, like it looks like they're an index that layer. Right.
说话人 1 01:06:40
Right, right, right, right. So n sub L is just the output of a matrix multiply and n l minus 1 is the. So for example, if you have a MLP, you're gonna have a matrix multiply that takes you from d model dimension to 4 times d model, like the DFF dimension, right? So that would give you NL over NL minus 1 of 4, for example. So all the different matrix shapes are giving you the NL and NL minus this one, the fan in and the fan out of a matrix. Yeah, exactly. So the input and output dimensions are determining all of these objects. Okay, excellent user thing. It's also interested in and out of the, yeah, and in the panel or like they put it up with their. Yeah, I was using those terms exchangeably, but it should have been a little bit more clear.
说话人 2 01:07:36
Oh, yes, since Lipstick One has a global revenue that they don't have over 1 updates.
说话人 1 01:07:43
So the question was like, since Deep Sea uses a global learning rate, does that mean they don't have an order 1 update? So, you know, all of this argument is asymptotic, right? It's basically saying, as I scale my width out to infinity, things will kind of be big or small. And I mean, if you look at. The mupe plot, for example, you do kind of see this, right? You see that the learning rates have to shift as the model gets larger in order to compensate for the fact that the updates are getting bigger and bigger, right? What's empirically, you know, been seen is if you do nail the learning rate, you don't need new P, right? Like it's not like new P is necessary for you to train a good model. It's really just an attempt to try to keep this shift as small as possible so you can use the same learning rate, you know, throughout scaling. And if you go back to deep seek, you know, if you remember the scaling law that I was, you know, being a slight hater for, you'll see that, you know, they too have learning rates that go down as a function of scale in order to try to compensate for the fact that the bigger models are gonna have bigger updates, right? And so, you know, to respond more directly to the question, yes, in the case of Deep Seek, right, as we scale the model up, you know, our activation updates will get bigger. So we have to shrink the global learning rate or we should shrink the global learning rate to compensate. Cool. Okay, nice questions. Okay, so that was kind of the conceptual, somewhat mathematical components of new P.
说话人 1 01:09:15
Now I want to talk about the empirical aspects of newp. And so I'm going to talk through a pre print or I think this one named Cappella. Com, a large scale exploration of new transfer. And I like this because it's got a bunch of oblations and I think I'm a  for ablation. So I'll present any paper that has large scale relations in the course. And so they do essentially with new P, as we've described it, just look at the right hand side, which is a more relevant piece. You know, they're scaling down the variances, they're scaling down the learning rates by the width, the global width of the model's m. And they're primarily keeping the depth fixed, which is a little bit of an unusual scaling regime because usually you scale depth and width together. But they really want to do a controlled experiment where they're only looking at with variations, and they want to see if Mupey precisely nails scaling in this regime. There's also a little bit of a kind of weird subtlety that all of the MUP papers seem to do, which is that if you remember your 224N lecture, you know, you remember that there's like a scaling on the attention activations. You scaled, you know, you do your inner product and you scale it down by one over the square root of d, you know, and I told you kind of this was a magic constant that was like the right thing to do. You know, MUPI and other papers use 1 over d scaling instead of a 1 over square root d for various arguments related to activation and update size stability. So that's another thing that I think was worth pointing out because you might sort of not initially think of that as being something that's related to mute.
说话人 1 01:10:51
Okay, architecture is mostly similar to the standard, you know, transformer stuff. And as I already mentioned before, they only consider with scaling, right? So they take a standard transformer train auto regressively on, you know, pre training text. And they want to basically make the model wider and wider on the MLPs and sort of the model residual stream dimensions, they're going to make that bigger and bigger. And what they want is for the optimum learning rate to remain the same as they scale the width up. And if it remains stable, then that's a big victory from up, right? So the game is hopefully cleared everybody. You just want to scale with I want my learning rate that's optimal to stay the same. So question number one is, you know, does it work? Well, the answer is yes. So we have different width, 128,5,12,20,48. We have different learning rates across the columns. And, you know, the sort of idealized strategy here is we run a sweep of learning rates at the small scale. I pick the smallest scale and I scale that up and hopefully that base learning rate remains optimal. And yeah, it seems like, you know, learning rates transfer very reliably across model sizes if we're doing this like, you know, somewhat precise with scaling.
说话人 1 01:12:08
And so then I think you start asking questions of, all right, very similar to the previous question that was just asked of like, okay, when does Meep break, right? So you can ask that question in theory, but you can also ask that question in practice, right? So I'm just gonna try all sorts of modern variations to architectures that people do. And then I'm gonna ask, you know, does this hyper parameter transfer thing continue to hold under these variations or not? And, you know, the paper is quite nice because they just go through a lot of different stuff. They'll vary the activations, they'll vary the batch sizes, the initializations, the RMS, norm gains. They'll even use like a really exotic optimizer as like sort of sign gradient style stuff. And then they'll also vary the regularizers. So which one of these prevents learning rate transfer? So the first one, you know, which I think is probably relevant if you were kind of looking at that deep linear network and saying, oh, no one, just multiply his matrix. Together, there's like non linearities in between, right? So does VP work when we change non linearities around, well, Swiglu squared value and the baseline sort of new p approach of value all have the same minimal learning rate. So no changes at all. You know, we just see that, for example, swiglu and squared value, you just do better than baseline VP.
说话人 1 01:13:23
Unsurprising, sort of agrees with a lot of what we've Learned in the course, right? We might vary the batch sizes because we know that batch sizes are kind of going to be sensitive to scale. Like we've seen many CPM and we've seen deep seek basically fit scaling, lost the batch sizes to try to get what the optimum batch size was. You know, once again, we see that, you know, as we scale up backsizes by 4, up or down, you know, learning, optimum learning rates remain stable.
说话人 1 01:13:53
What about initializations, right? You know, there are some initializations that, you know, people vary. Like for example, some people set the query matrix to zero so that all the different items get uniform attention. Maybe that's more stable. Some people sort of the unembedding layer at the very top, they'll scale differently based on either you standard parameterization or MUP. Maybe that matters a lot. Turns out neither of those do. You know the center column, the optimum learning rate remains optimal in all of these cases. You know, what is it not robust to you? Well, you know, it's not gonna work for every single case. For example, if you add sort of learnable gains, that turns out to break new P, right? So you need to remove the biases. But you know, if you remove them, then up works. If you add them back in, don't necessarily work.
说话人 1 01:14:45
Similarly, you can try sort of more exotic optimizers. Lyon is an optimizer that takes like the sign of the gradient updates, which to me feel a little bit crazy, but I think this was searched. This was found through like evolutionary search or something like this to find like the fastest optimizer. If you use this kind of a more crazy optimizer, it really breaks down. And I think this is what you expect, right? New P is designed to adapt to a very particular optimizer like FMW to control the update sizes. So, you know, if you're using a totally different optimizer, I don't know why you'd expect the learning rates to transfer. So maybe expected that this thing fails. And then sort of finally, what is it, you know, also not robust, you turns out if you really have much stronger weight decay, newp actually starts to fail. And so this is one of the few significant newp failures that are in there. A lot of the other ones are kind of just like, oh, we maybe expect to that or that's not standard to do, you know, weight decay is something that you actually do.
说话人 1 01:15:45
Okay, so mute seems generally useful. Like if you take standard parameterization, like kind of going back to the baseline, right, you might ask like, all right, what if I just do, you know, standard baseline stuff?
说话人 1 01:15:57
You know, you can't use the same learning rate results in, you know, significantly worse losses at 2048, right? Like your model just blows up, gives you basically, you know, degenerate losses. You would have been very sad scaling up at the same learning rate, you know, and we see also that the learning rate needs to scale down predictably as a function of the web. On the other hand, you know, even if you scale up all the way to a 10 b parameter model, you know, you see that the base loss remains the same. So they do one large scale experiment and they see that the learning rate remain sort of ideal at the 2 to the negative 6 level, which is a kind of cool validation, right? So they do the whole study at a medium to small scale. They do one big sort of hero run and then the learning rate remains optimal. So the empirical results on that look somewhat promising. The fact that meta used it for law before is also quite nice. But as far as I know, it's not a consensus that people use mute.
说话人 1 01:16:49
Okay, so putting it all together, you know, how do you scale in the wild? I have never trained a, you know, 70B model at, you know, superpotential of sizes. And so we're gonna have to rely a lot on case studies. And we saw several examples of scaling in the wild. We saw people setting things like model hyper parameters, especially learning rate and batch sizes using scaling laws. We saw, you know, people using things like new P or assume stability to try to avoid search over these spaces. And then also the use of things like alternative learning schedules like WSD can decrease the amount of compute that you need in order to fit a lot of these scaling laws. So that's all I got.
