### Top-K 路由机制详解



Top-K路由是现代混合专家模型（MoE）的基石。它负责为每个输入的令牌（token）从海量的专家库中，高效地选择出最相关的一小部分（K个）专家进行计算。本笔记将深入解析其工作流程和关键的数学公式。



#### 1. 核心目标与流程



Top-K路由的目标是计算出一个**稀疏的门控向量 `g_t`**，该向量的维度等于专家的总数 `N`。向量 `g_t` 中只有K个非零值，这些值对应被选中的专家，并作为权重来组合专家们的输出。



整个流程可以分为三步：

1.  **计算亲和度分数 (Affinity Score)**：为输入令牌 `u_t` 和每个专家 `i` 计算一个标量分数 `s_i,t`，表示它们之间的匹配程度。

2.  **Top-K选择与门控 (Top-K Selection & Gating)**：从所有 `N` 个分数中选出最高的K个，并根据这些分数生成门控向量 `g_t`。

3.  **加权求和输出 (Weighted Sum Output)**：使用门控向量 `g_t` 对所有专家的输出进行加权求和。



#### 2. 数学公式拆解



下面是描述Top-K路由的典型公式，以DeepSeek V1/V2的实现为例：



1.  **最终输出 `h_t`**:

    `h_t = u_t + Σ (g_i,t * FFN_i(u_t))`

    - `h_t`: 该MoE层的最终输出。

    - `u_t`: 输入令牌的向量表示（来自上一层的残差流）。

    - `FFN_i`: 第 `i` 个专家的前馈网络。

    - `g_i,t`: 门控向量中对应第 `i` 个专家的权重值。

    - `u_t + ...`: 残差连接，是Transformer的标准组件。



2.  **亲和度分数 `s_i,t`**:

    `s_i,t = Softmax_i(u_t^T * e_i)`

    - `e_i`: 一个可学习的向量，可以理解为第 `i` 个专家的“身份嵌入”或“能力描述符”。每个专家都有一个自己专属的 `e_i`。

    - `u_t^T * e_i`: 计算输入令牌 `u_t` 和专家嵌入 `e_i` 的点积。这与注意力机制中Query和Key的点积非常相似，值越大代表亲和度越高。

    - `Softmax_i(...)`: 对所有 `N` 个专家的点积得分进行Softmax归一化，得到一组和为1的概率分布。这个概率 `s_i,t` 代表了路由器认为令牌 `t` 应该被分配给专家 `i` 的“信心”或“意图”。



3.  **门控向量 `g_i,t`**:

    `g_i,t = { s_i,t  if s_i,t ∈ TopK({s_j,t}, K),  else 0 }`

    - 这一步是实现**稀疏性**的关键。

    - `TopK({s_j,t}, K)`: 从所有 `N` 个Softmax概率 `s_j,t` 中，选出值最大的K个。

    - `if ... else ...`: 如果某个专家的概率 `s_i,t` 入选了Top-K，则其门控值 `g_i,t` 就等于该概率值 `s_i,t`；否则，其门控值 `g_i,t` 被强制设为0。

    - 经过这一步，向量 `g_t` 中就只有K个非零元素了。



#### 3. 张量流动与维度分析



让我们追踪一下数据在路由过程中的形状变化：

- **输入 (`u_t`)**: 假设批大小为B，序列长度为L，隐藏层维度为D。输入张量的形状为 `[B, L, D]`。

- **专家嵌入 (`e_i`)**: 路由器的权重矩阵，可以看作是 `N` 个专家的嵌入堆叠而成。形状为 `[N, D]`。

- **计算点积**: 将 `[B, L, D]` 和 `[N, D]` 的转置 `[D, N]` 相乘，得到形状为 `[B, L, N]` 的亲和度分数矩阵。矩阵中的每个元素 `(b, l, n)` 代表该批次中第 `b` 个样本的第 `l` 个令牌与第 `n` 个专家的原始匹配分数。

- **Softmax**: 沿着最后一个维度（`N`）进行Softmax，形状不变，仍为 `[B, L, N]`。现在每个 `(b, l, :)` 都是一个和为1的概率分布。

- **Top-K与门控**: 对最后一个维度的 `N` 个概率值进行Top-K筛选。这一步虽然概念上是筛选，但在实现上通常是通过掩码（masking）完成的。最终得到的门控张量 `g_t` 形状仍为 `[B, L, N]`，但每一行 `(b, l, :)` 中最多只有K个非零值。



#### 4. 实现上的微妙差异：Softmax的位置



值得注意的是，不同模型在Softmax的应用上存在细微差别，这会影响门控值的性质。



- **Softmax-Before-TopK (如 DeepSeek V1/V2, Grok)**:

  - 这是我们上面详细解释的流程。

  - **优点**: 门控值 `g_i,t` 是归一化的概率，其总和（在Top-K个专家中）小于等于1。这使得加权求和的输出在数值上更稳定。

  - **缺点**: 经过Top-K筛选后，剩余的门控值之和不再为1。例如，如果Top-2的概率是0.6和0.3，它们的和是0.9。



- **Softmax-After-TopK (如 Mixtral, DeepSeek V3)**:

  - 流程变为：1) 计算原始点积分数 -> 2) 选出分数最高的Top-K个专家 -> 3) **只对这K个专家的分数进行Softmax**。

  - **优点**: 最终的门控值之和严格为1，形成一个真正的加权平均。

  - **缺点**: 这样做在概念上可能略显奇怪，因为选择（Top-K）发生在归一化之前。



在实践中，这两种方法的性能差异并不显著。因为后续的网络层（如LayerNorm）可以轻松地重新调整输出的尺度，所以门控值之和是否严格为1并非至关重要。这更多地反映了不同研究团队在架构设计上的美学偏好。