大家好. 希望大家在完成作业1的过程中玩得开心. 它今晚截止, 如果需要延期请告诉我们. 作业2也即将发布, 我们正在对其中的一些Triton部分进行最后的润色. 希望你们会喜欢, 你们将有机会实现Flash Attention 2或者它的部分内容, 我想这会很有趣.

今天我们来谈谈**GPU** (图形处理单元). GPU是驱动我们语言模型运行的核心部件, 所以正确地理解它至关重要. 如果你没有真正研究过驱动模型运行的硬件, 它们可能看起来相当神秘. 所以我今天的目标是尝试让CUDA和GPU不再那么神秘.

### 课程大纲与目标

我想揭开神秘面纱的一件事是: GPU为什么会变慢? 它们会以非常神秘的方式变慢. 在讲座的后半部分, 我会尝试讲解下面这张图. 当你增加矩阵乘法的大小时, 你可能会预期它会变慢或变快, 或者其他什么. 但你会看到这些非常难以预测的波浪状模式. 你可能会想, 为什么我的GPU在某些数字的特定倍数下速度快, 而在其他数字下就慢呢? 这非常神秘, 我们将尝试理解这一点.

![矩阵乘法性能图](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/n3I158e2K-4Y6hH4QO5L32P4.png)
> 图1: 矩阵乘法性能表现出复杂的波浪形态, 这与硬件的底层工作方式有关.

另一件事是, 我们希望了解如何设计出快速的算法. 我想你们几乎所有人都听说过 **[FlashAttention详解](./Lecture5-Flash-Attention-Explained.md)**. 它通过在Transformer内部非常巧妙地计算注意力操作, 使得处理更长的上下文成为可能. 所以, 也许你也想提出像Flash Attention那样的新算法或新实现. 我们需要理解哪些原语和组件才能做到这一点呢?

所以, 这就是今天的两个学习目标. 首先, 在讲座结束时, 你应该对GPU感到相当自在了, 对它们的工作方式有一个大致的了解. 其次, 你应该能自如地加速算法的某些部分. 当你设计一个新的架构时, 希望你能感觉可以尝试用CUDA来加速它.

因为硬件不一定是我工作的领域, 我要特别感谢一些资源, 尤其是Horace He的博客, 上面有很多有趣的GPU知识点. 比如, 为什么填充了零的矩阵乘法比没有填充零的要快? 你可以通过他的博客学到这些. 此外, 我还借鉴了Kuda Mode小组和Google出品的TPU书籍等其他资源.

今天的讲座将分为三个部分:
1.  **深入GPU**: 它们如何工作以及重要的组成部分.
2.  **理解GPU性能**: 什么让GPU快, 什么让它慢.
3.  **融会贯通**: 剖析 **[FlashAttention详解](./Lecture5-Flash-Attention-Explained.md)**, 看看所有知识点如何结合在一起.

### 性能的舞台: 计算能力推动进步

我们知道, 拥有更多的计算能力对于训练大型语言模型非常有帮助. 这是一个预训练的缩放法则图表, 但你也可以用一个推理的缩放图来替换它. 人们普遍认为, 你拥有的计算能力越多, 你可以在数据上做的处理就越多, 你可以消化更多数据, 训练更大的模型. 所有这些都会带来性能的提升.

![语言模型的缩放法则](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/8rU-8G2t54-7K9OQ6-wQz20-.png)
> 图2: Kaplan等人提出的神经缩放法则, 显示了计算量 (PetaFLOP/s-days) 与模型性能 (验证损失) 之间的可预测关系.

所以你可能会想, 深度学习固然重要, 但真正驱动性能的是更快的硬件、更好的利用率和改进的并行化. 这就为我们理解硬件的重要性设定了舞台.

### 计算能力的演进: 从登纳德缩放到并行缩放

一旦你开始思考计算能力的缩放, 你会问, 我们如何获得计算能力的缩放?

在半导体缩放的早期, 如果你思考CPU如何变得更快, 它们遵循一种叫做**登纳德缩放 (Dennard Scaling)**的定律. 随着摩尔定律, 芯片上的晶体管数量每年翻倍. 这种翻倍导致了登纳德缩放, 即越来越小的晶体管可以用越来越低的功率在越来越快的时钟速度下驱动, 这反过来又给你带来更多的性能.

![处理器42年发展数据](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/nN_b2f9mQx6sV28Uj8R6J4Vp.png)
> 图3: Hennessy和Patterson展示的处理器数据显示, 单线程性能在2000年代后趋于平缓, 标志着登纳德缩放的结束.

然而, 从1980年代到2000年代, 这种方式逐渐达到了极限. 你可以在这张图表中看到, 单线程性能 (蓝色点) 基本上开始趋于平缓. 当然, 晶体管的数量并没有真正开始下降, 芯片的晶体管密度越来越高, 但这对提升单线程的吞吐量没有帮助. 这意味着我们不能仅仅在绝对意义上让计算变得更快, 我们必须用**并行缩放 (Parallel Scaling)**来弥补.

因此, 深度学习和神经网络的缩放故事, 就是从单线程缩放 (在绝对意义上更快地完成计算) 转向并行缩放 (同时计算大量工作负载). 这是Bill Dally主题演讲中我最喜欢的计算缩放图之一, 它展示了每秒整数操作数的超指数级增长, 从早期的K20 GPU到H100 GPU. 这是一条非常惊人的曲线.

![单芯片推理性能在10年内提升1000倍](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/yM6E81F9P96pE-G69zXU1d9J.png)
> 图4: Bill Dally在HotChips主题演讲中展示的图表, 显示了并行缩放带来的巨大性能飞跃.

所以, 我们必须真正理解如何利用这条曲线, 才能真正从我们的语言模型中获得最大收益.

### GPU与CPU的核心区别

**CPU** (中央处理单元) 为少数快速线程进行优化, 而**GPU** (图形处理单元) 为海量线程进行优化.

为了支持CPU的执行模型 (有很多分支和条件控制逻辑), 你需要大的**控制单元 (Control Unit)**和**缓存 (Cache)**. 而GPU则将更多的芯片面积用于海量的微型计算单元, 也就是**算术逻辑单元 (ALU)**, 而用于控制和缓存的部分则少得多.

![CPU与GPU的架构对比](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/m2q00g-Xj-jE5uX2Gq36sM9j.png)
> 图5: CPU将大量芯片面积用于控制和缓存, 而GPU则用于大量的ALU.

从设计目标来看, 它们为截然不同的目标而设计.
- **CPU优化延迟 (Latency)**: 尽快完成单个任务.
- **GPU优化吞吐量 (Throughput)**: 尽快完成所有任务的总和.

在GPU中, 线程可以非常轻量地进入休眠和被唤醒. 最终, 你会比CPU更早地完成所有的工作负载 (T1到T4), 尽管单个任务的延迟可能更高.

### GPU剖析 Part 1: 执行单元

GPU有一个相当不同的内部结构. 其核心概念是**流式多处理器 (Streaming Multiprocessor, SM)**. 你可以把SM看作一个原子单元. 当你在像Triton这样的环境中编程时, 操作将在SM的层面上进行.

在每个SM内部, 包含许多**流式处理器 (Streaming Processor, SP)**. 一个流式处理器将执行大量的并行线程. 一种思考方式是, SM拥有一套控制逻辑, 它可以决定执行什么, 比如进行分支. SP则负责接收相同的指令, 并将其应用于许多不同的数据片段.

![GA100 GPU的完整架构图](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/k_pW1s1t3wK53gQp21w3UqYv.png)
> 图6: 一个完整的GA100 (A100 GPU的核心) 包含128个SM. 每个SM内部又有许多执行单元, 如FP32核、张量核(Tensor Core)等.

一个A100 GPU拥有128个SM, 这远多于大多数CPU的核心数, 并且每个SM都将拥有大量的SP和专门的矩阵乘法单元.

### GPU剖析 Part 2: 内存

在GPU中, 计算固然重要, 但**内存**可以说更为关键, 并且在未来会越来越重要.

要理解内存, 你必须了解GPU和芯片的物理布局, 因为在如此高的运行速度下, 内存的**物理邻近性 (physical proximity)**变得至关重要.

![NVIDIA A100 GPU的芯片布局和内存延迟](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/o_T_qC6Vp4g94d4iP_9lY4iM.png)
> 图7: A100 GPU的物理布局图 (右侧) 和不同内存类型的访问延迟 (左侧). 越靠近SM的内存速度越快.

这引出了 **[GPU内存层级结构](./Lecture5-GPU-Memory-Hierarchy.md)** 的概念. 简而言之, 内存离SM越近, 速度就越快.
- **L1缓存和共享内存 (Shared Memory)**: 位于SM内部, 速度最快 (约20-30个时钟周期).
- **L2缓存**: 位于芯片上, 紧邻SM, 速度次之 (约200个周期).
- **全局内存 (Global Memory, 通常是DRAM/HBM)**: 位于GPU芯片外部, 通过HBM (高带宽内存) 连接器相连, 速度最慢 (约300个周期).

这个数量级的速度差异会严重影响性能. 如果你的计算需要频繁访问全局内存, SM可能大部分时间都在等待数据而处于空闲状态. 因此, 思考内存问题, 在某种程度上就是思考GPU如何工作的关键.

### GPU的执行与内存模型

在为GPU编写高性能代码时, 你必须思考其**执行模型**和**内存模型**.

**[GPU执行模型: 线程、线程束与线程块](./Lecture5-GPU-Execution-Model.md)** 是核心. 它有三个层次的粒度需要你思考:
- **线程 (Thread)**: 执行工作的最小单位.
- **线程块 (Block)**: 一组线程, 每一个线程块会被分配到一个SM上执行.
- **线程束 (Warp)**: 线程块内的线程会以32个为一组, 称作一个线程束, 一同执行.

![GPU执行模型示意图](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/1oQ9_wI15wO5v-G6_Z6-R41Q.png)
> 图8: 一个CUDA程序被划分为多个线程块 (Blocks), 每个块被分配到一个SM上. 在SM内部, 块被进一步划分为线程束 (Warps), 由Warp调度器执行.

**内存模型**则定义了不同执行单元可以访问的内存范围:
- 每个线程拥有自己的**寄存器 (Register)**和**本地内存 (Local Memory)**.
- 同一个线程块内的所有线程共享该块的**共享内存 (Shared Memory)**.
- 所有线程都可以访问**全局内存 (Global Memory)**.

关键点是: **跨越线程块的信息传递需要通过缓慢的全局内存来读写.** 因此, 理想的程序应该让一个线程块处理一小块数据, 将其加载到快速的共享内存中, 完成所有计算后再写回.

### GPU模型的优势

- **易于扩展**: 通过增加更多的SM, 可以轻松扩展以处理繁重的工作负载.
- **易于编程 (?)**: 由于**SIMT (单指令多线程)**模型, 编程相对容易. 每个线程束中的所有线程执行相同的指令, 只是处理不同的数据.
- **线程轻量级**: 线程可以被非常快速地停止和启动, 这使得GPU能够高效地隐藏延迟, 保持高利用率.

### 让机器学习工作负载在GPU上跑得更快

现在你们都是GPU专家了. 我们来谈谈如何让机器学习工作负载在GPU上跑得飞快.

核心问题是: **我们如何避免受限于内存?**

![屋顶线模型 (Roofline Model)](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/5V-u7w-D9-L5gA27pB62Y-7L.png)
> 图9: 屋顶线模型展示了性能 (吞吐量) 如何受到计算强度和内存带宽的限制.

这张图是**屋顶线模型 (Roofline Model)**, 它描绘了性能的两个区域:
- **受内存限制 (Memory-bound)**: 在图的左侧斜坡部分, 性能的瓶颈是内存带宽, 即使有再多的计算能力也无法发挥.
- **受计算限制 (Compute-bound)**: 在图的右侧平顶部分, 我们已经充分利用了计算单元, 性能达到了硬件的理论峰值.

我们的目标是让我们的算法处于右侧区域. 以下是一些实现这一目标的技巧.

#### 技巧1: 控制流分歧 (非内存问题)

正如前面所说, GPU在SIMT模型下运行. 这意味着在一个线程束中, 所有32个线程都在执行相同的指令.

![控制流分歧的执行示意图](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/wYI8z7-e-j5-M6-i_V7M-g8c.png)
> 图10: 当线程束遇到if-else分支时, 一部分线程执行if代码块 (另一部分休眠), 然后另一部分线程执行else代码块 (第一部分休眠), 导致总执行时间是两个分支之和.

当代码中出现`if-else`这样的条件分支时, 如果一个线程束内的线程走向了不同的分支, 就会发生**控制流分歧 (Control Divergence)**. GPU必须串行执行这两个分支, 导致性能下降.

#### 技巧2: 低精度计算

这是一个重要的技巧. 使用较低的数值精度有几个好处:
- **减少数据移动**: 如果你的位数更少, 你需要移动的比特就更少.
- **利用专用硬件**: 现代GPU有专门为低精度 (如FP16, BF16, INT8) 设计的硬件, 如**张量核 (Tensor Core)**, 它们执行矩阵乘法的速度远超标准FP32单元.

![混合精度计算在Tensor Core中的实现](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/i2V0N-e-y9I-P-d4v3R-g6g.png)
> 图11: Tensor Core使用16位输入进行乘法, 但在32位累加器中进行求和, 以保持数值精度.

**混合精度训练 (Mixed-Precision Training)**就是利用了这一点. 大部分计算 (如矩阵乘法) 可以用16位完成以获得速度, 而一些需要更高精度的部分 (如累加或损失函数计算) 则保持在32位.

#### 技巧3: 算子融合 (Operator Fusion)

把GPU想象成一个工厂. 输入来自一个仓库 (内存), 然后在工厂里加工 (计算). 如果计算能力 (工厂规模) 提升了, 但内存带宽 (传送带速度) 没有, 那么传送带就会成为瓶颈.

![GPU作为工厂的比喻](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/i-T_n5p-J-H_t1J7Q3X1I-l2.png)
> 图12: 随着计算能力的扩展, 内存带宽成为瓶颈.

如果我们有一系列操作, 比如A->B->C->D, 天真的做法是每次操作后都把结果写回全局内存, 然后再读出来进行下一次操作. 这就像把半成品反复运回仓库再取出来一样愚蠢.

![未融合与融合的内核内存访问对比](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/t-F8u8R6A1W9T-m_U-l7H9b3.png)
> 图13: 未融合的内核 (左) 需要多次往返内存, 而融合的内核 (右) 将中间结果保留在计算单元 (如寄存器) 中, 只需一次内存读写.

**算子融合 (Operator Fusion)**就是将多个连续的操作合并成一个单一的GPU内核. 这样, 中间结果可以一直保留在快速的寄存器或共享内存中, 极大地减少了对慢速全局内存的访问. 像`torch.compile`这样的编译器可以自动完成许多简单的融合.

#### 技巧4: 重计算 (Recomputation)

在反向传播中, 为了计算梯度, 我们通常需要存储前向传播过程中的**激活值**. 存储和读取这些激活值可能会非常昂贵, 尤其是当模型很深时.

![前向传播和反向传播示意图](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/t-X-n-R3S1k-M_g-v4A_V2S0.png)
> 图14: 在反向传播中, 我们存储激活值(黄色)并计算雅可比矩阵(绿色).

**重计算 (Recomputation)**, 也被称为**激活检查点 (Activation Checkpointing)**, 是一种用计算换内存的策略. 在前向传播时, 我们不存储所有的激活值, 只存储其中一部分 (检查点). 在反向传播时, 当需要一个未存储的激活值时, 我们从最近的一个检查点开始, **重新计算**前向传播路径以得到它.

![重计算前后的内存访问对比](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/m_u-U3q_D8F8N7E-k_g-r-X8.png)
> 图15: 通过在前向传播中丢弃激活值并在反向传播中重计算它们, 我们可以显著减少内存读写次数.

丢弃计算结果然后重新计算, 听起来可能效率低下, 但如果你的瓶颈是内存带宽, 这样做实际上可能是最优的.

#### 技巧5: 内存合并访问 (Memory Coalescing)

DRAM (全局内存) 是以**突发模式 (burst mode)**读取的. 这意味着每次读取请求, 你得到的不仅仅是你请求的那一个字节, 而是它周围的一整块数据 (一个burst section).

![DRAM的突发模式](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/n3I158e2K-4Y6hH4QO5L32P4.png)
> 图16: 内存地址空间被划分为突发段. 访问段内任何位置都会导致整个段被传送到处理器.

**内存合并访问 (Memory Coalescing)**是一个关键的优化. 如果一个线程束 (warp) 中的所有32个线程同时发出的内存读取请求, 都落在同一个突发段内, 那么硬件可以将这32个请求合并成一个单一的DRAM请求. 这会带来巨大的性能提升.

![合并与非合并的内存加载](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/t-F8u8R6A1W9T-m_U-l7H9b3.png)
> 图17: 当一个线程束中的所有线程访问同一个突发段时, 访问是合并的 (左); 否则是非合并的, 会触发多次DRAM请求.

对于行主序 (row-major) 的矩阵, 如果线程是沿着行移动, 那么它们的访问是连续的, 很容易实现合并访问. 如果是沿着列移动, 访问就会跳跃, 导致非合并访问.

#### 技巧6: 平铺 (Tiling)

这是最重要的技巧之一: **[平铺优化 (Tiling for Performance)](./Lecture5-Tiling-for-Performance.md)**. 它的核心思想是通过组织线程来最大化数据重用, 从而最小化对全局内存的访问.

让我们回到矩阵乘法. 在一个朴素的实现中, 每个线程计算输出矩阵的一个元素. 这会导致对输入矩阵的元素进行大量重复和非合并的读取.

![朴素矩阵乘法中的重复内存访问](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/8rU-8G2t54-7K9OQ6-wQz20-.png)
> 图18: 在朴素实现中, M0,0和N1,0等元素被多次从全局内存中读取.

通过平铺, 我们将大矩阵分割成小的**“瓦片 (tiles)”**. 计算过程分阶段进行:
1.  一个线程块负责加载输入矩阵M和N的一个瓦片到快速的**共享内存**中.
2.  线程块内的所有线程使用共享内存中的数据, 计算输出矩阵P的一个瓦片的部分和.
3.  加载下一组瓦片, 更新部分和.
4.  重复此过程, 直到所有瓦片都被处理完毕.

![平铺矩阵乘法示意图](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/m2q00g-Xj-jE5uX2Gq36sM9j.png)
> 图19: 通过将瓦片加载到共享内存, 重复读取发生在快速内存中, 并且加载过程可以被设计为合并访问.

这种方法的优势是巨大的: 重复的读取现在访问的是共享内存, 而不是全局内存, 并且加载瓦片的过程可以精心设计以实现内存合并访问.

### 融会贯通: 理解一个矩阵之谜

现在我们已经掌握了这些技巧, 让我们回到开头那个神秘的性能图.

![矩阵乘法性能图](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/k_pW1s1t3wK53gQp21w3UqYv.png)
> 图20: 再看这张图, 我们现在可以识别出一些模式, 如计算强度和可能的平铺效应.

这张图展示了不同矩阵维度下的性能. 我们可以用我们学到的知识来解释它.

![平铺对性能的影响](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/o_T_qC6Vp4g94d4iP_9lY4iM.png)
> 图21: 将性能点按矩阵维度是否能被K整除进行着色. 维度对齐得越好 (可被更大的数整除), 性能越高.

**第一部分: 平铺**. 图中不同的性能曲线簇, 实际上对应着不同的**内存对齐 (memory alignment)**情况. 当矩阵的维度能够很好地被瓦片大小或内存突发段大小整除时 (例如, 是128的倍数), 内存访问就是对齐的, 性能就高 (紫色点). 当对齐很差时, 就会导致“坏瓦片”, 需要多次内存读取, 性能就低 (蓝色点). 这解释了为什么Andre Karpathy通过将词汇表大小从50257增加到50304 (64的倍数) 获得了25%的加速——他改善了内存对齐, 使得GPU可以走上一个利用率高得多的内核路径.

**第二部分: 波浪量化 (Wave Quantization)**. 那些周期性的性能波动是怎么回事?

![波浪量化效应](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/1oQ9_wI15wO5v-G6_Z6-r41Q.png)
> 图22: 当矩阵维度从1792增加到1793时, 性能出现断崖式下跌.

这与GPU上**SM (流式多处理器)**的数量有关. 假设我们使用256x128的瓦片大小:
- 当矩阵大小为1792x1792时, 我们需要 `(1792/256) x (1792/128) = 7 x 14 = 98` 个线程块 (tiles).
- 当大小增加到1793x1793时, 我们需要 `ceil(1793/256) x ceil(1793/128) = 8 x 15 = 120` 个线程块.

一个A100 GPU有108个SM. 当有98个块时, 它们可以全部被同时调度到不同的SM上, 并行执行, 这被称为一个**“波 (wave)”**. 当有120个块时, GPU无法一次性执行所有块. 它会先执行一个包含108个块的波, 完成后再执行第二个只包含12个块的波. 第二个波的**利用率极低**, 导致整体性能急剧下降. 这就是所谓的**波浪量化**效应.

### 案例研究: FlashAttention

现在我们用所学的一切来理解 **[FlashAttention详解](./Lecture5-Flash-Attention-Explained.md)**. 它通过应用两个已建立的技术——**平铺 (tiling)**和**重计算 (recomputation)**——来显著加速注意力计算.

![标准注意力和FlashAttention的性能对比](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/i2V0N-e-y9I-P-d4v3R-g6g.png)
> 图23: FlashAttention将HBM (全局内存) 的读写从40.3GB减少到4.4GB, 运行时间从41.7ms减少到7.3ms.

标准的注意力计算需要实例化一个巨大的N x N的注意力分数矩阵, 这需要O(N^2)的内存. FlashAttention的核心思想是避免完全物化这个矩阵.

它将Q, K, V矩阵分割成瓦片. 然后, 它在外部循环中遍历K和V的瓦片, 在内部循环中遍历Q的瓦片. 对于每一对Q和K的瓦片, 它将它们加载到SRAM中, 计算出一个N_tile x N_tile的注意力分数块, **但最关键的是, 它并不将这个块写回全局内存**.

![FlashAttention中的平铺操作](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/i-T_n5p-J-H_t1J7Q3X1I-l2.png)
> 图24: FlashAttention将Q, K, V矩阵平铺, 将块加载到SRAM中进行计算, 避免写回大的中间矩阵.

但Softmax是一个全局操作, 它需要知道整行的最大值和总和. FlashAttention如何做到这一点呢? 它使用了一种叫做**在线Softmax (online softmax)**的技巧.

![标准Softmax与在线Softmax](https://storage.googleapis.com/static.slab.com/prod/uploads/7d890538/posts/images/t-F8u8R6A1W9T-m_U-l7H9b3.png)
> 图25: 在线Softmax算法允许以流式方式、逐块地更新Softmax的归一化项.

它在处理每个瓦片时, 会跟踪当前为止看到的最大值和归一化项的总和. 当处理下一个瓦片时, 它会用新的数据更新这两个统计量, 并对之前计算出的输出值进行重新缩放以修正它们. 这种巧妙的伸缩求和技巧 (telescoping sum trick) 使得Softmax可以逐瓦片计算.

因此, FlashAttention的整个前向传播过程可以看作是:
1.  **逐瓦片**计算内部乘积S = QK^T.
2.  **融合**指数算子.
3.  通过在线、伸缩求和技巧**逐瓦片**计算Softmax.

对于反向传播, 它同样采用逐瓦片的方式, 并通过**重计算**来避免存储巨大的注意力分数矩阵.

### 课程总结

- **硬件驱动扩展**: 硬件能力是扩展的基础, 而底层的细节决定了什么能扩展, 什么不能.
- **Matmul + 数据移动**: 当前基于GPU的计算强烈鼓励我们围绕矩阵乘法 (Matmul) 和数据移动来思考问题.
- **精心思考GPU**: 仔细思考GPU的特性 (合并访问、平铺、融合) 会带来良好的性能.