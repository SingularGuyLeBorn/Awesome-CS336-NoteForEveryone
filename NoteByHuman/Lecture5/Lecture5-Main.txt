好的, 我将严格按照您的要求, 将课程幻灯片中的所有图片按照指定的Markdown格式和图注规范, 整合到我已经生成的 `Main.md` 和各个精英笔记中. 这将确保最终的讲义不仅内容完整, 而且图文并茂, 具有极高的可读性.

以下是更新后的完整输出.

--- EDITORIAL BLUEPRINT ---
- **精英链接笔记 (创建独立文件, 总数不超过5个):**
  - **GPU内存层级 (GPU Memory Hierarchy)** [理由: 这是理解所有GPU性能优化的绝对基石. 全局内存与片上内存之间巨大的延迟差异是所有优化技巧试图解决的核心矛盾. 深入探讨其物理基础 (SRAM vs. DRAM) 和架构设计, 能让读者从“为什么”的层面领会优化的必要性. ]
  - **GPU执行模型: 网格、线程块与线程束 (GPU Execution Model: Grid, Block & Warp)** [理由: 这是另一大基础支柱, 阐明了大规模并行任务是如何被组织和调度的. 理解Grid-Block-Thread的逻辑层次如何映射到SM-Warp的物理执行单元, 对于掌握Tiling和内存合并等高级技巧至关重要. ]
  - **平铺 (Tiling)** [理由: 讲师明确称之为“大家伙 (the big one)”. 这是讲座中最重要的性能优化技术, 它完美结合了内存层级和执行模型的知识, 是GPU高性能计算的典范. 其工作机制值得一篇独立的笔记进行深度剖析. ]
  - **内存合并 (Memory Coalescing)** [理由: 这是一个非常底层且非直观的优化. 它解释了线程束 (Warp) 内的并行访存请求如何与DRAM的物理特性 (突发模式, Burst Mode) 相互作用. 深入理解这一机制是榨干内存带宽的关键. ]
  - **FlashAttention中的在线Softmax (Online Softmax in FlashAttention)** [理由: 这是使FlashAttention成为可能的关键*算法创新*. 它通过一个非凡的数学技巧 (伸缩和, Telescoping Sum) 将一个全局操作 (Softmax) 转化为逐块计算的局部操作, 从而实现了整个注意力模块的算子融合. 其理论深度远超课堂范畴. ]
- **正文内详述概念 (仅在正文加粗, 不创建链接):**
  - CPU vs. GPU 架构差异
  - Tensor Core (张量核心)
  - SIMT (单指令多线程) 模型
  - 控制发散 (Control Divergence)
  - 低精度与混合精度计算
  - 算子融合 (Operator Fusion)
  - 重计算 (Recomputation / 激活检查点)
  - “矩阵之谜”与波形量化 (Wave Quantization)
--- END OF BLUEPRINT ---

--- FILE: Lecture5-Main.md ---
大家好. 希望大家在完成作业1的过程中一切顺利. 作业今晚截止, 如果需要延期请告知我们. 作业2也即将发布, 我们正在对其中的Triton部分进行最后的完善. 希望你们会喜欢, 你们将有机会实现Flash Attention 2或其部分内容, 我认为这会很棒.

今天我们来讨论GPU. GPU是驱动我们语言模型运行的核心部件, 因此正确地理解它至关重要. 如果你没有真正研究过驱动模型运行的硬件, 它们可能看起来相当神秘. 所以, 我今天的目标是让CUDA和GPU不再那么“魔法”.

### 1. 本讲目标: 揭开GPU的神秘面纱

![本讲大纲和目标](https://pic4.zhimg.com/v2-9f79cfd8219fb496d0163e0ee12d5195_1440w.jpg)
> 图 1: 本讲的三个核心目标: 揭开GPU的神秘面纱, 理解其性能瓶颈, 并通过FlashAttention案例掌握如何设计高速算法.

本讲主要有三个学习目标:
1.  **揭开CUDA和GPU的神秘面纱**: 课程结束后, 你应该对GPU感到自如, 并理解它们的工作原理.
2.  **理解GPU性能瓶颈**: 我们会探讨GPU在何种情况下会变慢. 比如上图左侧的矩阵乘法性能图, 随着矩阵尺寸增加, 性能呈现出非常难以预测的波浪形态. 为何在某些特定尺寸的倍数上GPU性能会突飞猛进, 而在其他尺寸上则会骤降? 我们将解开这个谜团.
3.  **学习如何构建高速算法**: 以**FlashAttention**为例, 它通过巧妙地计算Transformer中的注意力操作, 使得更长的上下文处理成为可能. 我们将拆解其背后的原理, 让你了解构建类似高效算法需要哪些基础模块和组件.

在硬件方面, 我要特别感谢一些资料来源, 尤其是Horace He的博客, 其中有很多关于GPU的有趣知识. 比如, 为什么填充了零的矩阵乘法比没有填充零的更快? 你可以在他的博客中找到答案.

![课程组织结构](https://pic4.zhimg.com/v2-b2b69f98245374a8b050d9f425bd22a3_1440w.jpg)
> 图 2: 本次课程的三个主要部分.

今天的课程将分为三个部分:
- **Part 1: 深入GPU** - 它们如何工作以及重要的组成部分.
- **Part 2: 理解GPU性能** - 什么让GPU变快, 什么让它变慢.
- **Part 3: 融会贯通** - 拆解FlashAttention.

### 2. 时代背景: 从串行计算到并行计算的飞跃

我们知道, 为大型语言模型提供更多的计算资源是有益的. 不论是预训练还是推理, 更多的计算力意味着可以处理更多数据、训练更大的模型, 最终带来性能的提升. 可以说, 如今的进展很大程度上是由**更快的硬件、更高的利用率和改进的并行化**共同驱动的.

![登纳德缩放定律的终结](https://pic4.zhimg.com/v2-cc92e0f61a3be1f2292671431916854d_1440w.jpg)
> 图 3: 著名的处理器性能历史图表. 单线程性能 (蓝色圆点) 在2000年代中期趋于平缓, 标志着依赖频率提升的“免费午餐”时代结束.

那么, 计算力的提升从何而来? 早期, 我们依赖**登纳德缩放定律(Dennard scaling)**: 随着晶体管越做越小, 它们可以在更低的功耗下以更快的时钟频率运行, 这带来了单线程性能的“免费午餐”. 然而, 大约在2000年代中期, 这种方式走到了尽头, 单线程性能的提升趋于停滞.

![并行计算的崛起](https://pica.zhimg.com/v2-1ef0dfd8277014336ac8ffa8e5866fe7_1440w.jpg)
> 图 4: Bill Dally展示的GPU并行计算能力的指数级增长. 如果没有GPU的并行扩展, 就没有我们今天所知的大语言模型扩展.

当单线程性能的道路被堵死后, **并行计算**成为了新的出路. GPU的并行扩展在过去十年中实现了超过1000倍的性能飞跃. 这也引出了我们的核心议题: GPU是如何实现这种大规模并行的? 它与CPU有何根本不同?

### Part 1: 深入GPU

#### 1.1 CPU vs. GPU: 延迟与吞吐量的抉择

CPU和GPU为不同的目标而优化: **CPU优化延迟 (latency)**, 而**GPU优化吞吐量 (throughput)**.

![CPU与GPU架构对比](https://pic1.zhimg.com/v2-30edc9e8d05781b6420f48c2ddc40719_1440w.jpg)
> 图 5: CPU (左) 将大量芯片面积用于复杂的控制逻辑和缓存, 以快速完成单个任务. GPU (右) 则将绝大部分面积用于大量的简单计算单元 (ALU), 以同时处理海量任务.

- **CPU (中央处理器)**: 就像一个专家团队, 每个核心都非常强大, 拥有复杂的控制逻辑 (如分支预测、乱序执行) 和巨大的缓存. 它的目标是让单个任务 (线程) 尽快完成. 这对于需要快速响应的通用计算任务至关重要.
- **GPU (图形处理器)**: 就像一个庞大的工人军团, 每个计算单元 (ALU) 都很简单, 但数量极其庞大. 它牺牲了单个任务的执行速度, 换取了同时处理成千上万个任务的能力. 它的目标是在单位时间内完成尽可能多的工作总量.

![CPU与GPU执行模式对比](https://pic4.zhimg.com/v2-bf449911b95b0bc678c00a0c9ee6dea5_1440w.jpg)
> 图 6: CPU核心 (下) 专注于快速完成每个任务 (T1, T2, T3, T4). GPU (上) 通过在等待数据 (橙色) 的线程间快速切换, 保持计算单元 (绿色) 持续繁忙, 从而实现高吞吐量.

#### 1.2 GPU的解剖学 (一): 执行单元

GPU的并行能力源于其层次化的执行单元结构.
- **流式多处理器 (Streaming Multiprocessor, SM)**: 这是GPU的一个核心构建块, 可以看作一个独立的计算引擎. 一个GPU通常包含数十到上百个SM (例如, A100 GPU有128个SM).
- **流处理器 (Streaming Processor, SP) 或 CUDA核心**: 每个SM内部又包含大量的SP. 这些是执行实际算术运算 (如浮点加法和乘法) 的最基本单元.

![GA100 GPU架构图](https://pica.zhimg.com/v2-d16c32a7b11f7c5dadddbe631145b4aa_1440w.jpg)
> 图 7: NVIDIA GA100 GPU的芯片图, 绿色区域是SM. GPU通过集成大量的SM来实现大规模并行.

当你在GPU上编程时, 你的任务会被组织成一个**[GPU执行模型: 网格、线程块与线程束](./Lecture5-GPU-Execution-Model.md)**. 简单来说, 一个大的任务 (称为一个**Grid**) 被划分为多个**线程块 (Block)**, 每个Block被调度到一个SM上独立执行. Block内部又由许多**线程 (Thread)**组成, 这些线程是并行执行任务的最小单位.

#### 1.3 GPU的解剖学 (二): 内存系统

理解GPU的性能, 关键在于理解其**[GPU内存层级 (GPU Memory Hierarchy)](./Lecture5-GPU-Memory-Hierarchy.md)**. 内存离SM越近, 速度越快, 但容量也越小、成本越高.

![GPU内存层级与延迟](https://pic3.zhimg.com/v2-aca36fa6ad71d552803a5caabedf570e_1440w.jpg)
> 图 8: GPU内存层级示意. 片上 (On-chip) 的L1缓存/共享内存速度极快 (延迟约几十个时钟周期), 而片外 (Off-chip) 的全局内存 (DRAM/HBM) 则慢得多 (延迟数百个时钟周期).

- **寄存器 (Registers)**: 每个线程私有, 速度最快.
- **L1缓存/共享内存 (Shared Memory)**: 每个SM内建, 速度极快 (基于SRAM技术). 共享内存允许一个Block内的所有线程高效地共享数据, 这是**平铺 (Tiling)**等高级优化的基础.
- **L2缓存**: 所有SM共享, 速度较快, 容量更大.
- **全局内存 (Global Memory)**: 通常指GPU显存 (基于DRAM/HBM技术), 容量最大 (几十GB), 但速度最慢. 它是GPU与主机 (CPU) 交换数据的地方, 也是不同Block之间通信的唯一途径.

**关键点**: 从全局内存读取一次数据的延迟, 可能足够一个计算单元执行成百上千次浮点运算. 这就是所谓的“**内存墙 (Memory Wall)**”问题: **计算速度的增长远超内存访问速度的增长**. 因此, GPU性能优化的核心就是**最大限度地减少对慢速全局内存的访问**.

![计算与内存带宽的增长差异](https://pic3.zhimg.com/v2-d8c6cc4725d35b72dd56940a8f40a562_1440w.jpg)
> 图 9: 计算能力 (黑色) 的增长速度远远超过了DRAM内存带宽 (绿色) 的增长速度. 这使得我们越来越难以让计算单元保持数据饱和状态.

### Part 2: 让机器学习负载在GPU上跑得更快

性能优化可以看作是一场与内存延迟和带宽的博弈. 我们的核心问题是: **如何避免受限于内存?**

![Roofline模型](https://picx.zhimg.com/v2-7ece08ba5fed55535ecdba157f9ec173_1440w.jpg)
> 图 10: Roofline模型. 程序的性能上限 (Throughput) 取决于其计算强度 (Operational Intensity, 即每字节内存访问对应的浮点运算次数) 和硬件的峰值性能/内存带宽. 程序的性能要么受限于内存带宽 (斜线部分), 要么受限于计算峰值 (水平线部分).

#### 2.1 六大优化技巧

以下是六个关键的优化技巧, 用于提升GPU上机器学习工作负载的性能.

1.  **控制发散 (Control Divergence)**
    GPU采用**SIMT (单指令多线程)**模型, 即一个线程束 (Warp, 通常是32个线程) 中的所有线程在同一时间执行完全相同的指令. 如果代码中出现条件分支 (if-else), 就会产生**控制发散**: 一部分线程执行`if`分支, 另一部分执行`else`分支. 由于SIMT的限制, 硬件实际上会串行执行这两个分支, 每个分支执行时, 另一部分线程会被暂时“屏蔽”掉. 这会造成计算资源的浪费, 降低效率.

2.  **低精度计算 (Low Precision Computation)**
    这是最直接的优化之一. 使用更低的数据精度 (如从32位浮点数FP32降到16位浮点数FP16或BF16) 有两个好处:
    - **减少内存占用和带宽需求**: 数据量减半意味着传输时间减半.
    - **利用专门的硬件**: 现代GPU (如带有**Tensor Core**的NVIDIA GPU) 为低精度/混合精度计算提供了专门的、速度极快的硬件单元. 矩阵乘法在低精度下的速度可能是FP32的数倍.

3.  **算子融合 (Operator Fusion)**
    把GPU想象成一个工厂 (计算单元), 仓库是内存. 如果一系列操作 (A->B, B->C, C->D) 每个都需要从仓库取料再送回仓库, 那么来回运输 (内存读写) 的开销会非常大. **算子融合**就是将多个连续的操作合并成一个单一的GPU核 (Kernel), 使得中间结果 (如B和C) 直接保留在高速的片上内存 (寄存器或共享内存) 中, 无需写回慢速的全局内存, 从而大幅减少内存访问. `torch.compile`等现代编译器可以自动完成许多这类“简单”的融合.

    ![算子融合示意图](https://pic3.zhimg.com/v2-1d8bc11b6b31cc4a3a6e93bbc2f4970c_1440w.jpg)
    > 图 11: 未融合 (左), 每个操作都需与内存交互. 融合后 (右), 多个操作在一个“融合核”内完成, 中间结果不离开计算单元, 极大减少了内存往返.

4.  **重计算 (Recomputation)**
    也称为**激活检查点 (Activation Checkpointing)**. 在训练神经网络时, 反向传播需要前向传播过程中计算出的激活值. 存储所有这些激活值会消耗大量内存. **重计算**是一种用计算换内存的策略: 在前向传播时, 只存储少量关键的激活值 (检查点), 在反向传播需要某个激活值时, 从最近的检查点开始重新向前计算得到它. 这样虽然增加了计算量, 但极大地减少了内存占用, 使得训练更大的模型成为可能. 对于那些受内存带宽限制的操作, 这种“浪费”计算来节省内存访问的方式甚至可能提升整体速度.

    ![标准反向传播与重计算的内存访问对比](https://pic3.zhimg.com/v2-fef3ab99f682d4f52680347e16eced8c_1440w.jpg)
    > 图 12: 左侧为标准反向传播, 需存储所有中间激活值 (s1, s2), 导致8次内存读写. 右侧为重计算, 不存储s1和s2, 在反向传播时重新计算它们, 将内存读写次数减少到5次.

5.  **[内存合并 (Memory Coalescing)](./Lecture5-Memory-Coalescing.md)**
    这是一个底层的内存访问优化. 当一个线程束 (Warp) 中的32个线程同时访问全局内存时, 如果它们访问的地址是连续的且对齐的, GPU硬件可以将这些分散的请求**合并**成一次或少数几次对DRAM的大块“突发”读取. 这极大地提高了内存带宽的利用率. 反之, 如果访问模式是分散、随机的 (非合并的), 硬件就需要为每个线程单独发起一次内存事务, 效率会急剧下降. 这对于矩阵乘法等操作的数据布局至关重要.

6.  **[平铺 (Tiling)](./Lecture5-Tiling.md)**
    这是讲座中最重要的优化技巧, 被称为“大家伙”. **平铺**的核心思想是利用高速的**共享内存**来减少对慢速**全局内存**的访问. 以矩阵乘法为例:
    - **非平铺方法**: 计算结果矩阵的每个元素, 都需要完整地读取输入矩阵的一行和一列. 这会导致输入矩阵中的每个元素被反复地从慢速的全局内存中读取N次.
    - **平铺方法**: 将大矩阵切分成小的“瓦片” (Tile). 将计算所需的一对瓦片从全局内存加载到SM内部的快速共享内存中. 然后, Block内的所有线程只访问共享内存来完成这个瓦片对的所有计算. 当这对瓦片的计算全部完成后, 再加载下一对瓦片.
    - **优势**: 输入矩阵的每个元素只需要从全局内存读取 $N/T$ 次 (T为瓦片尺寸), 之后在瓦片内部被重用T次. 这将全局内存的访问次数减少了一个T因子, 极大地提升了性能.

    ![平铺矩阵乘法示意图](https://picx.zhimg.com/v2-669a219fa8f39a0d13f6bbd2df6e3255_1440w.jpg)
    > 图 13: 通过将矩阵A和B加载到共享内存中的小瓦片, 可以在高速内存中重复使用数据来计算结果矩阵C的局部和, 从而显著减少对全局内存的访问.

#### 2.2 “矩阵之谜”的解答

现在, 我们可以回头解答开篇的性能图之谜了. Andrej Karpathy曾发推说, 将nanoGPT的词汇表大小从50257增加到50304 (64的倍数), 获得了约25%的速度提升. 为什么更大的矩阵反而更快?

![矩阵性能之谜](https://picx.zhimg.com/v2-60ed618c0c1c534bb1b0de2832143811_1440w.jpg)
> 图 14: 性能图按矩阵维度是否能被特定值(K)整除进行着色. 可以看到, 能被更大数值(如32或128)整除的维度 (紫色、绿色点) 性能普遍更高.

- **原因一: 平铺与内存对齐**. 正如我们所见, 平铺技术对性能至关重要. 高效的平铺依赖于**内存合并**, 而内存合并又要求内存访问是对齐的. 选择64或128的倍数作为矩阵维度, 能让数据在内存中更好地对齐, 使得加载瓦片的操作能够实现完美的内存合并, 从而最大化带宽利用率.
- **原因二: 波形量化 (Wave Quantization)**. 性能图中的周期性骤降现象与SM的利用率有关. 假设一个A100 GPU有108个SM, 而一个特定的瓦片尺寸导致计算一个1792x1792的矩阵需要98个线程块. 这很好, 108个SM可以一次性处理完所有块. 但如果矩阵尺寸增加到1793, 即使只大了一点点, 所需的线程块数量可能会因为取整效应跃升到120个. 这时, GPU需要两“波”才能完成任务: 第一波108个SM满载运行, 第二波只有12个SM在工作, 另外96个处于空闲状态, 导致整体利用率大幅下降. 这就是所谓的**波形量化**效应.

### Part 3: 融会贯通 - 拆解FlashAttention

FlashAttention是一个典型的例子, 它综合运用了上述技巧, 尤其是**平铺**和**重计算**, 来优化Transformer中的注意力机制.

标准注意力计算的核心是 $Softmax(QK^T)V$. 这里面有两个性能瓶颈:
1.  $QK^T$ 矩阵的显式物化: 对于长度为N的序列, 这个矩阵的大小是 $N \times N$, 当N很大时, 会占用巨大的内存.
2.  Softmax的全局性: Softmax需要对 $QK^T$ 的每一行进行归一化, 这意味着在计算任何一个输出前, 必须知道整行的所有值, 这与我们希望的逐块计算 (平铺) 产生了根本冲突.

FlashAttention的解决方案:
1.  **平铺 (Tiling)**: 将Q, K, V矩阵切分成瓦片, 从HBM (全局内存) 加载到SRAM (共享内存) 中进行计算, 从而避免物化整个 $N \times N$ 的 $QK^T$ 矩阵.
2.  **[FlashAttention中的在线Softmax (Online Softmax)](./Lecture5-Online-Softmax.md)**: 这是最关键的算法创新. 它采用了一种**在线**计算Softmax的方法. 在处理每个K, V瓦片时, 它会更新两个统计量: 当前行的最大值和归一化项的分子总和. 通过一个巧妙的数学技巧 (伸缩和), 它可以在不访问未来数据的情况下, 正确地更新最终的Softmax输出. 这使得Softmax可以和矩阵乘法完美地融合在一个GPU核中, 实现了真正的端到端平铺计算.
3.  **重计算**: 在反向传播时, FlashAttention不会存储前向计算中巨大的 $QK^T$ 矩阵, 而是重新计算所需的瓦片, 这正是重计算思想的应用.

![FlashAttention前向传播图示](https://pic3.zhimg.com/v2-fdf7c079307d62728ca6502b99843ac0_1440w.jpg)
> 图 15: FlashAttention通过逐块计算内积 (S), 融合指数运算, 并使用在线技巧计算Softmax, 实现了对整个注意力操作的平铺计算, 避免了在HBM中物化巨大的中间矩阵.

### 全课总结

![课程回顾](https://pic3.zhimg.com/v2-86044d906229d7671a9774f05d992bdc_1440w.jpg)
> 图 16: 本讲核心概念回顾: 并行硬件扩展、数据移动的重要性以及优化技巧.

- **硬件驱动扩展**: 硬件的扩展能力由并行计算驱动, 而性能的发挥则取决于对底层细节的理解.
- **数据移动是核心**: 当前基于GPU的计算范式强烈鼓励我们围绕**矩阵乘法 + 数据移动**来思考问题. 性能瓶颈通常不在于计算本身, 而在于如何高效地将数据喂给计算单元.
- **精心设计是关键**: 仔细思考GPU的工作方式——**内存合并、平铺、算子融合**——是通往高性能的必由之路.

通过本讲, 希望你不仅理解了GPU是什么, 更掌握了如何让它为你的模型跑得更快的核心思想.
--- END OF FILE ---

--- FILE: Lecture5-GPU-Memory-Hierarchy.md ---
### GPU内存层级 (GPU Memory Hierarchy)

#### 1. 核心矛盾: 计算与访存的速度鸿沟

理解GPU性能优化的出发点, 必须认识到一个根本性的矛盾: **计算单元处理数据的速度, 远远快于从主内存中获取数据的速度**. 这个现象被称为“**内存墙 (Memory Wall)**”. 现代GPU可以在一纳秒内完成数千次浮点运算, 但从主显存 (DRAM) 中读取一个数据则可能需要数百纳秒. 如果计算单元总是处于等待数据的状态, 那么再强大的计算能力也无法发挥.

为了缓解这个矛盾, GPU设计了一套复杂的多级内存层级结构. 其基本原则是: **离计算单元越近的内存, 访问速度越快, 但容量越小, 成本也越高.**

![GPU内存金字塔](https://pic4.zhimg.com/v2-d011721d1f72b8ab363c478a5c608c43_1440w.jpg)
> 图 1: GPU内存层级的金字塔结构. 从底层的DRAM到顶层的SRAM, 带宽急剧增加, 但容量和成本也相应变化.

#### 2. 内存层级详解

一个典型的现代GPU内存系统可以分为以下几个主要层次, 从最慢到最快排列:

1.  **全局内存 (Global Memory / HBM / DRAM)**
    - **物理位置**: 位于GPU芯片**之外**的独立显存颗粒上, 通过高带宽总线与GPU核心连接.
    - **技术**: 通常使用高带宽内存 (HBM) 或GDDR SDRAM技术. 它们是动态随机存取存储器 (DRAM), 其存储单元由一个电容和一个晶体管构成, 需要周期性刷新来维持数据, 密度高但速度相对较慢.
    - **特点**:
        - **容量最大**: 几十GB, 是存储模型权重、数据集等大块数据的主要场所.
        - **延迟最高**: 访问延迟可达**数百个时钟周期**.
        - **作用域**: 对GPU上运行的所有线程都是可见的, 是不同线程块 (Block) 之间进行数据交换的唯一途径.
        - **瓶颈所在**: 这是绝大多数性能问题的根源.

2.  **L2缓存 (L2 Cache)**
    - **物理位置**: 位于GPU芯片**之上 (On-die)**, 但在流式多处理器 (SM) 之外.
    - **技术**: 静态随机存取存储器 (SRAM). SRAM的存储单元由多个晶体管构成 (通常是6个), 无需刷新, 速度远快于DRAM.
    - **特点**:
        - **容量中等**: MB级别.
        - **延迟较低**: 访问延迟比全局内存低一个数量级, 大约在**200个时钟周期**左右.
        - **作用域**: 所有SM共享, 作为全局内存和SM之间的一个高速缓冲.
        - **对程序员透明**: 通常由硬件自动管理, 程序员无法直接控制其内容.

3.  **L1缓存 / 共享内存 (L1 Cache / Shared Memory)**
    - **物理位置**: 位于**每个SM内部**, 与计算单元物理距离最近.
    - **技术**: SRAM, 速度极快.
    - **特点**:
        - **容量很小**: KB级别 (例如, A100 GPU每个SM有192KB的L1/共享内存空间).
        - **延迟极低**: 访问延迟仅为**几十个时钟周期**, 与寄存器访问速度在同一个数量级.
        - **双重角色**:
            - **L1缓存**: 作为L2缓存和全局内存的又一层硬件管理的缓存.
            - **共享内存 (Shared Memory)**: 这是**程序员可编程控制**的一块高速暂存区.
        - **作用域**: **仅对同一个线程块 (Block) 内的所有线程可见**.
        - **性能关键**: 共享内存是实现**平铺 (Tiling)**等高级优化技巧的核心. 程序员可以将需要重复使用的数据从全局内存显式加载到共享内存, Block内的所有线程就可以在接下来的计算中高速访问这些数据, 从而避免了多次对慢速全局内存的昂贵访问.

4.  **寄存器 (Registers)**
    - **物理位置**: 直接集成在SM的计算单元旁边.
    - **技术**: 最快的SRAM实现.
    - **特点**:
        - **容量最小**: 每个线程只能使用几十个寄存器.
        - **速度最快**: 访问延迟极低, 通常与计算指令的执行速度相匹配.
        - **作用域**: **每个线程私有**, 用于存储局部变量和计算的中间结果.

#### 3. 延迟的量化对比

下表直观地展示了不同内存类型之间的巨大性能差异 (以NVIDIA A100 GPU为例):

| 内存类型 | 作用域 | 典型延迟 (时钟周期) | 相对速度比较 |
| :--- | :--- | :--- | :--- |
| 寄存器 | 单个线程 | ~20-30 | ~1x (基准) |
| 共享内存 (L1) | 单个线程块 | ~30-40 | ~1.5x |
| L2缓存 | 所有线程块 | ~200 | ~8x |
| 全局内存 (HBM) | 所有线程块 | ~300-400+ | ~15x |

**结论**: 从全局内存中获取一次数据的时间, 足够GPU在共享内存中进行十几次的读写操作. 这种巨大的性能差异决定了GPU编程的**第一原则**: **将算法的内存访问模式与硬件的内存层级对齐, 最大化数据在高速缓存/内存中的停留和重用, 最小化对低速全局内存的访问.** 所有高级的性能优化技巧, 都是围绕这一核心原则展开的.
--- END OF FILE ---

--- FILE: Lecture5-GPU-Execution-Model.md ---
### GPU执行模型: 网格、线程块与线程束

GPU通过一个层次化的执行模型来组织和管理成千上万个并行任务. 理解这个模型是编写高效GPU程序 (如使用CUDA或Triton) 的前提. 该模型包含逻辑层面 (程序员如何组织代码) 和物理层面 (硬件如何执行代码) 的对应关系.

#### 1. 逻辑层面: Grid, Block, Thread

从程序员的角度, 一个将在GPU上执行的计算任务 (称为一个**核函数, Kernel**) 被组织成一个三级层次结构:

- **网格 (Grid)**: 代表整个核函数的全部执行实例. 一个Grid包含了本次调用需要执行的所有线程.
- **线程块 (Block)**: Grid被划分为若干个大小相同的线程块. Block是资源分配和调度的重要单位. **同一Block内的所有线程可以在同一个SM上协同工作, 并通过高速的共享内存 (Shared Memory) 来交换数据**. 这是实现线程间高效协作的基础.
- **线程 (Thread)**: Block由若干线程组成. 线程是执行计算的最小单位, 每个线程都有一个唯一的ID, 程序员可以利用这个ID来区分不同的线程, 让它们处理不同的数据.

![GPU执行模型逻辑层次](https://pica.zhimg.com/v2-3d857ac6ca3b1282de521537fd0cc27a_1440w.jpg)
> 图 1: 一个CUDA程序被组织成一个Grid, Grid由多个Block组成, 每个Block又由多个Thread组成.

这种Grid-Block-Thread的结构为程序员提供了强大的灵活性, 可以将问题映射到一维、二维或三维的线程网格上, 非常适合处理图像、矩阵等结构化数据.

#### 2. 物理层面: SM, Warp

当一个核函数被启动时, GPU硬件会按照以下方式执行它:

- **线程块到SM的映射**: GPU的调度器会将Grid中的**线程块 (Block)**分配给可用的**流式多处理器 (SM)**. 每个Block在生命周期内都会在一个SM上执行, 不会中途迁移. 一个SM可以同时执行一个或多个Block, 具体取决于每个Block所需的资源 (如寄存器、共享内存) 和SM的总资源量.

- **线程到线程束的组织**: 在SM内部, Block中的线程并不会独立执行, 而是被硬件自动分组为大小固定的**线程束 (Warp)**. 在NVIDIA GPU上, 一个Warp通常由**32个连续的线程**组成. Warp是硬件实际调度和执行的基本单位.

#### 3. SIMT: Warp的核心执行原则

Warp的执行遵循**单指令多线程 (Single-Instruction, Multiple-Thread, SIMT)**模型. 这意味着:
- **指令同步**: 在任何一个时钟周期, 一个Warp中的所有32个线程都**执行完全相同的指令**.
- **数据并行**: 尽管指令相同, 但每个线程可以根据自己的线程ID处理不同的数据.

![SIMT模型示意图](https://pic1.zhimg.com/v2-6e8076c635a597cd5b725df9e1ec16e0_1440w.jpg)
> 图 2: 指令解码器向Warp调度器发出一条指令, 该指令被广播给Warp内的所有CUDA核心 (线程), 每个核心在各自的寄存器上处理不同的数据.

**SIMT与控制发散**: SIMT模型对性能有深远影响. 当Warp中遇到条件分支 (如`if-else`) 并且不同线程选择不同路径时, 就会发生**控制发散 (Control Divergence)**. 硬件会串行化这两个分支: 首先执行`if`路径 (此时走`else`路径的线程被禁用), 然后再执行`else`路径 (此时走`if`路径的线程被禁用). 这导致Warp的一部分计算能力被浪费, 是需要尽量避免的性能陷阱.

#### 4. 完整映射关系

![GPU内存模型](https://picx.zhimg.com/v2-133f6a2b77154655b6808542539a2929_1440w.jpg)
> 图 3: 完整的执行流程. 程序员定义的Grid被分解为Blocks. 每个Block被分配到一个SM. 在SM内部, Block的线程被组织成Warps. SM上的Warp调度器选择一个“就绪”的Warp, 将其下一条指令分派给SM内的计算单元执行.

**总结**:
- **程序员视角 (逻辑)**: `Grid -> Block -> Thread`
- **硬件视角 (物理)**: `Kernel -> Blocks on SMs -> Warps of Threads`

这个执行模型的核心在于**线程块 (Block)**. 它既是程序员组织线程协作 (通过共享内存) 的单元, 也是硬件将任务映射到物理资源 (SM) 上的单元. 而**线程束 (Warp)**则是硬件层面实现高效指令分发和执行的机制, 其SIMT特性直接决定了内存访问模式 (内存合并) 和代码分支 (控制发散) 的性能表现.
--- END OF FILE ---

--- FILE: Lecture5-Tiling.md ---
### 平铺 (Tiling): GPU高性能计算的核心策略

**平铺 (Tiling)**, 有时也称为分块 (Blocking), 是GPU编程中最重要、最有效的性能优化技术之一. 它是一种通过**最大化数据重用**来**最小化对慢速全局内存访问**的策略, 直接应对了计算与访存速度之间的巨大鸿沟.

#### 1. 问题的根源: 数据重用与全局内存瓶颈

让我们以一个经典的计算任务——矩阵乘法 $C = A \times B$ 为例, 来分析为什么需要平铺.

在一个朴素的实现中, 计算结果矩阵C中的每一个元素 $C_{ij}$, 都需要A矩阵的第i行和B矩阵的第j列进行点积.

- **内存访问模式**:
  - 为了计算 $C_{i,j}$, 我们需要读取A的整行和B的整列.
  - 为了计算 $C_{i,j+1}$, 我们需要**重新读取**A的整行, 并读取B的下一列.
  - 为了计算 $C_{i+1,j}$, 我们需要读取A的下一行, 并**重新读取**B的整列.

![非平铺矩阵乘法的低效访存](https://pic1.zhimg.com/v2-c820c559d8c9a43ae67a05a5a5f481c8_1440w.jpg)
> 图 1: 在一个非平铺的矩阵乘法中, 计算结果P的不同元素会导致对输入矩阵M和N的相同元素的重复读取 (例如M0,0和N1,0被多个线程重复访问). 并且, 沿行读取通常会导致非合并的内存访问.

这种实现方式存在两个致命的性能问题:
1.  **极低的数据重用率**: 输入矩阵A和B中的每个元素, 都会被从**慢速的全局内存**中反复读取N次 (N是矩阵维度).
2.  **糟糕的内存访问模式**: 根据数据布局 (如行主序), 对行或列的访问很容易导致**非合并的内存访问**, 进一步降低了本已很低的内存带宽利用率.

#### 2. 解决方案: 利用共享内存进行平铺

平铺技术的核心就是利用SM内部**速度极快的共享内存 (Shared Memory)**来打破这一瓶颈.

**执行流程**:
1.  **划分瓦片 (Tile)**: 将大的输入矩阵A和B逻辑上划分为小的、可以完全载入共享内存的子矩阵, 称为“瓦片” (Tile).
2.  **协同加载**: 一个线程块 (Block) 负责计算结果矩阵C中的一个瓦片. 在计算开始前, 该Block内的所有线程**协同地**从全局内存中读取计算当前C瓦片所需的A瓦片和B瓦片, 并将它们存入共享内存. 这个加载过程本身也可以被优化以实现**内存合并**.
3.  **高速计算**: 一旦数据加载到共享内存, Block内的所有线程就可以在接下来的计算中, **只访问高速的共享内存**. 它们会在这里完成所有必要的乘加运算, 计算出C瓦片的**部分和 (Partial Sum)**.
4.  **循环迭代**: Block会循环加载A的行瓦片和B的列瓦片, 不断累加部分和, 直到最终计算出C瓦片的完整结果.
5.  **写回结果**: 最后, Block内的线程协同地将计算完成的C瓦片从共享内存写回到全局内存.

![平铺计算流程](https://pic1.zhimg.com/v2-c820c559d8c9a43ae67a05a5a5f481c8_1440w.jpg)
> 图 2: 平铺计算流程. (1) Block将M0,0和N0,0瓦片加载到共享内存. (2) 在共享内存中计算P的部分和. (3) 加载下一组瓦片 (如M0,0和N2,0), 继续累加P的部分和. (4) 如此循环, 直至完成.

#### 3. 性能收益的数学分析

![平铺的数学原理](https://pic4.zhimg.com/v2-c032667a40be8e6e7fad2488f89d6683_1440w.jpg)
> 图 3: 矩阵乘法的平铺视图. T是瓦片尺寸, N是矩阵尺寸.

- **非平铺矩阵乘法**: 每个输入元素需要从全局内存读取 **N** 次.
- **平铺矩阵乘法**: 每个输入元素只需要从全局内存读取 **N / T** 次 (T是瓦片尺寸), 以加载它所在的瓦片. 一旦瓦片进入共享内存, 它将在内部被重用 **T** 次.

最终, 全局内存的访问量被减少了 **T** 倍. 考虑到全局内存访问是性能的主要瓶颈, 这是一个巨大的提升.

#### 4. 平铺的复杂性

虽然平铺威力巨大, 但实现高效的平铺并非易事, 需要考虑诸多因素:
- **瓦片尺寸选择**: 瓦片尺寸必须仔细选择, 以平衡寄存器压力、共享内存容量, 并确保有足够的并行度来占满SM.
- **内存对齐**: 为了在加载瓦片时实现完美的内存合并, 矩阵的维度和数据在内存中的起始地址最好是特定值的倍数 (如128字节). 这也是为什么调整矩阵维度 (如padding) 有时能带来巨大性能提升的原因.
- **边界条件处理**: 当矩阵维度不能被瓦片尺寸整除时, 需要处理边缘的“不完整”瓦片, 这会增加代码的复杂性.

![平铺的复杂性: 维度不可整除问题](https://pic1.zhimg.com/v2-1c7c11901869649a09996111f1bbcc34_1440w.jpg)
> 图 4: 当矩阵维度 (如257) 不能被瓦片维度 (如128) 整除时, 会产生利用率很低的线程块, 浪费计算资源.

**总结**: 平铺是典型的“空间换时间”思想在GPU架构上的应用. 它通过使用SM内部宝贵但快速的共享内存空间, 换取了对慢速全局内存访问次数的大幅减少. 它是所有高性能GPU计算库 (如cuBLAS, cuDNN) 和领域特定语言 (如Triton) 中矩阵乘法实现的核心技术.
--- END OF FILE ---

--- FILE: Lecture5-Memory-Coalescing.md ---
### 内存合并 (Memory Coalescing)

**内存合并 (Memory Coalescing)** 是一种至关重要的底层GPU性能优化技术, 它旨在最大化全局内存 (DRAM) 带宽的利用率. 理解内存合并需要了解线程束 (Warp) 的执行方式以及DRAM的物理工作原理.

#### 1. 背景: Warp的同步执行与DRAM的突发模式

- **Warp的同步访存**: 在GPU的SIMT执行模型中, 一个线程束 (Warp) 内的32个线程在同一时刻执行相同的指令. 当这条指令是内存加载或存储指令时, 这32个线程会同时向内存系统发起访存请求.
- **DRAM的突发模式 (Burst Mode)**: 从物理层面看, DRAM的访问开销很大一部分来自于定位数据所在的行和列的初始延迟. 为了摊销这个开销, DRAM被设计为在一次请求中, 以“突发” (Burst) 的方式返回一大块连续的数据 (例如128字节), 而非单个字节. 这个数据块被称为一个**内存事务 (Memory Transaction)** 或 **段 (Segment)**. 从一个段中读取后续字节的成本远低于发起一次新的读取.

![DRAM的突发模式](https://picx.zhimg.com/v2-18533c89e7f2df03bceebd98ee1ec915_1440w.jpg)
> 图 1: 全局内存被划分为多个连续的“突发段” (Burst section). 当访问段内的任何一个位置时, 整个段的数据都会被传送到处理器.

#### 2. 什么是内存合并?

**内存合并**是指GPU的内存控制器能够将一个Warp中32个线程发出的多个独立的内存请求, **合并**成一次或极少数几次对DRAM的大块内存事务.

**理想情况 (完全合并的访问)**:
- Warp中的32个线程访问的内存地址是**连续的**.
- 这些地址恰好能落在一个或两个对齐的DRAM突发段内.

在这种情况下, 硬件可以高效地执行: 它只需要发起一次 (或两次) 内存事务, 就能满足Warp中所有32个线程的数据需求. 此时, 内存带宽的利用率接近100%.

![合并内存访问](https://picx.zhimg.com/v2-cc7a74e8a0e4439776c61c878a388247_1440w.jpg)
> 图 2: Warp中的线程 T0, T1, T2, T3 分别访问地址 0, 1, 2, 3. 由于这些地址是连续的且在一个突发段内, 硬件可以将这4个请求合并为一次DRAM请求, 实现完全合并访问.

**糟糕情况 (非合并的访问)**:
- Warp中的线程访问的内存地址是**随机、不连续的 (strided or scattered)**.
- 这些地址分散在许多不同的DRAM突发段中.

在这种情况下, 硬件无法进行合并. 它必须为每个线程 (或每几个线程) 单独发起一次内存事务. 如果32个线程的访问地址分散在32个不同的段中, 就可能需要执行32次独立的内存事务. 这将导致极低的内存带宽利用率, 性能急剧下降.

#### 3. 实例: 矩阵数据访问

内存合并的重要性在处理二维数据 (如矩阵) 时尤为突出. 假设我们有一个按**行主序 (row-major)**存储的矩阵, 这意味着矩阵的每一行在内存中是连续的.

- **非合并访问 (沿列遍历)**: 假设Warp中的第`i`个线程处理第`i`列. 当所有线程同时读取它们所在列的第一个元素时 (即M, M, M...), 它们访问的内存地址是不连续的 (间隔为矩阵的行宽). 这是一种典型的**跨步访问 (strided access)**, 会导致非常糟糕的非合并访问.

- **合并访问 (沿行遍历)**: 假设Warp中的第`i`个线程处理第`i`个元素. 当所有线程同时读取第一行的元素时 (即M, M, M...), 它们访问的内存地址是完全连续的. 这就是理想的**合并访问**.

![合并与非合并的矩阵访问模式](https://pic4.zhimg.com/v2-56925e035bb53932908caf8ea4a0d4af_1440w.jpg)
> 图 3: 对于行主序存储的矩阵, 沿行访问 (右侧B图, Coalesced) 是合并的, 因为相邻线程访问相邻内存地址. 沿列访问 (左侧A图, Not Coalesced) 是非合并的, 因为相邻线程访问的内存地址相隔一个行宽.

#### 4. 总结与实践建议

- **核心思想**: 内存合并是将逻辑上的并行线程访问模式与物理上的DRAM突发读取模式进行匹配的艺术.
- **编程准则**: 在编写GPU核函数时, 必须精心设计线程ID到内存地址的映射关系, 确保**同一个Warp内的相邻线程访问相邻的内存地址**.
- **常见应用**:
  - 在处理矩阵时, 优先选择沿数据连续存储的方向进行访问.
  - 在进行**平铺 (Tiling)**时, Block内的线程需要协同地、以合并的方式将数据瓦片从全局内存加载到共享内存.
  - 在某些情况下, 如果访问模式无法避免非合并, 可以先将数据加载到共享内存, 在共享内存中进行转置等操作, 然后再进行计算, 以此来将多次非合并的全局内存访问转化为一次非合并加载 + 多次快速的共享内存访问.

忽略内存合并是初学者编写GPU程序时最常见的性能错误之一. 一个看似无害的数据访问模式的改变, 可能会因为破坏了内存合并而导致数十倍的性能差异.
--- END OF FILE ---

--- FILE: Lecture5-Online-Softmax.md ---
### FlashAttention中的在线Softmax (Online Softmax)

**在线Softmax (Online Softmax)** 是FlashAttention算法的核心数学和算法创新. 它通过一种巧妙的增量计算方法, 将传统Softmax这一**全局**操作, 转化为可以**逐块 (tile-by-tile)** 处理的**局部**操作, 从而使得整个注意力计算过程能够被高效地平铺 (Tiling) 和融合 (Fusion) 到单个GPU核中.

#### 1. 传统Softmax的挑战: 全局性

标准的Softmax函数作用于一个向量 $x = [x_1, x_2, ..., x_N]$, 其定义为:
$$
Softmax(x)_i = \frac{e^{x_i}}{\sum_{j=1}^{N} e^{x_j}}
$$
为了数值稳定性, 我们通常会减去向量中的最大值:
$$
m = \max_{j}(x_j)
$$
$$
Softmax(x)_i = \frac{e^{x_i - m}}{\sum_{j=1}^{N} e^{x_j - m}}
$$
这里的挑战在于**全局性**:
- **全局最大值**: 在计算任何一个元素的分子之前, 我们需要遍历整个向量 $x$ 找到最大值 $m$.
- **全局求和**: 在计算分母时, 我们需要遍历整个向量 $x$, 计算所有元素的指数并求和.

在注意力的上下文中, 向量 $x$ 就是 $QK^T$ 矩阵的一行. 这意味着, 如果我们想用平铺技术, 每次只处理 $QK^T$ 的一个“瓦片” (一小部分列), 我们将无法完成Softmax计算, 因为我们没有看到完整的行, 既不知道全局最大值, 也无法计算完整的归一化分母.

#### 2. 在线Softmax: 增量计算的魔法

在线Softmax通过维护和迭代更新两个核心统计量, 解决了这个难题. 假设我们正在逐块处理行向量 $x$, 当前处理的是第 $i$ 个块 $x^{(i)}$.

我们维护两个统计量:
- $m^{(i)}$: 到目前为止 (处理完第 $i$ 个块后) 观察到的最大值.
- $l^{(i)}$: 到目前为止 (处理完第 $i$ 个块后) 的归一化分母的**未缩放**版本.

**算法流程**:
1.  **初始化**: $m^{(0)} = -\infty$, $l^{(0)} = 0$.

2.  **对于每个块 $x^{(i)}$ (从 i=1 到 B)**:
    a. **计算当前块的最大值**: $\tilde{m}^{(i)} = \max(x^{(i)})$.
    b. **找出新的全局最大值**: $m_{new} = \max(m^{(i-1)}, \tilde{m}^{(i)})$.
    c. **计算当前块的和**: $\tilde{l}^{(i)} = \sum_{j \in x^{(i)}} e^{x_j - m_{new}}$.
    d. **更新全局归一化项**:
    $$
    l^{(i)} = l^{(i-1)} \cdot e^{m^{(i-1)} - m_{new}} + \tilde{l}^{(i)}
    $$
    e. **更新全局最大值**: $m^{(i)} = m_{new}$.

![标准Softmax与在线Softmax的算法对比](https://pic1.zhimg.com/v2-c12b1514dcad0879a32bc425ed92435a_1440w.jpg)
> 图 1: 左侧为标准(安全)Softmax算法, 需要两次遍历数据 (一次找最大值, 一次求和). 右侧为在线Softmax算法, 只需一次遍历, 并在过程中增量更新最大值和归一化项.

#### 3. 数学推导: 伸缩和 (Telescoping Sum) 的威力

上述更新步骤d是整个算法的核心. 让我们来理解它为何正确.
假设我们已经处理了前 $i-1$ 个块, 得到了当时的最大值 $m^{(i-1)}$ 和分母 $l^{(i-1)}$:
$$
l^{(i-1)} = \sum_{j=1}^{i-1} \sum_{k \in x^{(j)}} e^{x_k - m^{(i-1)}}
$$
现在我们引入了第 $i$ 个块 $x^{(i)}$, 并找到了新的全局最大值 $m^{(i)} = m_{new}$. 我们希望计算新的总分母 $l^{(i)}$:
$$
l^{(i)} = \sum_{j=1}^{i} \sum_{k \in x^{(j)}} e^{x_k - m^{(i)}}
$$
我们可以将其拆分为两部分:
$$
l^{(i)} = \left( \sum_{j=1}^{i-1} \sum_{k \in x^{(j)}} e^{x_k - m^{(i)}} \right) + \left( \sum_{k \in x^{(i)}} e^{x_k - m^{(i)}} \right)
$$
第二部分正是我们计算的 $\tilde{l}^{(i)}$. 关键在于如何处理第一部分. 我们可以通过乘以 $1 = e^{m^{(i-1)}} / e^{m^{(i-1)}}$ 来巧妙地变换它:
$$
\sum_{j=1}^{i-1} \sum_{k \in x^{(j)}} e^{x_k - m^{(i)}} = \sum_{j=1}^{i-1} \sum_{k \in x^{(j)}} e^{x_k - m^{(i-1)} + m^{(i-1)} - m^{(i)}}
$$
$$
= e^{m^{(i-1)} - m^{(i)}} \cdot \left( \sum_{j=1}^{i-1} \sum_{k \in x^{(j)}} e^{x_k - m^{(i-1)}} \right)
$$
$$
= e^{m^{(i-1)} - m^{(i)}} \cdot l^{(i-1)}
$$
将两部分合在一起, 我们就得到了更新公式:
$$
l^{(i)} = l^{(i-1)} \cdot e^{m^{(i-1)} - m^{(i)}} + \tilde{l}^{(i)}
$$
这个公式的精妙之处在于, 它允许我们用**旧的统计量 ($m^{(i-1)}, l^{(i-1)}$) 和当前块的信息 ($\tilde{l}^{(i)}$)**, 正确地计算出**新的全局统计量 ($l^{(i)}$)**.

**对输出的调整**: 同样地, 当我们计算最终的注意力输出时, 如果因为一个新的块而导致全局最大值 $m$ 发生了变化, 我们也需要对**之前已经计算好的输出进行重新缩放**.
$$
O_{new} = O_{old} \cdot \frac{l_{old}}{l_{new}} \cdot e^{m_{old} - m_{new}} + \frac{1}{l_{new}} \cdot \Delta O
$$
其中 $\Delta O$ 是由当前块的K和V计算出的新输出部分.

#### 4. 结论

在线Softmax通过增量维护最大值和归一化项, 将Softmax的计算分解到每个数据块上.
- **消除了对全局信息的依赖**: 在处理任何一个瓦片时, 都不再需要访问该瓦片之外的数据来完成Softmax计算.
- **实现了完全的算子融合**: 使得矩阵乘法 ($Q \times K^T$, $P \times V$)、Softmax及其相关的指数、求和、缩放操作, 都可以被合并到同一个GPU核中.
- **避免了内存瓶颈**: 无需在全局内存中物化和读写巨大的 $N \times N$ 注意力矩阵.

正是这个算法上的突破, 才使得FlashAttention能够充分利用GPU的计算能力和内存层级, 实现了相对于标准注意力的大幅性能提升.
--- END OF FILE ---