# 第六讲:手写高性能算子

> 居然还是Tatsunori Hashimoto这位老师，他一开始不像Percy一样用py脚本用传统的PDF我还不太习惯，他自己一开始也说比较传统（？）然后我现在听了三节课终于习惯了他又改了！艹 damn。

大家好. 在本次课程中, 我们将深入探讨如何为GPU编写高性能代码. 这不仅仅是理论探讨, 在作业2中, 大家将需要进行大量的性能分析 (profiling), 为Flash Attention V2编写自己的Triton核函数 (kernel), 并致力于实现极致的性能. 本次课程的目标就是为构建语言模型中的标准组件编写高性能代码.

课程计划如下:

1. **GPU基础回顾**: 快速回顾GPU的关键组件, 为理解后续内容打下基础.
2. **基准测试 (Benchmarking) 与性能分析 (Profiling)**: 学习衡量与诊断代码性能的基本功, 这对完成作业和未来工作都至关重要.
3. **编写核函数**: 我们将亲手实践, 分别使用**CUDA C++**和**Triton**编写核函数.
4. **JIT编译器**: 探索利用PyTorch现有的即时 (Just-In-Time) 编译器 `torch.compile` 自动为我们优化代码的便捷方式.

在整个过程中, 我们将进行深入剖析, 甚至会探究到**PTX** (一种GPU的汇编语言), 以理解在不同代码实现下, GPU的底层实际工作情况. 最后, 我们将以编写一个快速的Softmax Triton实现作为收尾.

### 1. GPU核心概念回顾

让我们快速回顾一下GPU的工作原理. 无论是A100还是H100, 其核心都由大量的**流式多处理器 (Streaming Multiprocessors, SMs)** 构成.

#### 1.1 硬件架构

- **计算单元**: 每个SM内部包含众多可以执行计算的单元, 如INT32和FP32核心.
- **内存层级**:
  - **DRAM (全局内存)**: 容量巨大 (A100可达80GB), 但速度较慢.
  - **L1/L2缓存**: 容量小得多, 但速度快很多. L1缓存是每个SM独享的.
  - **寄存器文件 (Register File)**: 这是每个线程可以访问的**最快**的内存, 容量极小, 是我们编写高性能代码时需要重点利用的资源.
- **线程 (Threads)**: 每个SM能够启动成千上万个线程, 它们是执行计算的基本单位.

![GPU硬件架构图](https://miro.medium.com/v2/resize:fit:2000/format:webp/1*6xoBKi5kL2dZpivFe1-zgw.jpeg)

> 图 1: GPU的硬件架构示意图, 展示了SMs以及DRAM, 缓存, 寄存器等内存层级.

#### 1.2 执行模型

GPU的并行计算是通过一个层级化的结构来组织的:

- **线程 (Thread)**: 执行最基础的计算任务, 例如处理向量中的一个元素.
- **线程块 (Thread Block)**: 由一组线程构成. **一个线程块会被调度到单个SM上执行**. 这是我们思考和编写Triton代码时的原子单元.
- **网格 (Grid)**: 由所有线程块共同组成, 对应整个计算任务.

**为什么需要线程块?** 关键在于**共享内存 (Shared Memory)**. 同一个线程块内的线程可以通过SM内部的高速共享内存 (速度接近L1缓存) 进行快速的数据交换和同步. 这对于矩阵乘法这类需要线程间通信的任务至关重要. 而跨线程块的通信则非常昂贵, 通常需要通过慢速的全局内存进行.

- **Warp与Wave**: 在硬件执行层面, 线程被进一步组织成大小为32的组, 称为**Warp**. 同一个Warp中的32个线程会以“步调一致”的方式执行指令 (SIMD: 单指令多数据流). 调度器以Warp为单位进行调度. 而多个线程块会被组织成**Wave**的形式调度到整个GPU的SMs上. 为了最大化硬件利用率 (Occupancy), 我们希望避免出现最后一个Wave的线程块数量过少导致部分SM闲置的情况. 一个经验法则是, 线程块的总数最好是SM数量的4倍以上.

#### 1.3 算术强度

**算术强度 (Arithmetic Intensity)** 是一个衡量计算任务类型的核心指标, 其定义为:

$$
\text{Arithmetic Intensity} = \frac{\text{浮点运算次数 (FLOPs)}}{\text{内存访问字节数 (Bytes)}}
$$

- **高算术强度**: 意味着计算量远大于数据搬运量, 这种任务是**计算密集型 (Compute-Bound)**. 这是理想情况.
- **低算术强度**: 意味着大量时间消耗在数据读写上, 这种任务是**内存密集型 (Memory-Bound)**.

一个普遍的规律是: **精心实现的矩阵乘法是计算密集型的, 而其他绝大多数操作 (如逐元素加法, 激活函数) 都是内存密集型的**. 我们性能优化的一个核心目标, 就是通过各种技巧 (如核函数融合) 来提高操作的算术强度, 缓解内存瓶颈.

### 2. 基准测试与性能分析: 优化的前提

如果你想编写高性能代码, 那么最重要的原则就是: **永远要对你的代码进行基准测试和性能分析**. 凭空猜测性能瓶颈然后花费数小时进行优化, 结果发现优化错了地方, 是非常常见且低效的. 使用专业的分析工具, 你可以精确地定位到代码的瓶颈所在.

#### 2.1 基准测试 (Benchmarking)

基准测试旨在测量一段代码端到端的**墙上时钟时间 (wall-clock time)**. 它的主要用途是:

- 比较不同实现的优劣 (例如, 我们的手写核函数和PyTorch官方实现哪个更快).
- 理解性能如何随输入规模扩展 (例如, 矩阵尺寸增大一倍, 运行时间增加多少).

为了准确地进行基准测试, 我们需要注意两个关键点: **预热 (Warm-up)** 和 **同步 (Synchronization)**.

```python
def benchmark(description: str, run: Callable, num_warmups: int = 1, num_trials: int = 3):
    # 1. 预热: 首次运行时可能涉及编译、缓存加载等一次性开销.
    # 我们关心的是稳定状态下的性能, 因此先运行几次.
    for _ in range(num_warmups):
        run()

    # 2. 同步: 确保在开始计时前, 所有先前提交到GPU的任务都已完成.
    if torch.cuda.is_available():
        torch.cuda.synchronize()

    times: list[float] = []
    for trial in range(num_trials):
        start_time = time.time()
        run()
        # 3. 再次同步: 确保run()函数中所有提交到GPU的任务都完成后再记录结束时间.
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        end_time = time.time()
        times.append((end_time - start_time) * 1000) # 转换为毫秒

    mean_time = sum(times) / len(times)
    return mean_time
```

- **预热**: 首次执行PyTorch代码时, 底层可能会发生代码编译、数据向GPU传输等初始化操作. 我们需要通过几次“热身”运行来跳过这些一次性开销, 测量稳态性能.
- **同步**: CPU和GPU是两个独立的计算单元, 它们异步执行. 当CPU上的Python代码调用一个GPU操作 (如 `a @ b`) 时, 它只是把这个任务**提交 (dispatch)** 到一个队列中, 然后会继续执行下一行Python代码, 而不会等待GPU完成计算. 如果我们不进行同步, `end_time` 可能会在GPU实际完成工作前就被记录, 导致测量时间无限接近于零的假象. `torch.cuda.synchronize()` 会阻塞CPU, 直到GPU处理完所有已提交的任务.

#### 2.2 性能分析 (Profiling)

基准测试告诉我们代码**慢不慢**, 而性能分析则告诉我们**慢在哪里**. 它可以提供函数级别的耗时分解, 甚至能让我们看到PyTorch API背后调用的具体CUDA核函数.

使用PyTorch内置的Profiler, 我们可以观察到一个简单的加法操作 `a + b` 背后的调用栈:

- `aten::add`: PyTorch的C++前端接口.
- `vectorized_elementwise_kernel`: 底层实际执行逐元素计算的CUDA核函数.
- `cudaLaunchKernel`: CPU提交核函数到GPU的动作本身也需要时间.
- `cudaDeviceSynchronize`: 等待GPU完成的时间.

通过分析, 我们能发现, 即使是简单的操作, 其CPU端的开销也可能远大于GPU的实际计算时间, 特别是对于小规模的运算.

对于更复杂的模型, 如一个多层感知机 (MLP), PyTorch Profiler的文本输出可能不够直观. 这时, 我们需要更强大的可视化工具, 如**NVIDIA Nsight Systems**.

Nsight Systems能够清晰地展示CPU和GPU两条并行的**时间轴**, 揭示它们之间复杂的异步交互. 这是理解现代深度学习系统性能的强大武器. 深入分析其时间线可以揭示许多非直观的性能现象.
[剖析CPU与GPU的异步执行模型](./Lecture6-CPU-GPU-Asynchronicity.md)

### 3. 核函数融合 (Kernel Fusion): 性能优化的黄金法则

**核函数融合**是GPU编程中最重要的优化思想之一. 我们可以用一个形象的比喻来理解它:

- **DRAM (全局内存)** 就像一个**仓库**, 容量大但取货慢.
- **SRAM (共享内存/缓存)** 就像SM内部的**工厂车间**, 空间小但加工快.

如果我们执行一系列操作, 比如 `y = a * b + c`, 未经优化的朴素实现会是:

1. 从"仓库" (DRAM) 取 `a` 和 `b` 到"工厂" (SRAM/寄存器), 计算 `t = a * b`, 把结果 `t` 送回"仓库". (一次读写)
2. 从"仓库"取 `t` 和 `c` 到"工厂", 计算 `y = t + c`, 把结果 `y` 送回"仓库". (又一次读写)

这种方式在"仓库"和"工厂"之间造成了大量不必要的数据往返, 成为性能瓶颈. 而**核函数融合**的思想是, 将多个操作合并成一个**单一的、更大的核函数**. 在这个融合后的核函数中:

1. 一次性从"仓库"取来所有需要的原材料 (`a`, `b`, `c`).
2. 在"工厂"内部完成所有计算步骤 (`a * b`, 然后 `+ c`).
3. 最后将最终产品 `y` 一次性送回"仓库".

![核函数融合示意图](https://horace.io/img/perf_intro/operator_fusion.png)

> 图 2: 核函数融合将多个独立的读-算-写操作合并为一次读-算-写, 大幅减少了对慢速DRAM的访问.

为了验证其效果, 我们以**GeLU**激活函数为例. PyTorch的`torch.nn.functional.gelu`是一个高度优化的**融合**实现. 我们可以手动用多个基础PyTorch操作模拟一个**非融合**版本:

```python
# 非融合的手写GeLU
def manual_gelu(x: torch.Tensor):
    return 0.5 * x * (1 + torch.tanh(0.79788456 * (x + 0.044715 * x * x * x)))

# PyTorch的融合GeLU
def pytorch_gelu(x: torch.Tensor):
    return torch.nn.functional.gelu(x, approximate="tanh")
```

在一个大规模张量上进行基准测试, 结果惊人:

- **manual_gelu (非融合)**: 8.1 ms
- **pytorch_gelu (融合)**: 1.1 ms

融合版本的速度是朴素实现的**近8倍**. 查看Profiler输出会发现, `manual_gelu`触发了多个独立的CUDA核函数 (乘法, 加法, tanh等), 而`pytorch_gelu`只调用了一个`gelu_kernel`. 这有力地证明了减少全局内存访问次数带来的巨大性能提升.

### 4. 编写自定义核函数: 掌握极致性能

既然核函数融合如此强大, 当PyTorch没有提供我们需要的融合算子时, 我们能否自己编写呢? 答案是肯定的. 主要有三种途径: **CUDA C++**, **Triton**, 以及利用torch.compile`**.

#### 4.1 传统方式: CUDA C++

CUDA是NVIDIA推出的并行计算平台和编程模型, 其核心是使用C++的扩展来编写可以直接在GPU上执行的核函数. 这是一个功能强大但相对底层和复杂的方式.
[代码解析: 手写CUDA C++ GeLU核函数](./Lecture6-Code-CUDA-GeLU.md)

编写CUDA核函数让我们能够对硬件进行精细控制, 但也要求开发者手动处理内存管理, 线程索引计算等诸多细节.

#### 4.2 现代方式: Triton

Triton是OpenAI开发的一种基于Python的领域特定语言 (DSL), 旨在让编写高性能GPU核函数变得更加容易. 它在易用性和性能之间取得了绝佳的平衡.

Triton的核心思想是让开发者从**线程块 (block)** 的层面思考问题, 而不是单个线程. 你只需要描述一个线程块需要加载哪些数据以及如何处理它们, Triton编译器会自动将这些描述转换成高效的CUDA代码, 并处理好内存合并, 线程调度等复杂底层优化.
[代码解析: 使用Triton编写GeLU核函数](./Lecture6-Code-Triton-GeLU.md)

使用Triton编写的GeLU核函数, 性能可以达到与手写CUDA C++版本相近的水平 (约1.8 ms), 同时代码完全在Python环境中, 更加简洁易读.

#### 4.3 便捷方式: `torch.compile`

编写和调试自定义核函数仍然是一项耗时的工作. PyTorch 2.0引入的`torch.compile`提供了一种更为便捷的途径. 它是一个即时 (Just-In-Time) 编译器, 能够自动分析你的标准PyTorch代码, 并将其中的多个操作**自动融合**成一个或多个优化的核函数.

```python
# 编译我们之前写的非融合版本
compiled_gelu = torch.compile(manual_gelu)
```

令人惊讶的是, `torch.compile` 的性能非常出色. 在GeLU的例子中, 它甚至比我们手写的CUDA和Triton核函数还要快 (约1.47 ms), 非常接近PyTorch的原生实现. 查看其Profiler输出, 我们会发现`torch.compile`在底层自动生成了高效的Triton核函数.

**结论**: 在许多情况下, `torch.compile` 是实现性能优化的首选, 因为它在几乎不增加代码复杂度的同时提供了显著的加速. 只有在`torch.compile`无法满足需求, 或者需要实现高度定制化的复杂并行算法 (如Flash Attention) 时, 才需要求助于手写Triton或CUDA核函数.

### 5. 进阶核函数: 以Softmax为例处理归约操作

目前为止, 我们处理的GeLU是**逐元素 (element-wise)** 操作, 这是最简单的一类并行任务. 更具挑战性的是包含**归约 (reduction)** 的操作, 例如求和, 求最大值. **Softmax**就是这类操作的典型代表.

Softmax需要计算每一行的最大值和总和, 这要求线程之间进行协作. 一个高效的Triton实现思路是**将矩阵的每一行分配给一个独立的线程块**. 这样, 计算一行Softmax所需的所有数据和中间计算 (求最大值, 求和) 都可以被限制在单个SM的高速共享内存中完成, 从而避免了昂贵的全局内存读写.
[代码解析: 使用Triton实现高性能Softmax](./Lecture6-Code-Triton-Softmax.md)

通过这种精心设计, 我们可以为Softmax编写出一个性能远超朴素PyTorch实现的Triton核函数, 再次证明了通过优化内存访问模式来提升性能的威力.

### 6. 总结与技术选型指南

经过本次课程的探索, 我们掌握了从高到低不同层次的GPU性能优化方法. 那么在实际项目中, 我们应该如何选择呢? 以下是一个简单的决策指南:

| 场景与需求 | 优先选择 | 理由 |
| :--- | :--- | :--- |
| **通用性能提升**: 你的代码主要由标准的PyTorch模块和函数构成, 你希望获得一个快速且无需重构的性能提升. | `torch.compile` | **易用性最高, 侵入性最小**. 它是PyTorch生态的未来, 对常见的算子融合模式有很好的自动优化效果. **永远应该作为你的第一尝试**. |
| **自定义算子/复杂融合**: 你需要实现一个PyTorch没有的, 独特的算子, 或者你需要将一个非常复杂的, 包含归约, 条件逻辑的操作序列融合成单个核函数. | **Triton** | **在易用性和性能之间达到完美平衡**. 你可以在Python中实现复杂的并行逻辑, 同时获得接近底层CUDA的性能和自动内存优化. |
| **极致性能/库集成**: 你正在追求硬件的最后一丝性能, 需要手动控制共享内存, 线程同步, 或者需要将你的核函数与一个庞大的C++代码库深度集成. | **CUDA C++** | **提供最强的控制力**. 它是性能的上限, 但开发效率最低, 调试也最困难. 只有在确认Triton无法满足需求时才考虑. |
