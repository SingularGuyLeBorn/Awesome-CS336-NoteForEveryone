### 核心概念: 浮点数精度格式 (FP32, FP16, BF16, FP8)
在深度学习中，浮点数是表示模型权重、梯度、激活值等几乎所有连续值的基础。不同的浮点数格式在内存占用、数值范围和计算速度之间做出了不同的权衡。选择合适的格式对模型训练的效率和稳定性至关重要。
一个浮点数通常由三部分组成：符号位 (Sign)、指数位 (Exponent) 和尾数/小数位 (Fraction/Mantissa)。
- **指数位 (Exponent)**: 决定了数值可以表示的**动态范围** (Dynamic Range)，即能表示的最大和最小的数。
- **尾数位 (Fraction)**: 决定了数值的**精度** (Precision)，即数值的密集程度。
---
#### 1. FP32 (float32 / 单精度)
- **结构**: 1位符号，8位指数，23位尾数（共32位）。
- **内存占用**: 4字节。
- **特点**:
  - **黄金标准**: 在传统科学计算和早期深度学习中，FP32是标准格式，提供了良好的动态范围和高精度。
  - **稳定可靠**: 由于其高精度和宽范围，使用FP32进行训练通常非常稳定，不易出现数值问题。
  - **资源消耗大**: 占用内存和显存较多，且在现代GPU上，其计算速度远低于低精度格式。在H100上，FP32的算力仅为FP16/BF16的几十分之一。
#### 2. FP16 (float16 / 半精度)
- **结构**: 1位符号，5位指数，10位尾数（共16位）。
- **内存占用**: 2字节。
- **特点**:
  - **内存减半**: 相比FP32，内存占用和传输开销减半。
  - **速度提升**: 在支持FP16计算的硬件（如NVIDIA Tensor Cores）上速度更快。
  - **动态范围受限**: 最大的问题是其指数位太少（只有5位），导致动态范围非常有限。在大型模型训练中，梯度值可能变得非常小，超出FP16的表示范围而变为0（下溢），或者变得非常大而导致上溢。这会严重影响训练的稳定性。因此，纯FP16训练已不常用。
#### 3. BF16 (bfloat16 / Brain Floating Point)
- **结构**: 1位符号，8位指数，7位尾数（共16位）。
- **特点**:
  - **专为深度学习设计**: 由Google Brain团队开发，旨在解决FP16的动态范围问题。
  - **FP32的动态范围**: 它拥有和FP32相同的8位指数，因此其动态范围与FP32完全一致，有效避免了上溢和下溢问题。
  - **牺牲精度**: 为了保留指数位，它将尾数位缩减到了7位，这意味着其精度低于FP16。实践证明，深度学习对动态范围的敏感度远高于对精度的敏感度，因此这种权衡非常成功。
  - **当前主流**: BF16已成为现代大模型训练（尤其是在TPU和新一代NVIDIA GPU上）的首选格式，因为它在保持训练稳定性的同时，提供了接近FP16的内存和速度优势。
#### 4. FP8 (8位浮点数)
- **结构**: 1位符号，通常有4或5位指数（共8位）。
- **内存占用**: 1字节。
- **特点**:
  - **极致的速度和效率**: 在最新硬件（如NVIDIA H100）上提供极高的理论算力。
  - **极低的精度和范围**: 只有8位可用，无论是动态范围还是精度都受到了极大限制。
  - **两种变体**: 通常提供两种模式，一种分配更多位给指数（E5M2，侧重动态范围），另一种分配更多位给尾数（E4M3，侧重精度），以便根据不同计算任务的需求进行切换。
  - **应用场景**: 主要用于**[混合精度训练](./Lecture2-混合精度训练-Mixed-Precision-Training.md)**中对数值不敏感的部分（如某些矩阵乘法），或在模型推理时进行量化。直接用FP8进行端到端的训练仍然是一个活跃的研究领域，需要复杂的数值稳定技术。
#### 总结对比
| 格式  | 总位数 | 指数位数 | 尾数位数 | 内存(Bytes) | 主要优势                         | 主要劣势                         |
|-------|--------|----------|----------|-------------|------------------------------------|----------------------------------|
| FP32  | 32     | 8        | 23       | 4           | 高精度，宽动态范围，稳定         | 内存占用大，计算速度慢           |
| FP16  | 16     | 5        | 10       | 2           | 内存小，速度快，精度较高         | 动态范围极小，易溢出             |
| **BF16**  | **16**     | **8**        | **7**        | **2**           | **与FP32相同的动态范围，内存小** | **精度相对较低**                 |
| FP8   | 8      | 4或5     | 3或2     | 1           | 极致的内存效率和计算速度         | 精度和动态范围都非常有限         |
