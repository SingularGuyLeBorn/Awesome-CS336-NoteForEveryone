### 核心概念: 餐巾纸数学 (Napkin Math)
“餐巾纸数学”是指在设计或评估大规模深度学习项目时，进行快速、简化的“信封背面计算”（Back-of-the-envelope calculation）。其目的不是得到精确无比的数字，而是在几分钟内估算出项目的核心资源需求（如时间、成本、内存），从而快速判断其可行性、发现潜在瓶颈，并指导架构设计。
在本次讲座中，Napkin Math被用来回答两个关键问题：
#### 1. 估算模型训练时长
**问题**: 在1024张H100 GPU上，用15万亿 (15T) tokens训练一个700亿 (70B) 参数的稠密Transformer模型需要多久？
**估算步骤**:
1.  **计算总Flops (浮点运算次数)**:
    - 运用**[训练成本黄金法则](./Lecture2-训练成本黄金法则-6-参数量-Token数.md)**：`总Flops ≈ 6 * 模型参数量 * 训练Token数`
    - `总Flops ≈ 6 * (70 * 10^9) * (15 * 10^12) = 6.3 * 10^24` Flops
2.  **计算硬件提供的总算力 (Flops per Day)**:
    - **单卡算力**: 一张H100在BF16/FP16下的理论算力约为 1000 TFlops/s (即 `10^15` Flops/s)。
    - **考虑MFU**: 实际中不可能达到100%效率，假设**[模型浮点运算利用率(MFU)](./Lecture2-模型浮点运算利用率-MFU-Model-Flops-Utilization.md)**为50% (0.5)，则单卡有效算力为 `0.5 * 10^15` Flops/s。
    - **集群总算力**: `集群算力 = 1024 (卡) * 0.5 * 10^15 (Flops/s/卡) * 86400 (秒/天) ≈ 4.42 * 10^22` Flops/天。
3.  **计算所需天数**:
    - `训练天数 = 总Flops / 集群总算力`
    - `训练天数 ≈ (6.3 * 10^24) / (4.42 * 10^22) ≈ 142.5` 天。
**结论**: 大约需要143天。这个快速计算为项目规划者提供了关于时间周期的关键信息。
#### 2. 估算最大可训练模型大小
**问题**: 在8张H100 GPU上，使用AdamW优化器，不考虑花哨的优化技巧，能训练的最大模型是多大？
**估算步骤**:
1.  **计算总可用内存**:
    - 一张H100的显存 (HBM) 为80GB。
    - `总内存 = 8 * 80 GB = 640 GB`
2.  **计算每个参数所需的字节数 (Bytes per Parameter)**:
    - 这是**[训练资源全面解析](./Lecture2-训练资源全面解析-Memory-Compute-Accounting.md)**中的关键部分。对于使用**Adam**或**AdamW**的混合精度训练：
      - **模型参数 (FP16/BF16)**: 2 Bytes
      - **梯度 (FP16/BF16)**: 2 Bytes
      - **优化器状态 (Adam)**:
        - 一阶矩 (Momentum, FP32): 4 Bytes
        - 二阶矩 (Variance, FP32): 4 Bytes
    - **保守估算**: 为了稳定性，通常参数本身和优化器状态都用FP32存储。
      - **模型参数 (FP32)**: 4 Bytes
      - **梯度 (FP32)**: 4 Bytes
      - **优化器状态 (FP32)**: 8 Bytes (4+4)
      - `总计/参数 = 4 + 4 + 8 = 16` Bytes。
3.  **计算最大参数量**:
    - `最大参数量 = 总可用内存 / 每个参数所需的字节数`
    - `最大参数量 ≈ (640 * 10^9) / 16 ≈ 40 * 10^9`，即400亿 (40B) 参数。
**结论**: 理论上可以训练约40B参数的模型。这个计算忽略了激活值占用的内存，因此实际值会更小，但这提供了一个很好的上限参考。
通过这两个例子，"餐巾纸数学"展现了其在系统设计初期的巨大价值，它将复杂的工程问题简化为几个关键参数的乘除法，帮助我们快速把握项目的规模和可行性。
