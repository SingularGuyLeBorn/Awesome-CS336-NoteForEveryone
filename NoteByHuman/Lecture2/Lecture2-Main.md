### CS336: PyTorch手把手搭建大语言模型
#### 摘要
本次讲座深入探讨了如何使用 **PyTorch** 从零开始构建一个**语言模型**，重点关注构建过程中的两大核心要素：**效率**和**资源核算**。讲座通过一系列“[餐巾纸数学](./Lecture2-%E9%A4%90%E5%B7%BE%E7%BA%B8%E6%95%B0%E5%AD%A6-Napkin-Math.md)”问题开篇，引导大家思考如何估算模型训练所需的算力和时间。随后，课程自底向上地讲解了 **PyTorch** 的核心组件，从**张量**的内存占用，到计算操作的成本，最终构建出一个完整的训练流程，并对每一步的资源消耗进行了精确的量化分析。
***
#### 1. 核心动机：效率就是金钱
在**大语言模型**领域，效率直接等同于成本。课程开篇通过两个实际问题强调了资源核算的重要性：
1. **训练时长估算**：在1024张H100 **GPU**上，用15T的tokens训练一个70B参数的稠密 **Transformer** 模型需要多久？
2. **最大模型估算**：在8张H100 **GPU**上，能训练多大的模型？
这些问题的解答依赖于对模型计算和内存需求的精确估算，这正是本次课程的核心——[训练资源全面解析](./Lecture2-%E8%AE%AD%E7%BB%83%E8%B5%84%E6%BA%90%E5%85%A8%E9%9D%A2%E8%A7%A3%E6%9E%90-Memory-Compute-Accounting.md)。
#### 2. 内存核算 (Memory Accounting)
##### 2.1 [PyTorch张量详解](./Lecture2-PyTorch%E5%BC%A0%E9%87%8F%E8%AF%A6%E8%A7%A3-Tensors-Internals.md)
**张量**是深度学习中存储所有数据（参数、梯度、优化器状态、激活值）的基础单元。理解其内存占用至关重要。
- **内存计算**：一个张量的内存占用由其元素数量和数据类型共同决定。
  - `内存(Bytes) = 元素数量 * 每个元素的大小(Bytes)`
##### 2.2 [浮点数精度格式](./Lecture2-%E6%B5%AE%E7%82%B9%E6%95%B0%E7%B2%BE%E5%BA%A6%E6%A0%BC%E5%BC%8F-FP32-FP16-BF16-FP8.md)
不同的浮点数格式在精度、动态范围和内存占用之间做出了不同的权衡：
- **FP32 (32位浮点数)**：单精度，是传统的“黄金标准”，内存占用为4字节。
- **FP16 (16位浮点数)**：半精度，内存减半，但动态范围小，容易出现上溢或下溢问题，不适合大规模训练。
- **BF16 (Brain Float 16)**：拥有与FP32相同的动态范围和FP16的内存占用（2字节），但牺牲了部分精度。这是目前主流的训练格式。
- **FP8 (8位浮点数)**：在H100等新硬件上支持，内存占用仅1字节，速度极快，但精度最低，使用时需格外小心。
#### 3. 计算核算 (Compute Accounting)
##### 3.1 硬件与数据转移
默认情况下，**张量**创建在 **CPU** 上，必须显式地将其移动到 **GPU** 才能利用其强大的并行计算能力。这个过程涉及数据传输开销。
##### 3.2 核心操作：矩阵乘法 (MatMuls)
**矩阵乘法**是深度学习中计算成本最高的操作。
- **Flops计算**：对于一个形状为 `(B, D, K)` 的矩阵乘法 `(B, D) @ (D, K)`，其浮点运算次数 (Flops) 约为 `2 * B * D * K`。这里的 `2` 代表一次乘法和一次加法。
- [Einops](./Lecture2-Einops-%E7%88%B1%E5%9B%A0%E6%96%AF%E5%9D%A6%E6%B1%82%E5%92%8C%E7%BA%A6%E5%AE%9A.md)：为了避免在处理高维张量时出现维度索引混乱（如 `transpose(-2, -1)`），推荐使用 `einops` 库。它通过命名维度的方式，让张量操作（如**矩阵乘法**、重排、归约）变得极其清晰和不易出错。
##### 3.3 [模型浮点运算利用率 (MFU)](./Lecture2-%E6%A8%A1%E5%9E%8B%E6%B5%AE%E7%82%B9%E8%BF%90%E7%AE%97%E5%88%A9%E7%94%A8%E7%8E%87-MFU-Model-Flops-Utilization.md)
MFU是衡量硬件利用效率的关键指标。
- **定义**：`MFU = (模型实际达到的Flops/秒) / (硬件理论峰值Flops/秒)`
- **意义**：一个高的MFU（通常 > 50%）意味着你的代码有效地利用了 **GPU** 的计算潜力。如果MFU过低，说明计算被内存访问、数据转移或其他开销所拖累，需要进行性能优化。
#### 4. 完整的训练成本分析
##### 4.1 [训练成本黄金法则 (6 * 参数量 * Token数)](./Lecture2-%E8%AE%AD%E7%BB%83%E6%88%90%E6%9C%AC%E9%BB%84%E9%87%91%E6%B3%95%E5%88%99-6-%E5%8F%82%E6%95%B0%E9%87%8F-Token%E6%95%B0.md)
对于一个典型的类**Transformer**模型，一次完整的训练（前向传播 + 反向传播）所需的总计算量可以被精确地估算：
- **前向传播 (Forward Pass)**：大约需要 `2 * 参数量 (P) * 数据点数 (D)` 的Flops。
- **反向传播 (Backward Pass)**：计算**梯度**的成本大约是前向传播的2倍，即 `4 * P * D` 的Flops。
- **总计**：一次迭代的总Flops约为 `6 * P * D`。这个法则是在课程开头的**[餐巾纸数学](./Lecture2-%E9%A4%90%E5%B7%BE%E7%BA%B8%E6%95%B0%E5%AD%A6-Napkin-Math.md)**中使用的关键公式。
##### 4.2 [训练资源全面解析](./Lecture2-%E8%AE%AD%E7%BB%83%E8%B5%84%E6%BA%90%E5%85%A8%E9%9D%A2%E8%A7%A3%E6%9E%90-Memory-Compute-Accounting.md)
一个模型在训练时，总内存占用主要由四部分构成：
1. **模型参数 (Parameters)**：模型权重本身。
2. **梯度 (Gradients)**：每个参数对应的梯度，与参数大小相同。
3. **优化器状态 (Optimizer States)**：如 **Adam** 优化器需要为每个参数存储一阶矩（动量）和二阶矩，通常是参数量的2倍。
4. **激活值 (Activations)**：中间计算结果，其大小依赖于批次大小（Batch Size）和序列长度（Sequence Length）。
#### 5. 构建与训练实践
##### 5.1 模型构建与参数初始化
- **参数初始化**：为避免训练不稳定，参数初始化至关重要。一种常见的策略（如Xavier初始化）是将随机初始化的权重除以 `sqrt(输入维度)`，以保证输出的方差稳定。
##### 5.2 优化器
- 课程以 `Adagrad` 为例，展示了如何实现一个自定义**优化器**，并分析了其状态（如梯度平方和）所需的额外内存。**Adam** 和 **AdamW** 是目前更常用的**优化器**。
##### 5.3 [混合精度训练](./Lecture2-%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83-Mixed-Precision-Training.md)
这是一种平衡速度和稳定性的高级策略。
- **核心思想**：
  - 使用低精度（如 **BF16**）进行计算密集型的前向和反向传播，以充分利用硬件加速。
  - 使用高精度（**FP32**）存储模型参数、**梯度**累积和**优化器**状态，以维持数值稳定性。
- **PyTorch** 提供了 `torch.autocast` 等工具来简化**混合精度训练**的实现。
***
**结论**：本次课程通过对一个简单线性模型的庖丁解牛，系统地揭示了训练**大语言模型**时进行资源核算的完整方法论。掌握这些内存和计算的估算技巧，是实现高效、经济的模型训练的基础。在后续的作业中，这些概念将被应用于更复杂的 **Transformer** 模型。
--- END OF FILE ---
--- FILE: Lecture2-餐巾纸数学-Napkin-Math.md ---
### 核心概念: 餐巾纸数学 (Napkin Math)
“餐巾纸数学”是指在设计或评估大规模深度学习项目时，进行快速、简化的“信封背面计算”（Back-of-the-envelope calculation）。其目的不是得到精确无比的数字，而是在几分钟内估算出项目的核心资源需求（如时间、成本、内存），从而快速判断其可行性、发现潜在瓶颈，并指导架构设计。
在本次讲座中，Napkin Math被用来回答两个关键问题：
#### 1. 估算模型训练时长
**问题**: 在1024张H100 GPU上，用15万亿 (15T) tokens训练一个700亿 (70B) 参数的稠密Transformer模型需要多久？
**估算步骤**:
1. **计算总Flops (浮点运算次数)**:
   - 运用**[训练成本黄金法则](./Lecture2-%E8%AE%AD%E7%BB%83%E6%88%90%E6%9C%AC%E9%BB%84%E9%87%91%E6%B3%95%E5%88%99-6-%E5%8F%82%E6%95%B0%E9%87%8F-Token%E6%95%B0.md)**：`总Flops ≈ 6 * 模型参数量 * 训练Token数`
   - `总Flops ≈ 6 * (70 * 10^9) * (15 * 10^12) = 6.3 * 10^24` Flops
2. **计算硬件提供的总算力 (Flops per Day)**:
   - **单卡算力**: 一张H100在BF16/FP16下的理论算力约为 1000 TFlops/s (即 `10^15` Flops/s)。
   - **考虑MFU**: 实际中不可能达到100%效率，假设**[模型浮点运算利用率(MFU)](./Lecture2-%E6%A8%A1%E5%9E%8B%E6%B5%AE%E7%82%B9%E8%BF%90%E7%AE%97%E5%88%A9%E7%94%A8%E7%8E%87-MFU-Model-Flops-Utilization.md)**为50% (0.5)，则单卡有效算力为 `0.5 * 10^15` Flops/s。
   - **集群总算力**: `集群算力 = 1024 (卡) * 0.5 * 10^15 (Flops/s/卡) * 86400 (秒/天) ≈ 4.42 * 10^22` Flops/天。
3. **计算所需天数**:
   - `训练天数 = 总Flops / 集群总算力`
   - `训练天数 ≈ (6.3 * 10^24) / (4.42 * 10^22) ≈ 142.5` 天。
**结论**: 大约需要143天。这个快速计算为项目规划者提供了关于时间周期的关键信息。
#### 2. 估算最大可训练模型大小
**问题**: 在8张H100 GPU上，使用AdamW优化器，不考虑花哨的优化技巧，能训练的最大模型是多大？
**估算步骤**:
1. **计算总可用内存**:
   - 一张H100的显存 (HBM) 为80GB。
   - `总内存 = 8 * 80 GB = 640 GB`
2. **计算每个参数所需的字节数 (Bytes per Parameter)**:
   - 这是**[训练资源全面解析](./Lecture2-%E8%AE%AD%E7%BB%83%E8%B5%84%E6%BA%90%E5%85%A8%E9%9D%A2%E8%A7%A3%E6%9E%90-Memory-Compute-Accounting.md)**中的关键部分。对于使用**Adam**或**AdamW**的混合精度训练：
     - **模型参数 (FP16/BF16)**: 2 Bytes
     - **梯度 (FP16/BF16)**: 2 Bytes
     - **优化器状态 (Adam)**:
       - 一阶矩 (Momentum, FP32): 4 Bytes
       - 二阶矩 (Variance, FP32): 4 Bytes
   - **保守估算**: 为了稳定性，通常参数本身和优化器状态都用FP32存储。
     - **模型参数 (FP32)**: 4 Bytes
     - **梯度 (FP32)**: 4 Bytes
     - **优化器状态 (FP32)**: 8 Bytes (4+4)
     - `总计/参数 = 4 + 4 + 8 = 16` Bytes。
3. **计算最大参数量**:
   - `最大参数量 = 总可用内存 / 每个参数所需的字节数`
   - `最大参数量 ≈ (640 * 10^9) / 16 ≈ 40 * 10^9`，即400亿 (40B) 参数。
**结论**: 理论上可以训练约40B参数的模型。这个计算忽略了激活值占用的内存，因此实际值会更小，但这提供了一个很好的上限参考。
通过这两个例子，"餐巾纸数学"展现了其在系统设计初期的巨大价值，它将复杂的工程问题简化为几个关键参数的乘除法，帮助我们快速把握项目的规模和可行性。
--- END OF FILE ---
--- FILE: Lecture2-浮点数精度格式-FP32-FP16-BF16-FP8.md ---
### 核心概念: 浮点数精度格式 (FP32, FP16, BF16, FP8)
在深度学习中，浮点数是表示模型权重、梯度、激活值等几乎所有连续值的基础。不同的浮点数格式在内存占用、数值范围和计算速度之间做出了不同的权衡。选择合适的格式对模型训练的效率和稳定性至关重要。
一个浮点数通常由三部分组成：符号位 (Sign)、指数位 (Exponent) 和尾数/小数位 (Fraction/Mantissa)。
- **指数位 (Exponent)**: 决定了数值可以表示的**动态范围** (Dynamic Range)，即能表示的最大和最小的数。
- **尾数位 (Fraction)**: 决定了数值的**精度** (Precision)，即数值的密集程度。
***
#### 1. FP32 (float32 / 单精度)
- **结构**: 1位符号，8位指数，23位尾数（共32位）。
- **内存占用**: 4字节。
- **特点**:
  - **黄金标准**: 在传统科学计算和早期深度学习中，FP32是标准格式，提供了良好的动态范围和高精度。
  - **稳定可靠**: 由于其高精度和宽范围，使用FP32进行训练通常非常稳定，不易出现数值问题。
  - **资源消耗大**: 占用内存和显存较多，且在现代GPU上，其计算速度远低于低精度格式。在H100上，FP32的算力仅为FP16/BF16的几十分之一。
#### 2. FP16 (float16 / 半精度)
- **结构**: 1位符号，5位指数，10位尾数（共16位）。
- **内存占用**: 2字节。
- **特点**:
  - **内存减半**: 相比FP32，内存占用和传输开销减半。
  - **速度提升**: 在支持FP16计算的硬件（如NVIDIA Tensor Cores）上速度更快。
  - **动态范围受限**: 最大的问题是其指数位太少（只有5位），导致动态范围非常有限。在大型模型训练中，梯度值可能变得非常小，超出FP16的表示范围而变为0（下溢），或者变得非常大而导致上溢。这会严重影响训练的稳定性。因此，纯FP16训练已不常用。
#### 3. BF16 (bfloat16 / Brain Floating Point)
- **结构**: 1位符号，8位指数，7位尾数（共16位）。
- **特点**:
  - **专为深度学习设计**: 由Google Brain团队开发，旨在解决FP16的动态范围问题。
  - **FP32的动态范围**: 它拥有和FP32相同的8位指数，因此其动态范围与FP32完全一致，有效避免了上溢和下溢问题。
  - **牺牲精度**: 为了保留指数位，它将尾数位缩减到了7位，这意味着其精度低于FP16。实践证明，深度学习对动态范围的敏感度远高于对精度的敏感度，因此这种权衡非常成功。
  - **当前主流**: BF16已成为现代大模型训练（尤其是在TPU和新一代NVIDIA GPU上）的首选格式，因为它在保持训练稳定性的同时，提供了接近FP16的内存和速度优势。
#### 4. FP8 (8位浮点数)
- **结构**: 1位符号，通常有4或5位指数（共8位）。
- **内存占用**: 1字节。
- **特点**:
  - **极致的速度和效率**: 在最新硬件（如NVIDIA H100）上提供极高的理论算力。
  - **极低的精度和范围**: 只有8位可用，无论是动态范围还是精度都受到了极大限制。
  - **两种变体**: 通常提供两种模式，一种分配更多位给指数（E5M2，侧重动态范围），另一种分配更多位给尾数（E4M3，侧重精度），以便根据不同计算任务的需求进行切换。
  - **应用场景**: 主要用于**[混合精度训练](./Lecture2-%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83-Mixed-Precision-Training.md)**中对数值不敏感的部分（如某些矩阵乘法），或在模型推理时进行量化。直接用FP8进行端到端的训练仍然是一个活跃的研究领域，需要复杂的数值稳定技术。
#### 总结对比
| 格式  | 总位数 | 指数位数 | 尾数位数 | 内存(Bytes) | 主要优势                         | 主要劣势                         |
|-------|--------|----------|----------|-------------|------------------------------------|----------------------------------|
| FP32  | 32     | 8        | 23       | 4           | 高精度，宽动态范围，稳定         | 内存占用大，计算速度慢           |
| FP16  | 16     | 5        | 10       | 2           | 内存小，速度快，精度较高         | 动态范围极小，易溢出             |
| **BF16**  | **16**     | **8**        | **7**        | **2**           | **与FP32相同的动态范围，内存小** | **精度相对较低**                 |
| FP8   | 8      | 4或5     | 3或2     | 1           | 极致的内存效率和计算速度         | 精度和动态范围都非常有限         |
--- END OF FILE ---
--- FILE: Lecture2-模型浮点运算利用率-MFU-Model-Flops-Utilization.md ---
### 核心概念: 模型浮点运算利用率 (MFU, Model Flops Utilization)
模型浮点运算利用率（MFU）是一个关键的性能指标，用于衡量在深度学习训练中，你的代码实际利用硬件计算能力的效率。它回答了一个核心问题：“我的模型训练代码，让GPU的计算单元忙起来的程度有多高？”
#### 定义与计算
MFU 的计算公式非常直观：
**MFU = (实际每秒浮点运算次数 (Actual Flops/s)) / (硬件理论峰值每秒浮点运算次数 (Promised Flops/s))**
其中：
1. **实际每秒浮点运算次数 (Actual Flops/s)**:
   - 这是通过你的模型计算得出的。首先，你需要根据模型的结构（主要是矩阵乘法）计算出执行一次前向和反向传播所需要的理论浮点运算总数（Flops）。
   - 然后，你测量执行这次计算实际花费的**墙上时间 (Wall Clock Time)**。
   - `Actual Flops/s = (一次迭代的总Flops) / (一次迭代的耗时)`。
   - **重点**: 这里的Flops是基于**模型**的，而不是硬件实际执行的底层操作数。它衡量的是对模型有用的计算量。这使得MFU成为一个标准化的、可跨不同实现进行比较的指标。
2. **硬件理论峰值每秒浮点运算次数 (Promised Flops/s)**:
   - 这个数值来自硬件制造商（如NVIDIA）提供的规格表。
   - 它是一个理论上限，通常取决于所使用的**数据类型**（如FP32, BF16, FP8）和是否利用了特定的计算单元（如Tensor Cores）。
   - **注意**: 在查阅规格时，要分清是否包含了“稀疏性 (Sparsity)”带来的加成。对于稠密矩阵运算，应使用不含稀疏性加成的理论值（通常是宣传值的一半）。
#### 示例
假设我们正在H100 GPU上运行一个矩阵乘法：
- **模型计算**: 我们的矩阵乘法需要 `5.4 * 10^14` Flops。
- **实际耗时**: 测量得到运行时间为 0.1秒。
- **实际算力**: `Actual Flops/s = (5.4 * 10^14) / 0.1s = 5.4 * 10^15` Flops/s，即 540 TFlops/s。
- **硬件理论峰值**: 查阅H100规格表，假设在当前数据类型下，其理论峰值为 670 TFlops/s。
- **计算MFU**: `MFU = 540 / 670 ≈ 0.80`，即 80%。
#### 为什么MFU很重要？
- **性能瓶颈的指示器**: 一个低的MFU（例如，低于30%）强烈暗示你的训练流程存在瓶颈。计算单元大部分时间在“空闲等待”，而不是在执行有用的计算。
- **常见瓶颈来源**:
  - **内存带宽限制 (Memory-bound)**: 数据在GPU内存（HBM）和计算单元之间的传输速度跟不上计算速度。这在小型矩阵运算中尤为常见。
  - **数据加载/预处理**: CPU端的数据加载和预处理成为瓶颈，GPU在等待数据输入。
  - **通信开销**: 在多GPU或多节点训练中，节点间的通信延迟过高。
  - **次优的核函数 (Kernel)**: PyTorch操作没有被编译成高效的GPU指令。
- **优化目标**:
  - 在大模型训练中，目标通常是让计算成为瓶颈 (Compute-bound)，因为这意味着你正在最大化地利用昂贵的计算资源。
  - **一个好的MFU通常被认为在 50% 以上**。达到90%或更高是非常困难的，因为它意味着几乎所有的开销（内存访问、内核启动等）都被隐藏在了计算之后。
总之，MFU是诊断和优化大模型训练性能的“听诊器”。定期监控MFU可以帮助你了解训练效率，并指导你将优化精力投入到最关键的地方。
--- END OF FILE ---
--- FILE: Lecture2-PyTorch张量详解-Tensors-Internals.md ---
### 核心概念: PyTorch张量详解 (视图, 存储与Einops)
在PyTorch中，`torch.Tensor` 不仅仅是一个多维数组，它的内部实现机制对于理解内存使用和代码效率至关重要。本次讲座重点介绍了张量的存储、视图（Views）以及如何使用 `einops` 进行更清晰的操作。
#### 1. 张量的内部结构：存储 (Storage) 与元数据 (Metadata)
一个PyTorch张量实际上是一个“描述符”，它并不直接拥有数据。它包含两部分：
- **存储 (Storage)**: 一个连续的一维内存块，实际存放着张量的数据。
- **元数据 (Metadata)**: 描述如何从这个一维存储中“解读”出多维张量的信息，主要包括：
  - **大小 (Size)**: 张量在每个维度上的形状，例如 `(2, 3)`。
  - **步长 (Stride)**: 在每个维度上，移动一个索引需要在一维存储中跳过多少个元素。
  - **存储偏移 (Storage Offset)**: 张量的第一个元素对应于一维存储中的哪个位置。
**示例**: 一个 `(4, 4)` 的矩阵
- **存储**: 一个包含16个元素的连续数组。
- **大小**: `(4, 4)`
- **步长**:
  - `stride[0]` (移动到下一行): 需要跳过4个元素，所以是 `4`。
  - `stride[1]` (移动到下一列): 需要跳过1个元素，所以是 `1`。
  - 因此，步长是 `(4, 1)`。
通过 `(行索引 * stride[0] + 列索引 * stride[1])` 就可以精确定位到任何元素在一维存储中的位置。
#### 2. 视图 (Views) vs. 副本 (Copies)
这是PyTorch中一个极其重要的概念，直接关系到内存效率和潜在的编程错误。
- **视图 (View)**: 许多操作**不会创建新的内存**。它们只是创建了一个新的张量对象（新的元数据），但与原始张量共享同一个底层**存储**。
  - **优点**: 几乎零成本，非常高效，因为避免了数据的复制。
  - **风险**: 修改视图会**同时改变**原始张量的数据，因为它们指向同一块内存。
  - **常见创建视图的操作**: `x[0, :]`, `x.transpose()`, `x.view()`, `torch.split()`。
- **副本 (Copy)**: 某些操作会分配一块全新的内存，并将数据复制过去。新张量与原始张量完全独立。
  - **常见创建副本的操作**: `x.clone()`, `x.contiguous()` (在非连续张量上调用时), `x + y` 等算术运算。
**连续性 (Contiguity)**:
- 如果一个张量在内存中的布局顺序与其维度的遍历顺序一致，则称其为“连续的”(Contiguous)。例如，一个标准的行主序矩阵。
- 对一个连续张量进行 `transpose()` 操作后，它就变成了“非连续的”，因为遍历新张量的一行需要在底层存储中进行跳跃。
- 对非连续张量执行某些操作（如 `view()`）会失败。此时，需要先调用 `.contiguous()` 创建一个内存连续的副本，然后才能继续操作。
#### 3. Einops: 更优雅的张量操作
**动机**: 传统的PyTorch代码在处理高维张量时，常常依赖于 `transpose`, `permute`, `view`, `reshape` 等操作，并使用数字索引（如 `-1`, `-2`）来指定维度。这使得代码难以阅读、理解和维护，并且极易出错。
**Einops (爱因斯坦求和约定风格)**:
`einops` 是一个第三方库，它通过一种声明式的、基于字符串的迷你语言，极大地简化了复杂的张量操作。
**核心函数**:
- **`rearrange`** **rearrange**: 用于重排和组合维度（等价于 `transpose`, `permute`, `view`, `reshape` 的组合）。
  ```python
  
  # 将 (batch, channels, height, width) 转换为 (batch, height, width, channels)
  
  rearrange(images, 'b c h w -> b h w c')
  
  
  
  # 将带有头部的维度拆分
  
  # (batch, seq, 8*64) -> (batch, seq, 8, 64)
  
  rearrange(x, 'b s (h d) -> b s h d', h=8)
  
  ```
- **`reduce`** **reduce**: 用于对指定维度进行归约（求和、求平均等）。
  ```python
  
  # 对高度和宽度维度求和
  
  # (batch, height, width, channels) -> (batch, channels)
  
  reduce(images, 'b h w c -> b c', 'sum')
  
  ```
- **`einsum`** **einsum**: PyTorch内置了 `torch.einsum`，`einops` 对其进行了封装和扩展，提供了更强大的功能。它通过直观的字符串来描述复杂的**矩阵乘法**和张量缩并。
  ```python
  
  # 两个序列的点积注意力
  
  # x: (batch, seq1, dim), y: (batch, seq2, dim) -> attn: (batch, seq1, seq2)
  
  einsum(x, y, 'b s1 d, b s2 d -> b s1 s2')
  
  ```
**使用Einops的优势**:
1. **可读性**: 代码即文档，`'b h w c -> b c'` 的意图远比一连串的 `permute` 和 `mean` 清晰。
2. **安全性**: 减少了因维度索引错误导致的bug。
3. **灵活性**: 可以轻松地组合和分解维度，代码更简洁。
在构建复杂的模型（如Transformer）时，投资时间学习并使用 `einops` 会极大地提高开发效率和代码质量。
--- END OF FILE ---
--- FILE: Lecture2-训练成本黄金法则-6-参数量-Token数.md ---
### 核心概念: 训练成本黄金法则 (6 * 参数量 * Token数)
这是一个在大语言模型领域广泛使用的经验法则，用于快速估算训练一个稠密 Transformer 模型所需的总计算量（以Flops为单位）。它构成了**[餐巾纸数学](./Lecture2-%E9%A4%90%E5%B7%BE%E7%BA%B8%E6%95%B0%E5%AD%A6-Napkin-Math.md)**的基石。
#### 法则公式
**总计算量 (Total Flops) ≈ 6 × 模型参数量 (P) × 训练Token总数 (D)**
- **P (Parameters)**: 模型中可训练参数的总数量。例如，对于一个70B模型，P ≈ 70 * 10^9。
- **D (Data/Tokens)**: 训练数据集包含的Token总数。例如，对于15T的Token，D ≈ 15 * 10^12。
#### 法则的推导与解释
这个法则中的系数 “6” 可以分解为两部分，分别对应训练过程中的前向传播和反向传播。
**1. 前向传播 (Forward Pass): 2 * P * D**
- 在一个典型的稠密Transformer模型中，绝大部分的计算量来自于**矩阵乘法**（主要在FFN层和Attention层）。
- 对于一个输入Token，当它流经模型时，可以近似认为它与模型中的每个参数都进行了一次乘法和一次加法运算。
- 一个乘法和一个加法通常被计为 **2** 次浮点运算 (Flops)。
- 因此，对于一个Token，前向传播的计算量大约是 `2 * P`。
- 将这个计算应用到整个数据集的所有Token上，总的前向传播计算量就是 `2 * P * D`。
**2. 反向传播 (Backward Pass): 4 * P * D**
- 反向传播的目的是计算损失函数关于模型每个参数的梯度。
- 根据链式法则，计算梯度的过程通常比前向传播需要更多的计算。
- 一个被广泛接受和在实践中验证过的经验是，**反向传播的计算量大约是前向传播的2倍**。
- 因此，总的反向传播计算量大约是 `2 * (前向传播Flops) = 2 * (2 * P * D) = 4 * P * D`。
**合并两者:**
- 一次完整的训练迭代（一次前向 + 一次反向）所需的总计算量为：
  `总Flops = 前向Flops + 反向Flops`
  `总Flops ≈ (2 * P * D) + (4 * P * D) = 6 * P * D`
#### 重要说明与注意事项
- **这是一个近似值**: 这个法则是一个高度简化的模型。实际的计算量会因模型架构的具体细节（如激活函数、LayerNorm、Softmax等）而略有偏差，但对于大模型而言，矩阵乘法的成本占主导地位，因此 `6 * P * D` 是一个非常可靠的估算。
- **不包括优化器更新**: 该法则计算的是前向和反向传播的Flops，通常不包括优化器更新参数的计算量。对于Adam等优化器，这部分计算量相对较小，可以忽略不计。
- **适用于稠密模型**: 这个法则对标准的、稠密的Transformer架构最为准确。对于使用了稀疏性、专家混合（MoE）或其他特殊结构的模型，需要对公式进行相应的调整。
- **应用**: 这个法则是进行项目规划、成本预算和资源申请的核心依据。通过它，你可以将一个抽象的“训练模型”任务，转化为具体可量化的计算资源需求。
--- END OF FILE ---
--- FILE: Lecture2-混合精度训练-Mixed-Precision-Training.md ---
### 核心概念: 混合精度训练 (Mixed Precision Training)
混合精度训练是一种在深度学习中显著提升训练速度、减少内存占用的关键技术，同时尽可能地保持与标准单精度（FP32）训练相当的准确性和稳定性。其核心思想是在训练过程的不同部分“混合”使用不同的数值精度格式。
#### 动机与权衡
- **高精度 (FP32)**:
  - **优点**: 数值稳定，动态范围宽，不易出现上溢或下溢。
  - **缺点**: 内存占用大（4字节/参数），计算速度慢（在现代GPU上尤其如此）。
- **低精度 (BF16 / FP16)**:
  - **优点**: 内存占用小（2字节/参数），在支持Tensor Cores的GPU上计算速度极快（通常是FP32的数倍）。
  - **缺点**: 数值范围和/或精度有限，直接用于所有计算和存储容易导致训练不稳定。
混合精度训练的目标是**两全其美**：利用低精度的速度优势，同时借助高精度的稳定性。
#### 核心策略
混合精度训练的通用策略如下：
1. **权重主副本 (Master Copy of Weights)**:
   - 在内存中始终维护一份**FP32**格式的模型参数（权重）。这是模型状态的“黄金副本”，用于确保在多次迭代后参数更新的精度不会丢失。
2. **前向/反向传播计算 (Forward/Backward Pass)**:
   - 在每次训练迭代开始时，将FP32的权重主副本**转换**为低精度格式（通常是**BF16**）。
   - 使用这个低精度的模型版本执行计算密集型的前向传播和反向传播。所有的矩阵乘法和卷积都在硬件加速的低精度下进行，从而获得巨大的速度提升。
   - 在这个过程中计算出的梯度也是低精度的（BF16）。
3. **梯度累积与参数更新 (Gradient Accumulation & Weight Update)**:
   - 将低精度的梯度**转换回**FP32格式。
   - 使用FP32的梯度来更新FP32的权重主副本。这一步至关重要，因为梯度的值通常很小，如果在低精度下反复累加，容易因舍入误差而丢失信息，导致模型无法有效学习。
4. **损失缩放 (Loss Scaling - 主要针对FP16)**:
   - 这是一个主要用于解决 **FP16** 动态范围不足问题的技巧（对于动态范围与FP32相同的 **BF16**，此步骤通常不是必需的）。
   - 在反向传播开始前，将计算出的损失值（Loss）乘以一个大的缩放因子（例如，1024）。
   - 根据链式法则，这个缩放会传递到所有的梯度上，将原本可能因过小而下溢到0的梯度值“放大”到FP16可表示的范围内。
   - 在参数更新前，将梯度再除以相同的缩放因子，将其恢复到原始大小。
#### PyTorch中的实现
PyTorch通过 `torch.cuda.amp` (Automatic Mixed Precision) 模块极大地简化了混合精度训练的实现。
```python
from torch.cuda.amp import autocast, GradScaler
# 1. 初始化GradScaler (用于损失缩放)
scaler = GradScaler()
# --- 在训练循环中 ---
optimizer.zero_grad()
# 2. 使用 autocast 上下文管理器
# 在此代码块内，符合条件的PyTorch操作会自动以低精度(如BF16)执行
with autocast(dtype=torch.bfloat16):
    outputs = model(inputs)
    loss = criterion(outputs, labels)
# 3. 使用scaler来缩放损失并进行反向传播
scaler.scale(loss).backward()
# 4. 使用scaler来反缩放梯度并更新权重
scaler.step(optimizer)
# 5. 更新缩放因子
scaler.update()
```
通过这套机制，开发者无需手动管理不同精度的转换，就可以轻松地在训练中集成混合精度，从而获得显著的性能提升。
--- END OF FILE ---
--- FILE: Lecture2-训练资源全面解析-Memory-Compute-Accounting.md ---
### 核心概念: 训练资源全面解析 (Memory & Compute Accounting)
在进行大语言模型训练时，精确地核算内存（Memory）和计算（Compute）资源是规划项目、优化性能和控制成本的基础。本次讲座提供了一个完整的框架来分析一个典型训练流程中的各项资源消耗。
***
### 一、 内存占用 (Memory Footprint)
模型在训练期间的总内存占用主要由以下四个部分构成。假设模型有 `P` 个参数，批次大小为 `B`，序列长度为 `S`，隐藏层维度为 `H`。
**1. 模型参数 (Model Parameters)**
- **内容**: 模型自身的权重（例如 `nn.Linear` 的 `weight` 和 `bias`）。
- **大小**: `P` 个参数。如果使用FP32，则占用 `4 * P` 字节；如果使用BF16/FP16，则占用 `2 * P` 字节。
- **特点**: 在整个训练过程中是持久存在的。
**2. 梯度 (Gradients)**
- **内容**: 损失函数对每个模型参数的导数。
- **大小**: 每个参数都需要一个对应的梯度，因此总大小与参数量相同。通常与参数精度一致，即 `4 * P` 或 `2 * P` 字节。
- **特点**: 在每次 `.backward()` 调用后生成，在 `.step()` 更新参数后可以被释放（`optimizer.zero_grad()`）。
**3. 优化器状态 (Optimizer States)**
- **内容**: 优化器为维持学习状态而存储的额外信息。
- **大小**: 取决于所用的优化器。
  - **SGD with Momentum**: 需要存储每个参数的动量（momentum），额外占用 `1 * P` 个参数的内存（通常是FP32，即 `4 * P` 字节）。
  - **Adam / AdamW**: 需要存储一阶矩（动量）和二阶矩（方差），额外占用 `2 * P` 个参数的内存（通常是FP32，即 `8 * P` 字节）。
- **特点**: 与模型参数一样，是持久存在的。这是内存消耗的一个大头。
**4. 激活值 (Activations)**
- **内容**: 模型在前向传播过程中产生的中间结果。为了在反向传播时计算梯度，这些结果必须被存储下来。
- **大小**: 这是最复杂的一部分，它不依赖于模型参数量 `P`，而是依赖于**批次大小** **`B`** **B**、**序列长度** **`S`** **S** 和**模型结构**（隐藏层维度 `H`、层数 `L` 等）。
- **估算**: 对于一个Transformer模型，其大小约等于 `B * S * H * L * (一些常数)`。
- **特点**: 激活值的内存占用非常大，并且是导致训练因“显存不足（Out of Memory）”而失败的常见原因。它也是唯一一个可以通过减小批次大小或序列长度来直接控制的内存部分。像“激活重计算/梯度检查点”（Activation Checkpointing）这样的技术就是专门用来优化这部分内存的。
**总内存估算 (以AdamW和混合精度为例):**
`总内存 ≈ 参数(4*P) + 梯度(2*P) + 优化器(8*P) + 激活值(...)`
- 仅考虑与参数相关的部分，每个参数大约需要 `(4+2+8) = 14` 字节，这还没算激活值。讲座中给出的 `16` 字节是一个更保守和常用的估算。
***
### 二、 计算成本 (Compute Cost - Flops)
计算成本主要衡量执行一次完整训练迭代所需的浮点运算次数 (Flops)。
**1. 前向传播 (Forward Pass)**
- **核心**: 主要由模型中的**矩阵乘法**主导。
- **估算**: 对于一个稠密模型，可以近似为 `2 * P * D`，其中 `P` 是参数量，`D` 是处理的数据点（Token）数量。
- **公式**: `Flops_fwd ≈ 2 * 参数量 * 批次大小 * 序列长度`
**2. 反向传播 (Backward Pass)**
- **核心**: 计算梯度，其计算量也主要由矩阵乘法构成。
- **估算**: 经验上，反向传播的计算量大约是前向传播的**两倍**。
- **公式**: `Flops_bwd ≈ 2 * Flops_fwd ≈ 4 * 参数量 * 批次大小 * 序列长度`
**总计算成本:**
- [黄金法则](./Lecture2-%E8%AE%AD%E7%BB%83%E6%88%90%E6%9C%AC%E9%BB%84%E9%87%91%E6%B3%95%E5%88%99-6-%E5%8F%82%E6%95%B0%E9%87%8F-Token%E6%95%B0.md): 将两者相加，得到一次完整迭代的总计算成本。
- `总Flops ≈ 6 * 参数量 * 批次大小 * 序列长度`
这个全面的资源核算框架是进行大模型训练的必备技能，它使得从系统层面理解和优化训练过程成为可能。
--- END OF FILE ---