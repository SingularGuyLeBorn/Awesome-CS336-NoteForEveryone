# 精英笔记:旋转位置编码 (RoPE)

在Transformer模型中,让模型理解单词的顺序和相对位置是至关重要的. **旋转位置编码(Rotary Position Embeddings, RoPE)**是当前解决此问题的最先进、最主流的方法. 它通过一种优雅的数学方式,将相对位置信息编码到自注意力机制中.

### 1. 目标:真正的相对位置编码

自注意力的核心是计算查询(Query)和键(Key)向量之间的点积,以得到注意力分数. 一个理想的位置编码方案应该使得这个点积只依赖于两个词的**内容**和它们的**相对位置**,而与它们的**绝对位置**无关.

用数学语言来说,给定两个词`x`和`y`,它们的绝对位置分别为`i`和`j`,我们希望它们的位置编码函数`f`满足:
  
`<f(x, i), f(y, j)> = g(x, y, i - j)`
  
其中 `<,>` 代表点积,`g` 是某个只依赖于相对位置`i - j`的函数.

早期的位置编码方法无法完美实现这一点:

- **加性绝对位置编码 (GPT系列)**: `f(x, i) = token_embedding(x) + pos_embedding(i)`. 点积展开后会包含与绝对位置`i`和`j`相关的交叉项,泄露了绝对位置信息.
- **正弦/余弦位置编码 (原始Transformer)**: 同样是加性嵌入,存在类似问题.

### 2. 核心思想:用旋转编码位置

RoPE的突破性思想源于一个简单的几何事实:**两个向量的点积在它们被同时旋转相同角度后保持不变**. RoPE巧妙地利用了这一点.

它不把位置信息作为加性向量,而是将位置信息视为一个**旋转操作**.

- 对于位于位置 `m` 的词 `x`,其最终的向量 `f(x, m)` 是通过将词嵌入向量 `x` **旋转**一个角度 `m * θ` 得到的.
- 对于位于位置 `n` 的词 `y`,其最终的向量 `f(y, n)` 是通过将词嵌入向量 `y` **旋转**一个角度 `n * θ` 得到的.

当计算它们的注意力分数时,实际上是计算旋转后的向量的点积. 由于点积的旋转不变性,这个分数只依赖于两个向量之间的**相对旋转角度**,即 `(m * θ) - (n * θ) = (m - n) * θ`. 这样,注意力分数就只依赖于相对位置 `m - n` 了.

![](https://storage.googleapis.com/static.a-b-c/project-daedalus/L3-P32.png)


  
上图直观地展示了这个过程:无论"we"和"know"在句子中的绝对位置如何变化,只要它们的相对距离保持不变,它们对应的旋转后向量之间的夹角就保持不变,从而点积(注意力分数)也保持稳定.

### 3. 高维空间的实现

在二维空间中,旋转是唯一的. 但在高维的词嵌入空间中(例如`d_model = 4096`),旋转的定义很复杂. RoPE采用了一种非常聪明的简化策略:

1. **分组**: 将`d`维的向量两两一组,看作`d/2`个二维平面(或复数).
2. **独立旋转**: 在每个二维平面上独立地进行旋转.
3. **不同频率**: 为每个二维平面分配一个不同的旋转“基频” `θ_k`. 这些频率通常被设计成从高频到低频排列(`θ_k = 10000^(-2k/d)`),与原始Transformer的正弦编码类似. 这使得模型既能捕捉到近距离的精细位置关系(高频),也能捕捉到远距离的粗略位置关系(低频).

### 4. 数学与代码实现

在数学上,对一个二维向量 `(x1, x2)` 进行角度 `mθ` 的旋转,等价于左乘一个2x2的旋转矩阵. 对于`d`维向量,这个操作等价于左乘一个块对角矩阵,其中每个对角块都是一个2x2的旋转矩阵.

![](https.storage.googleapis.com/static.a-b-c/project-daedalus/L3-P34.png)

在实践中,RoPE**并非在输入层一次性完成,而是在每个Transformer层的自注意力模块内部动态应用**.

- 当计算注意力时,模型会先生成常规的Q和K向量.
- 然后,根据每个token的位置,实时计算出对应的旋转矩阵(或等效的乘法因子).
- 将这个旋转操作分别应用到Q和K向量上,得到旋转后的`Q_rot`和`K_rot`.
- 最后,使用`Q_rot`和`K_rot`来计算注意力分数 `softmax(Q_rot * K_rot^T / sqrt(d_k))`.

![](https://storage.googleapis.com/static.a-b-c/project-daedalus/L3-P35.png)

这种在注意力计算时即时应用的“侵入式”方法,是保证相对位置编码有效性的关键. RoPE因其理论的优雅性和在长序列建模上的卓越表现,已成为现代LLM架构的黄金标准.