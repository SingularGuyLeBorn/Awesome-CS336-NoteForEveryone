# CS336  关于语言模型架构与训练的一切 - 讲座3笔记

本讲座将深入探讨Transformer架构的演进细节和训练中的超参数选择,这些细节往往在其他课程中被略过. 我们将从一个更偏向数据驱动的视角,通过分析过去几年中发布的大量大型语言模型(LLM),来理解哪些架构选择是至关重要的,哪些已经成为共识.

本次讲座的主题是:**最好的学习方式是亲身实践,而第二好的方式是从他人的经验中学习.** 

## 1. Transformer 架构回顾:从经典到现代

### 原始 Transformer

我们以经典的`Transformer`架构为起点,它包括了大家熟知的几个部分:

- **位置编码 (Positional Encoding)**: 使用正弦和余弦函数.
- **核心块**: 每个块包含一个多头自注意力(Multi-Head Self-Attention)层和一个前馈网络(Feed-Forward Network, FFN).
- **残差连接与规范化**: 在每个子层(注意力、FFN)之后进行加法(Add)和层规范化(Norm),这被称为**Post-Norm**.
- **输出层**: 最后通过一个线性层和Softmax函数得到概率分布.

![](https://storage.googleapis.com/static.a-b-c/project-daedalus/L3-P4.png)

### 现代 Transformer 变体

然而,你在课程作业中实现的并非原始版本,而是一个更现代的变体. 主要区别在于:

- **规范化层前置 (Pre-Norm)**: [Normalization Strategies in Transformers](./Lecture3-Normalization-Strategies.md) 是现代架构的基石之一. 规范化层(LayerNorm)被移到了注意力层和FFN层**之前**.
- **旋转位置编码 (RoPE)**: 我们使用 [Rotary Position Embeddings (RoPE)](./Lecture3-Rotary-Position-Embeddings.md),这是一种更先进的位置编码方法.
- **门控激活单元 (GLU)**: 前馈网络层(FFN)使用 [Gated Activations (GLU)](./Lecture3-Gated-Activations.md),特别是SwiGLU,而不是标准的ReLU.
- **无偏置项**: 线性层和规范化层中通常省略了偏置(bias)项.

![](https://storage.googleapis.com/static.a-b-c/project-daedalus/L3-P5.png)

这些改变并非随意为之,它们是过去数年里,从大量模型(如Llama系列、PaLM、GPT系列等)的实践中演化出的共识.

## 2. 核心架构变体分析

### 规范化:位置与类型

现代Transformer架构在规范化上有两大共识:

1. **位置 (****`Pre-Norm`** **Pre-Norm****)**: 几乎所有现代模型都采用`Pre-Norm`结构,即将规范化层放在残差连接的主路径之外、每个子模块(注意力/FFN)的输入端. 这极大地增强了训练的稳定性,缓解了梯度消失或爆炸问题,使得训练更深的网络成为可能.
2. **类型 (****`RMSNorm`** **RMSNorm****)**: 许多模型已从`LayerNorm`转向`RMSNorm`. `RMSNorm`简化了计算(不计算均值),减少了参数和内存移动,从而在保持模型性能的同时提升了训练速度.

这两个选择的背后,是业界对**训练稳定性**和**系统效率(尤其是内存带宽)**的深刻理解.

> **[深度解析]** 关于`Pre-Norm`为何优于`Post-Norm`,`RMSNorm`相比`LayerNorm`的效率优势,以及最新的“双重规范化”趋势,请参阅 [Normalization Strategies in Transformers](./Lecture3-Normalization-Strategies.md).

### 激活函数:门控机制的胜利

激活函数的选择也经历了演变.

- **早期模型**: 使用**ReLU** (如原始Transformer, T5) 或**GeLU** (如GPT系列).
- **现代模型**: 绝大多数模型,特别是2023年之后发布的,都转向了**门控线性单元(Gated Linear Units, GLU)****的变体,如****SwiGLU** (Llama, PaLM) 和**GeGLU** (T5-v1.1, Gemma).

实验反复证明,门控激活能持续带来性能提升. 门控机制通过一个与输入相关的“门”来动态控制信息流,赋予了网络更强的表达能力.

> **[深度解析]** 门控机制是什么？为什么它比简单的非线性函数更有效？请参阅 [Gated Activations (GLU)](./Lecture3-Gated-Activations.md).

### 位置编码:RoPE 的主导地位

位置编码是让模型理解序列顺序的关键.

- **早期方法**: 包括加性的**绝对位置编码**(GPT-1/2/3)或基于**正弦/余弦**的编码(原始Transformer).
- **现代标准**: **旋转位置编码 (RoPE)** 已成为绝对的主流. 它的核心思想是通过旋转操作将相对位置信息融入到查询(Query)和键(Key)向量中,理论上更优雅且在长序列上表现更佳.

> **[深度解析]** RoPE如何通过旋转实现相对位置编码？其背后的数学原理和实现细节是怎样的？请参阅 [Rotary Position Embeddings (RoPE)](./Lecture3-Rotary-Position-Embeddings.md).

### 串行 vs. 并行层

标准的Transformer块是**串行**的:输入先经过注意力层,其输出再经过FFN层.

- **并行层**: 一些模型(如GPT-J, PaLM)尝试过**并行**结构,即让注意力和FFN层同时处理相同的输入,然后将它们的输出相加.

  - **优势**: 理论上可以融合矩阵乘法,提升计算并行度,从而加速训练.
  - **现状**: 尽管有此优势,但目前大多数模型仍采用串行结构,这可能意味着串行结构在表达能力上更具优势.


## 3. 超参数选择:不成文的规则

在训练LLM时,许多超参数的选择实际上遵循着惊人的共识.

- **前馈网络维度比例 (****`d_ff / d_model`** **d_ff / d_model****)**:

  - 对于使用ReLU/GeLU的模型,一个经典的“经验法则”是`d_ff = 4 * d_model`.
  - 对于使用GLU变体的模型,为了保持参数量相近,这个比例通常调整为`d_ff ≈ (8/3) * d_model ≈ 2.67 * d_model`.
  - **例外**: T5模型曾大胆地使用了高达64倍的比例,但其后续版本T5-v1.1又回归到了更标准的2.5倍,这暗示了超大比例可能并非最优.

- **模型宽高比 (****`d_model / n_layer`** **d_model / n_layer****)**:

  - 这个比例衡量了模型的“宽度”(隐藏层维度)与“深度”(层数).
  - 大多数成功的模型似乎都找到了一个“甜蜜点”,该值大约在**100到200**之间. 例如,GPT-3、OPT、Mistral等模型的宽高比都在128左右.
  - 这个选择也受到系统并行化策略的影响:极深的模型有利于流水线并行,而极宽的模型有利于张量并行.

- **词汇表大小 (Vocabulary Size)**:

  - **单语模型**: 通常在3万到5万之间(如GPT-2/3, LLaMA).
  - **多语/生产系统**: 趋向于更大的词汇表,通常在10万到25万之间(如mT5, PaLM, GPT-4, Command A). 这是为了更好地支持多语言、特殊符号和代码等.

- **正则化 (Regularization)**:

  - **Dropout**: 在早期的模型中很常见,但在现代LLM的**预训练**阶段已基本弃用. 因为预训练数据量巨大,模型通常只过一遍数据,过拟合风险很低.
  - **权重衰减 (Weight Decay)**: 仍然被广泛使用,但其作用很奇特. 它主要不是为了防止过拟合,而是通过与学习率调度(特别是cosine schedule)的复杂相互作用,来改善训练末期的优化动态,从而获得更低的训练损失.


## 4. 推理效率与高级注意力机制

### GQA/MQA:为高效推理而设计

在模型训练时,我们可以并行处理整个批次,算力利用率高. 但在**自回归生成(推理)**时,模型必须逐个token生成,这导致了所谓的“KV缓存”瓶颈,即读写巨大的Key和Value缓存成为性能瓶颈,内存带宽而非计算能力成为限制因素.

- **多查询注意力 (MQA)** 和 **分组查询注意力 (GQA)** 正是为此而生. 它们通过让多个查询头(Query heads)共享同一组或几组键/值头(Key/Value heads),极大地减小了KV缓存的大小,从而显著提升了推理速度和吞吐量.

> **[深度解析]** 为何推理时的计算模式与训练时完全不同？KV缓存瓶颈是如何产生的？MQA/GQA又是如何从根本上解决这个问题的？请参阅 [Grouped-Query & Multi-Query Attention (GQA/MQA)](./Lecture3-GQA-MQA.md).

### **稀疏与滑动窗口注意力**

为了处理更长的上下文,研究人员提出了多种方法来避免计算完整的(二次复杂度的)注意力矩阵.

- **滑动窗口注意力 (Sliding Window Attention)**: 每个token只关注其邻近的一个窗口内的其他token.
- **稀疏注意力 (Sparse Attention)**: 设计更复杂的、非局部的注意力模式,如跨步(strided)或固定(fixed)模式.
- **现代混合策略**: 最新的长上下文模型(如Llama 4, Command A)采用了一种巧妙的混合策略:大部分层使用带RoPE的滑动窗口注意力来处理局部信息,同时每隔几层插入一个**不带位置编码的全局注意力层**来整合长距离信息. 这种设计兼顾了效率和长程依赖的建模能力.

### **训练稳定性技巧**

随着模型规模的增大,训练稳定性变得至关重要. 不稳定的训练表现为损失函数突然出现尖峰(spikes).

- **问题的根源**: `softmax`函数的指数和除法操作是数值不稳定的主要来源,这在输出层和注意力层都存在.
- **解决方案**:

  - **Z-Loss**: 在最终的损失函数中加入一个辅助项,惩罚输出层softmax的归一化因子`Z`偏离1,从而使其行为更稳定.
  - **QK Norm**: 在计算注意力分数前,对查询(Q)和键(K)向量分别进行LayerNorm. 这能有效控制输入到softmax的值的范围,防止其爆炸.
  
    这些技巧,尤其是QK Norm,再次凸显了**规范化**在稳定大型模型训练中的神奇效果.
