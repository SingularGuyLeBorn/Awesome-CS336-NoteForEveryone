# 技能：技术内容写作

## 核心原则

> **讲故事，不堆砌**：好的技术写作像讲故事，有起承转合，不是公式罗列。

---

## 一、写作前准备

### 1.1 明确三个问题
1. **读者是谁？** (有什么背景知识)
2. **读者想知道什么？** (核心问题)
3. **读完后读者应该能做什么？** (学习目标)

### 1.2 素材整理
- [ ] 核心概念定义
- [ ] 关键公式/算法
- [ ] 直觉类比
- [ ] 图示/图表
- [ ] 代码示例
- [ ] 实验数据
- [ ] 常见误区

---

## 二、结构化写作模式

### 2.1 金字塔模式 (推荐)

```
               [核心结论]
              /    |    \
     [论点1]   [论点2]   [论点3]
     /   \     /   \     /   \
  [细节] [细节] [细节] [细节] [细节] [细节]
```

**应用**:
1. 先给结论/Takeaway
2. 再展开解释为什么
3. 最后给细节证据

### 2.2 Why → What → How 模式

```markdown
## [概念名称]

### Why: 为什么需要这个？
[问题背景，痛点是什么]

### What: 这是什么？
[核心思想，一句话定义]

### How: 怎么实现？
[技术细节，算法流程]
```

### 2.3 问题-方案-效果 模式

```markdown
## [技术名称]

**问题**: Transformer 推理慢，因为自回归解码每次只生成一个 token。

**方案**: Speculative Decoding 用小模型快速生成多个候选，大模型并行验证。

**效果**: 在保证输出分布不变的前提下，实现 2-3x 加速。
```

---

## 三、概念解释技巧

### 3.1 三层解释法

```markdown
### KV Cache

**一句话**: 缓存已计算的 Key 和 Value，避免重复计算。

**类比**: 像是考试时的草稿纸——你把中间计算结果写下来，后面直接查，不用重算。

**技术定义**: 在自回归生成时，将前 t-1 步的 K, V 矩阵缓存下来，
第 t 步只需计算新 token 的 k_t, v_t，与缓存拼接后做注意力计算。
```

### 3.2 类比使用规范

好的类比:
- ✅ **简单熟悉**: 读者一定知道的事物
- ✅ **核心映射清晰**: 对应关系明确
- ✅ **局限性说明**: "当然，这个类比不完美..."

坏的类比:
- ❌ 用一个复杂概念解释另一个复杂概念
- ❌ 类比的边角情况误导理解
- ❌ 强行类比，映射关系混乱

### 3.3 公式解释模板

```markdown
$$
\text{Loss} = -\sum_{t=1}^{T} \log p(x_t | x_{<t})
$$

**符号说明**:
- $x_t$: 第 t 个 token
- $x_{<t}$: 前 t-1 个 token 组成的上下文
- $p(x_t | x_{<t})$: 模型预测的条件概率

**直觉解释**: 这个 Loss 衡量模型预测"下一个词"的准确度。
预测越准，概率越高，log 值越大 (负数绝对值越小)，Loss 越小。
```

---

## 四、视觉化技巧

### 4.1 表格对比

```markdown
| 方法 | 内存占用 | 计算开销 | 精度保持 |
|------|---------|---------|---------|
| MHA  | 高      | 高      | 100%    |
| MQA  | 低      | 低      | ~99%    |
| GQA  | 中      | 中      | ~99.5%  |
```

### 4.2 ASCII 流程图

```markdown
```
输入 x₀ → [Encoder] → context → [Decoder] → y₁ → y₂ → ... → yₙ
              ↓                      ↑
         隐状态 h               注意力机制
```
```

### 4.3 时间线/演进图

```markdown
## 技术演进

2017: Transformer (Vaswani)
  ↓
2019: GPT-2 (OpenAI)
  ↓
2022: Speculative Decoding (Google)
  ↓
2024: DeepSeek-V2 MLA
```

### 4.4 引用块强调

```markdown
> 💡 **核心洞见**: Speculative Decoding 的精妙之处在于，
> 它用"乐观猜测 + 事后验证"替代了"保守逐步生成"，
> 本质上是用计算换时间。
```

---

## 五、语言风格指南

### 5.1 简洁优先
- ❌ "在这种情况下，我们可以考虑采用..."
- ✅ "此时可以用..."

### 5.2 主动语态
- ❌ "该方法被用于..."
- ✅ "该方法用于..."

### 5.3 具体优于抽象
- ❌ "可以显著提升性能"
- ✅ "可以将推理速度提升 2-3 倍"

### 5.4 避免过度修饰
- ❌ "非常重要的、关键的核心概念"
- ✅ "核心概念"

### 5.5 中英文混排规范
- 英文术语首次出现加中文注释: `Speculative Decoding (投机解码)`
- 专有名词保持英文: `Transformer`, `Attention`
- 英文与中文之间加空格: `这是一个 Python 脚本`

---

## 六、常见写作误区

### 误区 1: 堆砌公式
**问题**: 一堆公式，没有解释
**解决**: 每个公式后加直觉解释

### 误区 2: 过于口语化
**问题**: "这个东西超级牛逼"
**解决**: 保持专业但不枯燥

### 误区 3: 假设读者知道一切
**问题**: 使用未定义的术语
**解决**: 首次使用时简要解释，或链接到定义

### 误区 4: 没有 Takeaway
**问题**: 读完不知道重点是什么
**解决**: 每节结尾给明确的 Takeaway

### 误区 5: 信息过载
**问题**: 一节塞太多内容
**解决**: 一节一个核心概念，复杂内容拆分

---

## 七、写作检查清单

### 内容检查
- [ ] 核心概念有明确定义
- [ ] 每个公式有直觉解释
- [ ] 重要结论有强调
- [ ] 专业术语有解释或链接

### 结构检查
- [ ] 层级清晰（H1 → H2 → H3）
- [ ] 段落长度适中（5-8 行）
- [ ] 有适当的视觉分隔

### 语言检查
- [ ] 无冗余修饰词
- [ ] 主动语态为主
- [ ] 中英文混排规范

### 读者体验
- [ ] 开头能抓住注意力
- [ ] 每节有 Takeaway
- [ ] 结尾有总结或延伸
