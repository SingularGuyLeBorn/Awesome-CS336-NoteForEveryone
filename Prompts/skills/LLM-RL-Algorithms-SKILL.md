---
name: LLM强化学习算法全景指南
description: 从PPO到GRPO到各种*PO变体的代码优先知识库构建方法
---

# LLM强化学习算法全景指南 (Code-First)

本技能文档帮助你构建一个**代码优先**的LLM强化学习算法知识库，覆盖从基础RL理论到最新*PO变体算法的完整图景。

---

## 一、知识库整体架构

### 1.1 目标愿景

```
LLM-RL-Algorithms/
├── README.md                           # 项目总览与导航
├── 01-RL-Foundations/                  # 数学基础
│   ├── 01-probability-spaces.md        # 概率空间
│   ├── 02-markov-decision-process.md   # MDP定义
│   ├── 03-bellman-equations.md         # 贝尔曼方程
│   └── 04-policy-gradient-theorem.md   # 策略梯度定理
├── 02-Classic-Algorithms/              # 经典RL算法
│   ├── 01-REINFORCE.py                 # 朴素策略梯度
│   ├── 02-Actor-Critic.py              # AC架构
│   ├── 03-A2C.py                       # 同步AC
│   ├── 04-TRPO.py                      # 信赖域
│   └── 05-PPO.py                       # 近端策略优化
├── 03-LLM-Alignment/                   # LLM对齐算法
│   ├── 01-RLHF-Pipeline.py             # 完整RLHF流水线
│   ├── 02-Reward-Modeling.py           # 奖励模型训练
│   └── 03-KL-Regularization.py         # KL惩罚机制
├── 04-Direct-Preference/               # 直接偏好优化系列
│   ├── 01-DPO.py                       # Direct Preference Optimization
│   ├── 02-IPO.py                       # Identity Preference Optimization
│   ├── 03-KTO.py                       # Kahneman-Tversky Optimization
│   ├── 04-SimPO.py                     # Simple Preference Optimization
│   ├── 05-ORPO.py                      # Odds Ratio Preference Optimization
│   └── 06-comparison.md                # DPO系列对比
├── 05-Group-Relative/                  # 组相对优化系列
│   ├── 01-GRPO.py                      # Group Relative Policy Optimization
│   ├── 02-Dr-GRPO.py                   # 修正版GRPO
│   ├── 03-GSPO.py                      # Group Sequence Policy Optimization
│   ├── 04-GMPO.py                      # Geometric Mean Policy Optimization
│   └── 05-comparison.md                # GRPO系列对比
├── 06-Advanced-Variants/               # 高级变体
│   ├── 01-DAPO.py                      # Decoupled Clip + Dynamic Sampling
│   ├── 02-Critique-GRPO.py             # 批评者增强GRPO
│   ├── 03-RLOO.py                      # Reinforce Leave One Out
│   └── 04-ReMax.py                     # Reinforce with Maximum
├── 07-Verifiable-Rewards/              # 可验证奖励RL
│   ├── 01-RLVR-Overview.md             # 可验证奖励概述
│   ├── 02-Math-Reward.py               # 数学答案验证
│   ├── 03-Code-Reward.py               # 代码测试验证
│   └── 04-Format-Reward.py             # 格式验证
├── 08-Case-Studies/                    # 案例研究
│   ├── DeepSeek-R1.md                  # R1训练细节
│   ├── Kimi-K1.5.md                    # K1.5技术报告
│   ├── Qwen3.md                        # Qwen3 GSPO
│   └── OpenAI-o1.md                    # o1推测分析
└── utils/                              # 工具代码
    ├── reward_utils.py                 # 奖励计算工具
    ├── sampling_utils.py               # 采样工具
    └── visualization.py                # 训练曲线可视化
```

### 1.2 核心设计原则

| 原则 | 描述 | 实践 |
|------|------|------|
| **Code-First** | 代码即文档 | 每个算法有完整可运行代码 |
| **数学-代码对照** | 公式与代码1:1映射 | 代码注释标注对应公式 |
| **渐进式复杂度** | 从简单到复杂 | REINFORCE → PPO → GRPO |
| **对比导向** | 强调差异 | 每个系列有comparison.md |

---

## 二、算法谱系图

### 2.1 演化路线图

```
                            ┌─────────────────────────────────────────────┐
                            │              策略梯度定理                    │
                            │  ∇J = E[∇log π(a|s) · R]                   │
                            └─────────────────┬───────────────────────────┘
                                              │
                    ┌─────────────────────────┼─────────────────────────┐
                    │                         │                         │
                    ▼                         ▼                         ▼
            ┌───────────────┐         ┌───────────────┐         ┌───────────────┐
            │   REINFORCE   │         │ Actor-Critic  │         │     TRPO      │
            │   (朴素PG)    │         │   (AC架构)    │         │  (信赖域约束)  │
            └───────────────┘         └───────────────┘         └───────┬───────┘
                                                                        │
                                                                        ▼
                                                                ┌───────────────┐
                                                                │      PPO      │
                                                                │  (裁剪替代)   │
                                                                └───────┬───────┘
                                                                        │
                    ┌───────────────────────────────────────────────────┼───────────────┐
                    │                                                   │               │
                    ▼                                                   ▼               ▼
            ┌───────────────┐                                   ┌───────────────┐ ┌───────────────┐
            │     RLHF      │                                   │     GRPO      │ │     DPO       │
            │ (人类反馈RL)  │                                   │ (组相对优化)  │ │ (直接偏好)    │
            └───────┬───────┘                                   └───────┬───────┘ └───────┬───────┘
                    │                                                   │               │
        ┌───────────┼───────────┐               ┌───────────┬───────────┼───────┐       │
        │           │           │               │           │           │       │       │
        ▼           ▼           ▼               ▼           ▼           ▼       ▼       ▼
    ┌───────┐  ┌───────┐  ┌───────┐       ┌───────┐  ┌───────┐  ┌───────┐ ┌───────┐ ┌───────┐
    │ RLAIF │  │  PPO  │  │ ReMax │       │ GSPO  │  │ GMPO  │  │ DAPO  │ │ SimPO │ │  IPO  │
    │       │  │ w/RM  │  │       │       │(Qwen3)│  │       │  │(字节) │ │       │ │       │
    └───────┘  └───────┘  └───────┘       └───────┘  └───────┘  └───────┘ └───────┘ └───────┘
```

### 2.2 算法分类表

| 类别 | 算法 | 核心创新 | 论文/来源 |
|------|------|----------|-----------|
| **经典PG** | REINFORCE | 朴素策略梯度 | Williams 1992 |
| | A2C | 同步AC+优势函数 | Mnih 2016 |
| | TRPO | 信赖域约束 | Schulman 2015 |
| | PPO | 裁剪替代TRPO | Schulman 2017 |
| **RLHF** | InstructGPT | SFT+RM+PPO | OpenAI 2022 |
| **直接偏好** | DPO | 隐式奖励 | Rafailov 2023 |
| | IPO | 正则化DPO | Azar 2023 |
| | KTO | 前景理论 | Ethayarajh 2024 |
| | SimPO | 去参考模型 | Meng 2024 |
| | ORPO | Odds Ratio | Hong 2024 |
| **组相对** | GRPO | 组内均值基线 | DeepSeek 2024 |
| | Dr-GRPO | 移除归一化 | Liu 2025 |
| | GSPO | 序列级优化 | Qwen 2025 |
| | GMPO | 几何均值 | 2025 |
| | DAPO | 解耦裁剪+动态采样 | ByteDance 2025 |
| **批评者增强** | Critique-GRPO | 自然语言反馈 | 2025 |

---

## 三、代码模板规范

### 3.1 算法文件结构

每个算法Python文件应遵循以下结构：

```python
"""
算法名称: [Full Name] (缩写)
论文: [Paper Title]
作者: [Authors]
年份: [Year]
链接: [arXiv/Paper URL]

核心创新:
1. [创新点1]
2. [创新点2]

与前作区别:
- [算法A] vs [本算法]: [差异描述]

数学公式:
$$
[核心损失函数]
$$
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple
from dataclasses import dataclass

# ============================================
# 第一部分: 配置与超参数
# ============================================

@dataclass
class AlgorithmConfig:
    """算法超参数配置"""
    # 核心参数
    beta: float = 0.1           # KL惩罚系数
    clip_epsilon: float = 0.2   # PPO裁剪范围
    
    # 采样参数
    num_samples: int = 8        # 每prompt采样数
    
    # 训练参数
    learning_rate: float = 1e-6
    batch_size: int = 32

# ============================================
# 第二部分: 核心损失函数
# ============================================

def compute_loss(
    policy_log_probs: torch.Tensor,     # [B, T] 当前策略log概率
    ref_log_probs: torch.Tensor,        # [B, T] 参考策略log概率
    rewards: torch.Tensor,              # [B] 奖励
    config: AlgorithmConfig
) -> Tuple[torch.Tensor, dict]:
    """
    计算算法损失函数
    
    数学公式:
    $$
    L = ...  # 在此标注对应的LaTeX公式
    $$
    
    Args:
        policy_log_probs: 当前策略的log概率
        ref_log_probs: 参考策略的log概率
        rewards: 回报
        config: 超参数配置
    
    Returns:
        loss: 标量损失
        metrics: 调试指标字典
    """
    # Step 1: [步骤说明] 
    # 对应公式: [公式编号或简写]
    ...
    
    # Step 2: [步骤说明]
    ...
    
    metrics = {
        "loss": loss.item(),
        "mean_reward": rewards.mean().item(),
        # 更多调试指标
    }
    
    return loss, metrics

# ============================================
# 第三部分: 优势/权重计算
# ============================================

def compute_advantages(
    rewards: torch.Tensor,
    group_size: int,
    mode: str = "centered"  # "raw" | "centered" | "normalized"
) -> torch.Tensor:
    """
    计算优势估计
    
    数学公式:
    - raw: A_i = R_i
    - centered: A_i = R_i - mean(R)
    - normalized: A_i = (R_i - mean(R)) / (std(R) + eps)
    """
    ...

# ============================================
# 第四部分: 完整训练步骤
# ============================================

def train_step(
    model: nn.Module,
    ref_model: nn.Module,
    optimizer: torch.optim.Optimizer,
    batch: dict,
    config: AlgorithmConfig
) -> dict:
    """
    单步训练
    
    Args:
        model: 当前策略模型
        ref_model: 冻结的参考模型
        optimizer: 优化器
        batch: 包含prompts, responses, rewards的批次
        config: 超参数
    
    Returns:
        metrics: 训练指标
    """
    ...

# ============================================
# 第五部分: 使用示例
# ============================================

if __name__ == "__main__":
    # 最小可运行示例
    config = AlgorithmConfig()
    
    # 模拟数据
    policy_log_probs = torch.randn(8, 100)
    ref_log_probs = torch.randn(8, 100)
    rewards = torch.tensor([1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0])
    
    loss, metrics = compute_loss(
        policy_log_probs, ref_log_probs, rewards, config
    )
    
    print(f"Loss: {loss.item():.4f}")
    print(f"Metrics: {metrics}")
```

### 3.2 对比分析文件结构

每个算法系列的`comparison.md`应包含：

```markdown
# [系列名] 算法对比分析

## 一、核心差异总结表

| 算法 | 基线估计 | 归一化 | 裁剪方式 | 核心优势 |
|------|----------|--------|----------|----------|
| A    | ...      | ...    | ...      | ...      |
| B    | ...      | ...    | ...      | ...      |

## 二、数学公式对比

### 算法A
$$L_A = ...$$

### 算法B  
$$L_B = ...$$

### 差异分析
[逐项对比公式差异]

## 三、代码差异对比

```python
# 算法A的关键代码
...

# 算法B的关键代码
...
```

## 四、实验结果对比

| 算法 | MATH | AIME | GSM8K |
|------|------|------|-------|
| A    | ...  | ...  | ...   |
| B    | ...  | ...  | ...   |

## 五、何时选择哪个算法

- 选择A当: [场景描述]
- 选择B当: [场景描述]
```

---

## 四、重点算法写作指南

### 4.1 DPO系列

| 算法 | 核心公式 | 关键创新 |
|------|----------|----------|
| **DPO** | $-\log\sigma(\beta\log\frac{\pi_\theta(y_w)}{\pi_{ref}(y_w)} - \beta\log\frac{\pi_\theta(y_l)}{\pi_{ref}(y_l)})$ | 隐式奖励建模 |
| **IPO** | $(r_\theta(y_w) - r_\theta(y_l) - 1/\beta)^2$ | 正则化防过拟合 |
| **KTO** | 分离好/坏样本的单独损失 | 不需要成对偏好 |
| **SimPO** | 去除参考模型，添加长度归一化 | 简化计算 |
| **ORPO** | 使用odds ratio替代log ratio | 更稳定的梯度 |

### 4.2 GRPO系列

| 算法 | 优势估计 | 核心改进 |
|------|----------|----------|
| **GRPO** | $A_i = (R_i - \bar{R}) / \sigma$ | 组内基线替代价值函数 |
| **Dr-GRPO** | $A_i = R_i - \bar{R}$ | 移除标准差归一化 |
| **GSPO** | 序列级重要性比率 | Token级→序列级 |
| **GMPO** | 几何均值优化 | 抵抗异常值 |
| **DAPO** | 解耦裁剪+动态采样 | 长CoT稳定性 |

### 4.3 关键代码片段

#### GRPO核心

```python
def grpo_loss(log_probs, rewards, group_size):
    """GRPO: Group Relative Policy Optimization"""
    # 重塑为组 [num_groups, group_size]
    rewards = rewards.view(-1, group_size)
    
    # 组内归一化 (原始GRPO)
    mean_r = rewards.mean(dim=1, keepdim=True)
    std_r = rewards.std(dim=1, keepdim=True)
    advantages = (rewards - mean_r) / (std_r + 1e-5)
    
    # 策略梯度损失
    loss = -(log_probs * advantages.view(-1)).mean()
    return loss
```

#### DPO核心

```python
def dpo_loss(policy_logps_w, policy_logps_l, 
             ref_logps_w, ref_logps_l, beta=0.1):
    """DPO: Direct Preference Optimization"""
    # 隐式奖励差
    implicit_reward_diff = beta * (
        (policy_logps_w - ref_logps_w) - 
        (policy_logps_l - ref_logps_l)
    )
    
    # 交叉熵损失
    loss = -F.logsigmoid(implicit_reward_diff).mean()
    return loss
```

#### DAPO核心改进

```python
def dapo_loss(log_probs, rewards, old_log_probs, 
              clip_low=0.8, clip_high=1.2):
    """DAPO: 解耦裁剪 + Token级策略梯度"""
    # 1. 解耦裁剪 (Decoupled Clip)
    ratio = torch.exp(log_probs - old_log_probs)
    
    # 对正负优势使用不同裁剪
    advantages = compute_advantages(rewards)
    
    clipped_ratio = torch.where(
        advantages > 0,
        torch.clamp(ratio, max=clip_high),  # 正优势只裁上界
        torch.clamp(ratio, min=clip_low)    # 负优势只裁下界
    )
    
    # 2. Token级策略梯度损失
    loss = -(clipped_ratio * advantages).mean()
    
    return loss
```

---

## 五、资源收集清单

### 5.1 必读论文

| 优先级 | 论文 | 链接 | 关键内容 |
|--------|------|------|----------|
| ⭐⭐⭐ | PPO | [arXiv](https://arxiv.org/abs/1707.06347) | 裁剪目标 |
| ⭐⭐⭐ | DPO | [arXiv](https://arxiv.org/abs/2305.18290) | 隐式奖励 |
| ⭐⭐⭐ | DeepSeekMath | [arXiv](https://arxiv.org/abs/2402.03300) | GRPO |
| ⭐⭐⭐ | DeepSeek R1 | [arXiv](https://arxiv.org/abs/2501.12948) | 纯RL R1-Zero |
| ⭐⭐ | Qwen3 Report | [技术报告] | GSPO |
| ⭐⭐ | DAPO | [GitHub](https://dapo-sia.github.io/) | 解耦裁剪 |
| ⭐⭐ | Dr. GRPO | [arXiv] | 归一化问题 |
| ⭐ | SimPO | [arXiv](https://arxiv.org/abs/2405.14734) | 简化DPO |
| ⭐ | IPO | [arXiv](https://arxiv.org/abs/2310.12036) | 正则化DPO |

### 5.2 开源代码库

| 库名 | 链接 | 包含算法 |
|------|------|----------|
| verl | https://github.com/volcengine/verl | PPO, GRPO, GSPO, DAPO |
| TRL | https://github.com/huggingface/trl | PPO, DPO, KTO |
| OpenRLHF | https://github.com/OpenLLMAI/OpenRLHF | PPO, GRPO |
| LLaMA-Factory | https://github.com/hiyouga/LLaMA-Factory | 多种算法 |

### 5.3 优质博客

| 作者 | 链接 | 主题 |
|------|------|------|
| Sebastian Raschka | sebastianraschka.com | GRPO深度解析 |
| Chip Huyen | huyenchip.com | RLHF综述 |
| Nathan Lambert | interconnects.ai | RLHF生态 |

---

## 六、写作流程

### 6.1 单个算法写作流程

```
1. 阅读原论文 (30-60分钟)
   ├── 标记核心公式
   ├── 理解与前作差异
   └── 记录实验设置

2. 查找开源实现 (15-30分钟)
   ├── GitHub搜索
   ├── verl/TRL等框架
   └── 论文附带代码

3. 编写代码模板 (60-90分钟)
   ├── 实现核心损失函数
   ├── 添加详细注释
   ├── 编写最小示例
   └── 验证代码可运行

4. 补充文档 (30分钟)
   ├── 添加论文信息
   ├── 编写数学公式
   └── 添加使用建议
```

### 6.2 系列对比写作流程

```
1. 收集系列内所有算法
2. 提取核心差异点
3. 制作对比表格
4. 编写代码对比
5. 整理实验结果
6. 总结选择建议
```

---

## 七、质量检查清单

### 7.1 代码文件检查

- [ ] 文件头部包含论文信息
- [ ] 核心函数有完整docstring
- [ ] 代码注释标注对应公式
- [ ] 有最小可运行示例
- [ ] 所有import在文件顶部
- [ ] 使用type hints

### 7.2 对比文件检查

- [ ] 有差异总结表
- [ ] 有公式对比
- [ ] 有代码对比
- [ ] 有实验结果
- [ ] 有选择建议

### 7.3 整体项目检查

- [ ] README有清晰导航
- [ ] 算法按逻辑分组
- [ ] utils可复用
- [ ] 代码风格一致

---

## 八、快速启动

### 8.1 创建项目结构

```bash
mkdir -p LLM-RL-Algorithms/{01-RL-Foundations,02-Classic-Algorithms,03-LLM-Alignment,04-Direct-Preference,05-Group-Relative,06-Advanced-Variants,07-Verifiable-Rewards,08-Case-Studies,utils}
```

### 8.2 推荐写作顺序

```
第一阶段（基础）:
1. REINFORCE.py
2. PPO.py
3. DPO.py
4. GRPO.py

第二阶段（变体）:
5. SimPO.py
6. IPO.py
7. Dr-GRPO.py
8. GSPO.py

第三阶段（最新）:
9. DAPO.py
10. GMPO.py
11. Critique-GRPO.py

第四阶段（对比总结）:
12. 各系列comparison.md
13. 案例研究
```

---

## 九、示例：GRPO完整实现

```python
"""
算法名称: Group Relative Policy Optimization (GRPO)
论文: DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models
作者: Zhihong Shao et al.
年份: 2024
链接: https://arxiv.org/abs/2402.03300

核心创新:
1. 使用组内均值替代价值函数作为基线
2. 组内标准差归一化（可选，有争议）
3. 无需训练单独的价值网络

数学公式:
$$
A_i = \frac{R_i - \text{mean}(R_1, ..., R_G)}{\text{std}(R_1, ..., R_G) + \epsilon}
$$
$$
L^{GRPO} = -\mathbb{E}[\min(r_t A_i, \text{clip}(r_t, 1-\epsilon, 1+\epsilon) A_i)]
$$
"""

import torch
import torch.nn.functional as F
from dataclasses import dataclass
from typing import Tuple, Optional

@dataclass
class GRPOConfig:
    """GRPO超参数"""
    group_size: int = 8           # 每prompt采样数
    clip_epsilon: float = 0.2     # PPO裁剪范围
    kl_coef: float = 0.01         # KL惩罚系数
    use_std_norm: bool = False    # 是否使用std归一化 (Dr. GRPO建议False)
    eps: float = 1e-5             # 数值稳定

def compute_grpo_advantages(
    rewards: torch.Tensor,
    group_size: int,
    use_std_norm: bool = False,
    eps: float = 1e-5
) -> torch.Tensor:
    """
    计算GRPO优势估计
    
    Args:
        rewards: [batch_size] 所有response的奖励
        group_size: 每个prompt的response数量
        use_std_norm: 是否除以标准差
        eps: 数值稳定项
    
    Returns:
        advantages: [batch_size] 优势值
    """
    # 重塑为 [num_prompts, group_size]
    num_prompts = rewards.shape[0] // group_size
    rewards = rewards.view(num_prompts, group_size)
    
    # 组内均值作为基线
    mean_rewards = rewards.mean(dim=1, keepdim=True)
    advantages = rewards - mean_rewards
    
    # 可选: 标准差归一化 (Dr. GRPO建议移除)
    if use_std_norm:
        std_rewards = rewards.std(dim=1, keepdim=True)
        advantages = advantages / (std_rewards + eps)
    
    return advantages.view(-1)

def compute_grpo_loss(
    policy_log_probs: torch.Tensor,   # [B, T]
    old_log_probs: torch.Tensor,      # [B, T]
    advantages: torch.Tensor,          # [B]
    config: GRPOConfig
) -> Tuple[torch.Tensor, dict]:
    """
    计算GRPO损失
    
    Args:
        policy_log_probs: 当前策略的序列log概率
        old_log_probs: 旧策略的序列log概率
        advantages: 优势值
        config: 配置
    
    Returns:
        loss: 标量损失
        metrics: 调试指标
    """
    # 序列总log概率
    seq_policy_logp = policy_log_probs.sum(dim=-1)  # [B]
    seq_old_logp = old_log_probs.sum(dim=-1)        # [B]
    
    # 重要性比率
    ratio = torch.exp(seq_policy_logp - seq_old_logp)
    
    # PPO裁剪
    clipped_ratio = torch.clamp(
        ratio, 
        1 - config.clip_epsilon, 
        1 + config.clip_epsilon
    )
    
    # 取较小值
    unclipped_loss = ratio * advantages
    clipped_loss = clipped_ratio * advantages
    policy_loss = -torch.min(unclipped_loss, clipped_loss).mean()
    
    # 指标
    metrics = {
        "policy_loss": policy_loss.item(),
        "mean_ratio": ratio.mean().item(),
        "mean_advantage": advantages.mean().item(),
        "clip_fraction": ((ratio - 1).abs() > config.clip_epsilon).float().mean().item()
    }
    
    return policy_loss, metrics

def grpo_train_step(
    model,
    ref_model,
    optimizer,
    prompts: torch.Tensor,
    responses: torch.Tensor,
    rewards: torch.Tensor,
    config: GRPOConfig
) -> dict:
    """完整GRPO训练步骤"""
    # 1. 计算优势
    advantages = compute_grpo_advantages(
        rewards, config.group_size, config.use_std_norm
    )
    
    # 2. 计算log概率
    with torch.no_grad():
        old_log_probs = compute_log_probs(model, prompts, responses)
        ref_log_probs = compute_log_probs(ref_model, prompts, responses)
    
    policy_log_probs = compute_log_probs(model, prompts, responses)
    
    # 3. 计算损失
    policy_loss, metrics = compute_grpo_loss(
        policy_log_probs, old_log_probs, advantages, config
    )
    
    # 4. KL惩罚
    kl_div = (policy_log_probs - ref_log_probs).sum(dim=-1).mean()
    total_loss = policy_loss + config.kl_coef * kl_div
    
    # 5. 更新
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()
    
    metrics["kl_div"] = kl_div.item()
    metrics["total_loss"] = total_loss.item()
    
    return metrics

# ============================================
# 使用示例
# ============================================

if __name__ == "__main__":
    config = GRPOConfig(group_size=4, use_std_norm=False)
    
    # 模拟数据: 2个prompt，每个4个response
    rewards = torch.tensor([1.0, 0.0, 1.0, 0.5,   # prompt 1
                            0.0, 0.0, 1.0, 1.0])  # prompt 2
    
    advantages = compute_grpo_advantages(
        rewards, config.group_size, config.use_std_norm
    )
    
    print(f"Rewards: {rewards}")
    print(f"Advantages: {advantages}")
    # 输出:
    # Rewards: tensor([1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0])
    # Advantages: tensor([0.375, -0.625, 0.375, -0.125, -0.5, -0.5, 0.5, 0.5])
```

---

> **"站在巨人的肩膀上，看得更远。"**
>
> 愿这份指南帮你构建LLM强化学习领域最全面的代码知识库！
