### 概念: 数据并行 (Data Parallelism)

#### 1. 核心定义

数据并行是分布式模型训练中最基础、最常见的并行化策略. 其核心思想是在集群中的每个计算设备 (GPU) 上都保留一份**完全相同**的模型副本, 然后将一个大的训练数据批次 (batch) 切分成多个小的微批次 (micro-batches), 每个设备独立处理一个微批次.

#### 2. 工作流程

一个典型的数据并行训练步骤如下:

1.  **分发 (Scatter)**: 将一个全局大批次 (Global Batch) 的数据样本分发给所有 N 个 GPU. 每个 GPU 获得 `Global Batch Size / N` 个样本.
2.  **前向传播 (Forward Pass)**: 每个 GPU 使用其本地的模型副本, 对分配给它的微批次数据进行前向计算, 得出损失 (loss).
3.  **反向传播 (Backward Pass)**: 每个 GPU 根据本地的损失, 计算出针对其模型副本的梯度. 此时, 每个 GPU 上的梯度是不同的, 因为它们处理的数据不同.
4.  **梯度同步 (Gradient Synchronization)**: 这是数据并行的关键步骤. 所有 GPU 参与一次**[All-Reduce](./Lecture7-Collective-Communication.md)**操作, 将各自计算出的梯度进行求和 (或求平均). 操作完成后, 每个 GPU 都拥有了基于整个全局批次计算出的、完全相同的梯度.
5.  **参数更新 (Parameter Update)**: 每个 GPU 使用同步后的梯度, 以完全相同的方式更新其本地的模型副本. 这保证了在下一次迭代开始时, 所有设备上的模型参数依然保持一致.

#### 3. 优缺点分析

- **优点**:
    - **概念简单**: 逻辑清晰, 容易理解和实现.
    - **计算扩展性好**: 只要全局批次大小足够大, 增加 GPU 数量就能近似线性地提升训练吞吐量.
    - **与模型架构解耦**: 不需要对模型结构进行修改, 可以通过一个简单的包装器 (wrapper) 应用于几乎任何神经网络.

- **缺点**:
    - **内存冗余**: 这是数据并行最大的瓶颈. 每个 GPU 都需要存储一份完整的模型参数、梯度以及优化器状态. 对于大型模型, **[Adam 优化器](./Lecture7-Adam-Optimizer.md)**的状态 (一阶和二阶矩) 会占用巨量内存, 往往是模型参数本身的数倍. 这导致内存使用量会随着 GPU 数量的增加而线性增长, 限制了能够训练的模型大小.
    - **通信开销**: 每次迭代都需要进行一次 All-Reduce 操作来同步梯度, 通信的数据量等于模型参数的总量. 当模型很大或网络带宽较低时, 这会成为性能瓶颈.
    - **依赖大批次**: 为了摊销通信开销并充分利用 GPU, 数据并行需要较大的全局批次. 然而, 批次大小的增加并非无限制, 超过某个“临界批次大小”后, 优化效率会进入递减回报区.

#### 4. 演进: 从朴素到 ZeRO/FSDP

为了克服内存冗余的缺点, 数据并行策略经历了重要的演进:

- **朴素数据并行 (DDP)**: 即上述的标准流程, 存在严重的内存冗余.
- **[ZeRO (Zero Redundancy Optimizer)](./Lecture7-ZeRO.md)**: 由微软提出的革命性优化. 通过将优化器状态、梯度甚至模型参数本身进行分片 (shard), 并巧妙地设计通信与计算的调度, **[ZeRO](./Lecture7-ZeRO.md)** 能够在保持数据并行简单性的同时, 大幅降低每个 GPU 的内存占用, 使得在同等硬件上能够训练远大于以往的模型.
- **[FSDP (Fully Sharded Data Parallel)](./Lecture7-FSDP.md)**: PyTorch 官方实现的、功能上等价于 ZeRO Stage 3 的数据并行方案, 现已成为社区的主流选择.