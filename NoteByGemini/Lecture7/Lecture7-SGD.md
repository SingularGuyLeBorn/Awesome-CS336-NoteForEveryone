### 概念: 随机梯度下降 (Stochastic Gradient Descent)

#### 1. 核心定义

随机梯度下降 (Stochastic Gradient Descent, SGD) 是一种用于优化 (通常是最小化) 可微函数的核心迭代算法. 在深度学习中, 它被广泛用于寻找神经网络模型参数的最优值, 以最小化损失函数 (Loss Function).

#### 2. 与其他梯度下降方法的区别

- **批量梯度下降 (Batch Gradient Descent, BGD)**:
    - **工作方式**: 在每次参数更新时, BGD 会计算**整个训练数据集**的平均梯度.
    - **优点**: 梯度是无偏的, 保证向着全局最优解的方向下降 (对于凸函数).
    - **缺点**: 计算成本极高. 对于大型数据集, 每次更新都需要遍历所有数据, 速度非常慢且内存消耗巨大.

- **随机梯度下降 (Stochastic Gradient Descent, SGD)**:
    - **工作方式**: 在每次参数更新时, SGD 只随机选择**一个**训练样本来计算梯度.
    - **优点**: 更新速度极快, 内存占用小.
    - **缺点**: 梯度估计的方差非常大, 导致收敛过程非常嘈杂, 可能会在最优点附近震荡而不是直接收敛.

- **小批量随机梯度下降 (Mini-batch SGD)**:
    - **工作方式**: 这是 BGD 和 SGD 的折中方案, 也是现代深度学习中**最常用**的方法. 它在每次更新时, 使用一小批 (mini-batch, 例如 32, 64, 128 个) 随机样本来计算平均梯度.
    - **优点**:
        - **降低方差**: 相比 SGD, 平均梯度更稳定, 收敛过程更平滑.
        - **计算效率**: 可以利用现代硬件 (如 GPU) 的并行计算能力, 高效处理小批量数据, 速度远快于 BGD.
        - **正则化效果**: 梯度中的噪声在一定程度上可以帮助优化过程跳出局部最小值.

在课程和日常讨论中, 当人们提到 "SGD" 时, 通常指的都是“小批量随机梯度下降”.

#### 3. 核心更新公式

对于模型参数 `θ`, 损失函数 `J(θ)`, 以及学习率 `η`, Mini-batch SGD 的更新规则如下:

$
\theta_{t+1} = \theta_t - \eta \cdot \nabla_{\theta} J(\theta_t; x^{(i:i+B)}; y^{(i:i+B)})
$

其中:
- $\theta_t$ 是在时间步 `t` 的参数.
- $B$ 是批次大小 (batch size).
- $x^{(i:i+B)}$ 和 $y^{(i:i+B)}$ 是从数据集中随机抽取的一批样本及其标签.
- $\nabla_{\theta} J$ 是损失函数关于参数 `θ` 的梯度, 在这个小批量上计算得出.

**[数据并行](./Lecture7-Data-Parallelism.md)**的本质就是将这个小批量 `B` 拆分到多个设备上并行计算梯度, 然后通过**[All-Reduce](./Lecture7-Collective-Communication.md)**聚合, 最终完成一次等效的 Mini-batch SGD 更新.