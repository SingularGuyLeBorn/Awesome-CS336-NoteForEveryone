### 概念: Adam 优化器 (Adam Optimizer)

#### 1. 核心定义

Adam (Adaptive Moment Estimation) 是一种基于梯度的优化算法, 广泛应用于深度学习模型的训练. 它结合了两种流行的梯度下降优化方法的优点: **动量 (Momentum)** 和 **RMSprop**. Adam 的核心特性是为模型中的**每个参数**计算自适应的学习率.

#### 2. 核心思想

Adam 旨在解决传统**[SGD](./Lecture7-SGD.md)**的两个主要问题:
1.  **学习率选择困难**: 一个固定的学习率可能对某些参数过大, 导致震荡; 对另一些参数又过小, 导致收敛缓慢.
2.  **梯度稀疏或噪声问题**: 在高维空间中, 梯度在不同方向上的大小可能差异巨大.

为了实现自适应学习率, Adam 维护了两个额外的变量, 它们分别是梯度的一阶矩 (均值) 和二阶矩 (非中心化方差) 的指数移动平均:

- **一阶矩 (First Moment), `m`**: 类似于动量, 它累积了过去梯度的指数衰减平均值. 这有助于加速在梯度方向一致的维度上的学习, 并抑制震荡.
    $m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t$

- **二阶矩 (Second Moment), `v`**: 类似于 RMSprop, 它累积了过去梯度平方的指数衰减平均值. 这可以衡量梯度的“变异性”或大小.
    $v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2$

其中, $g_t$ 是当前时间步的梯度, $\beta_1$ 和 $\beta_2$ 是衰减率 (通常接近 1, 如 0.9 和 0.999).

#### 3. 参数更新规则

在每次更新时, Adam 会对 `m` 和 `v` 进行偏差修正 (bias correction), 以解决它们在训练初期被初始化为零导致的偏差问题. 然后, 参数 `θ` 的更新规则如下:

$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \cdot \hat{m}_t
$

- $\hat{m}_t$ 和 $\hat{v}_t$ 是修正后的矩估计.
- $\eta$ 是全局学习率.
- $\epsilon$ 是一个很小的常数, 防止分母为零.

这个公式的核心在于分母 $\sqrt{\hat{v}_t}$. 如果某个参数的梯度历史平方和 (`v`) 很大, 意味着它的梯度要么很大要么很善变, 那么它的有效学习率就会变小; 反之, 如果梯度历史平方和很小, 它的有效学习率就会变大.

#### 4. 在分布式训练中的内存影响

Adam 优化器虽然高效, 但对内存的消耗非常大, 这是**[数据并行](./Lecture7-Data-Parallelism.md)**中的一个主要瓶颈. 对于模型中的每一个参数, Adam 需要额外存储:
- **一阶矩 `m`**: 通常与参数本身精度相同 (如 FP32).
- **二阶矩 `v`**: 通常与参数本身精度相同 (如 FP32).

此外, 在混合精度训练中, 通常还会保留一份 FP32 的主权重 (master weights). 这导致每个参数需要额外 8 到 12 字节的存储空间. 当模型参数达到数十亿时, 仅优化器状态就会占用数百 GB 的内存. 这也正是**[ZeRO](./Lecture7-ZeRO.md)**等技术首先致力于分片优化器状态的原因.