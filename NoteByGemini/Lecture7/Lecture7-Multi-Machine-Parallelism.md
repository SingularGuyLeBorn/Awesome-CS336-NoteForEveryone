### 概念: 多机并行化 (Multi-machine Parallelism)

#### 1. 核心定义

多机并行化是一种分布式计算策略, 指的是利用多台独立的计算机 (节点), 每台计算机可能包含一个或多个加速器 (如 GPU), 协同工作来训练一个单一的、巨大的机器学习模型. 当模型的规模 (参数量) 或训练数据量大到无法在单台机器上高效处理时, 这种策略就变得至关重要.

#### 2. 为何需要多机并行化?

- **内存限制 (Memory Constraints)**: 当前最先进的大型语言模型 (LLM) 拥有数百亿甚至上万亿的参数. 即使是拥有 80GB 显存的顶级 GPU, 也无法一次性装载整个模型及其优化器状态和梯度. 多机并行化通过将模型或数据切分到多个节点的内存中, 实现了内存容量的线性扩展.
- **计算限制 (Compute Constraints)**: 训练大模型需要海量的浮点运算 (FLOPs). 仅靠单台机器的算力, 完成一次训练可能需要数月甚至数年. 多机并行化通过汇集数百上千个 GPU 的算力, 将训练时间缩短到可接受的范围内 (几天或几周), 实现了计算能力的线性扩展.
- **通信开销 (Communication Overhead)**: 虽然并行化能带来巨大收益, 但也引入了新的挑战, 即机器间的通信开销. 如何设计高效的算法, 以最小化通信时间, 并使其与计算时间重叠, 是多机并行化研究的核心问题.

#### 3. 并行化的主要维度

多机并行化主要沿着以下三个维度展开, 它们可以独立使用, 也常常组合使用:

- **[数据并行 (Data Parallelism)](./Lecture7-Data-Parallelism.md)**: 在每个计算节点上都保留一份完整的模型副本, 但将训练数据集切分, 每个节点处理数据的一个子集. 节点间需要同步的是梯度信息.
- **[模型并行 (Model Parallelism)](./Lecture7-Model-Parallelism.md)**: 将模型本身进行切分, 不同的计算节点负责模型不同部分的计算. 节点间需要传递的是层与层之间的激活值或部分计算结果.
- **[激活并行 (Activation Parallelism)](./Lecture7-Activation-Parallelism.md)**: 专注于减少和分片在训练过程中产生的中间结果——激活值 (activations) 的内存占用. 这对于处理长序列输入尤为重要.

#### 4. 关键目标

一个成功的多机并行化策略旨在实现以下两个“线性扩展”目标:

1.  **线性内存扩展**: 随着 GPU 数量的增加, 能够训练的最大模型规模也随之线性增长.
2.  **线性计算扩展**: 随着 GPU 数量的增加, 训练模型的有效吞吐量 (FLOPs) 也随之线性增长, 即所谓的“加速比”接近理想状态.

最终, 多机并行化将整个数据中心抽象成一个巨大的、统一的计算单元, 使得训练千亿甚至万亿参数的模型成为可能.