### 课程前言: 从单机到数据中心的跨越

欢迎来到第七讲. 今天我们的焦点将从优化单个 GPU 的吞吐量, 转向一个更宏大、更复杂的领域: **[多机并行化](./Lecture7-Multi-Machine-Parallelism.md)**. 当模型规模增长到单个 GPU 无法容纳时, 我们必须将它们拆分到多台机器上, 并利用所有可用的计算资源来加速训练.

本讲将深入探讨在训练真正的大型语言模型 (LLM) 时所面临的计算、内存和通信三大挑战. 我们将从网络基础和**[集体通信操作](./Lecture7-Collective-Communication.md)**讲起, 分析不同的**[GPU 与 TPU 网络拓扑](./Lecture7-GPU-TPU-Networking.md)**如何影响并行策略的选择. 随后, 我们将系统性地剖析三种核心的并行化范式: **[数据并行](./Lecture7-Data-Parallelism.md)**, **[模型并行](./Lecture7-Model-Parallelism.md)**, 以及**[激活并行](./Lecture7-Activation-Parallelism.md)**. 我们将看到这些策略如何演进, 例如从朴素的数据并行发展到精巧的**[ZeRO](./Lecture7-ZeRO.md)**优化器.

最终, 我们将讨论如何将这些不同的策略组合成所谓的**[3D 并行](./Lecture7-3D-Parallelism.md)**, 并通过分析**[Megatron-LM](./Lecture7-Megatron-LM.md)**以及 Llama 3 等前沿模型的实际案例, 为你提供一套在实践中部署大规模分布式训练的经验法则. 我们的目标是将整个数据中心视为一个新的计算单元, 实现模型大小和训练速度的线性扩展.

### 正文

#### 第一部分: 分布式训练的基础设施

##### 1.1 挑战: 计算与内存的瓶颈

训练尖端大模型有两个主要驱动因素. 从计算角度看, 单个 GPU 的性能虽然在飞速增长, 但仍不足以满足快速训练的需求. 世界上最快的超级计算机拥有百亿亿次级 (Exaflops) 的计算能力, 训练顶级模型必须依赖这样的资源.

从内存角度看, 模型的参数量已达数十亿甚至更多, 远超单个 GPU 的内存容量. 例如, 一个 75 亿参数的模型, 如果采用朴素的**[数据并行](./Lecture7-Data-Parallelism.md)**策略, 在 64 个加速器上运行时, 总内存占用会随着 GPU 数量线性增长, 这是不可接受的. 因此, 我们必须找到方法来扩展计算和内存, 这就引出了**[多机并行化](./Lecture7-Multi-Machine-Parallelism.md)**.

##### 1.2 硬件层级: 节点内与节点间的通信差异

在典型的计算集群中, 硬件并非同质. 一台物理服务器 (节点) 内通常包含多个 GPU (例如 8 个), 它们通过 NVLink 和 NVSwitch 等高速互连技术连接, 实现极快的内部通信. 然而, 当数据需要跨越节点传输时, 就必须通过 InfiniBand 等速度慢得多的网络交换机. 这种节点内快、节点间慢的硬件层级结构, 对我们设计并行化策略具有深远影响. 例如, 带宽需求极高的**[张量并行](./Lecture7-Tensor-Parallelism.md)**通常被限制在单个节点内部署.

##### 1.3 集体通信原语

分布式算法的许多核心操作都可以被分解为一组简单的**[集体通信操作](./Lecture7-Collective-Communication.md)**. 理解这些原语对于分析并行算法的性能至关重要.
- **All-Reduce**: 将所有节点的数据进行规约 (如求和), 然后将结果分发回所有节点. 这是数据并行中同步梯度的核心操作.
- **Reduce-Scatter**: 对数据进行规约, 然后将结果的不同分片 (shard) 发送给不同的节点.
- **All-Gather**: 每个节点将其拥有的数据分片广播给所有其他节点.

一个至关重要的等价关系是: **一次 All-Reduce 操作在带宽受限的情况下, 其通信成本等价于一次 Reduce-Scatter 加上一次 All-Gather**. 这个恒等式是理解**[ZeRO](./Lecture7-ZeRO.md)**优化器“零开销”特性的关键.

#### 第二部分: 三大并行化策略详解

##### 2.1 数据并行 (Data Parallelism)

**[数据并行](./Lecture7-Data-Parallelism.md)**是最直观的策略: 将模型的完整副本分发到每个 GPU 上, 然后将数据批次 (batch) 切分, 每个 GPU 处理一小部分数据.
- **朴素实现**: 每个 GPU 计算出本地数据的梯度后, 通过一次 All-Reduce 操作同步所有梯度, 然后在每个设备上更新模型参数.
- **计算扩展**: 只要批次大小足够大, 就能很好地利用多 GPU 的计算能力.
- **内存瓶颈**: 内存扩展性极差. 每个 GPU 都需要存储完整的模型参数、梯度, 以及非常消耗内存的**[Adam 优化器](./Lecture7-Adam-Optimizer.md)**状态. 详尽的**[模型内存计算](./Lecture7-Model-Memory-Calculation.md)**表明, 优化器状态可能占用比模型参数本身多几倍的内存.

为了解决内存瓶颈, **[ZeRO](./Lecture7-ZeRO.md)** (零冗余优化器) 应运而生.
- **ZeRO-1 (优化器状态分片)**: 利用 `Reduce-Scatter` + `All-Gather` 替代 `All-Reduce`, 巧妙地在两个通信步骤之间插入参数更新计算. 这使得每个 GPU 只需存储优化器状态的一个分片, 极大地降低了内存占用, 且在通信成本上几乎是“免费”的.
- **ZeRO-2 (梯度分片)**: 在 ZeRO-1 的基础上, 进一步对梯度进行分片. 在反向传播过程中, 每计算完一层的梯度就立即通过 Reduce 操作将其发送到对应的 GPU, 从而避免在任何时刻实例化完整的梯度向量.
- **ZeRO-3 / [FSDP](./Lecture7-FSDP.md)**: 极致的分片策略, 将模型参数、梯度和优化器状态全部进行分片. 在前向和反向传播过程中, 按需通过 All-Gather 请求当前计算层所需的参数, 并在计算完成后立即释放. 通过**[通信与计算重叠](./Lecture7-Communication-Computation-Overlap.md)**的技术, FSDP 能以惊人的低开销实现内存占用的线性扩展.

##### 2.2 模型并行 (Model Parallelism)

当模型大到无法在单个 GPU 中容纳时 (即使使用 FSDP), 我们就需要**[模型并行](./Lecture7-Model-Parallelism.md)**. 其核心思想是将模型本身切分到不同设备上, 设备间传递的是激活值而非参数.

- **[流水线并行 (Pipeline Parallelism)](./Lecture7-Pipeline-Parallelism.md)**:
    - **原理**: 按层切分模型, 每个 GPU 负责一部分连续的层. GPU 之间依次传递前向传播的激活值和反向传播的梯度.
    - **挑战**: 朴素的实现会导致严重的 GPU 空闲, 形成所谓的**[流水线气泡](./Lecture7-Pipeline-Bubble.md)**.
    - **优化**: 通过将大批次切分为多个微批次 (micro-batch) 并行处理, 可以显著减小气泡, 但这会消耗宝贵的“批次大小”资源. 更高级的技术如“零气泡流水线”通过重新调度计算依赖, 能进一步提升效率, 但实现极为复杂.
    - **适用场景**: 通常用于节点间较慢的网络连接, 因为它只传递激活值, 通信量相对较小.

- **[张量并行 (Tensor Parallelism)](./Lecture7-Tensor-Parallelism.md)**:
    - **原理**: 在更细的粒度上切分模型, 即对单个大矩阵乘法 (如 Transformer 中的权重矩阵) 进行并行化. 将权重矩阵按列或按行切分, 分配给不同 GPU, 然后通过集体通信 (如 All-Reduce) 合并计算结果.
    - **优势**: 不会产生流水线气泡, 且对批次大小没有依赖. 实现相对简单.
    - **劣势**: 通信开销巨大, 每层都需要进行 All-Reduce 同步, 强依赖于低延迟、高带宽的节点内互连.
    - **适用场景**: 通常在单个节点内的 8 个 GPU 之间使用, 超过此范围性能会急剧下降.

##### 2.3 激活并行与序列并行

即使使用了模型并行, **激活值 (activations)** 的内存占用仍可能成为瓶颈, 尤其是在长序列场景下.
- **激活内存分析**: Transformer 每层的激活内存由两部分组成: 与 MLP 和逐点操作相关的线性项, 以及与注意力计算相关的二次项.
- **[序列并行 (Sequence Parallelism)](./Lecture7-Sequence-Parallelism.md)**: **[张量并行](./Lecture7-Tensor-Parallelism.md)**可以减少大部分与矩阵乘法相关的激活内存, 但对于 LayerNorm, Dropout 等逐点 (point-wise) 操作无能为力. 序列并行沿序列长度维度对这些操作进行切分, 使得所有激活内存都能随着并行度的增加而线性减少.
- **[激活重计算 (Activation Recomputation)](./Lecture7-Activation-Recomputation.md)**: 类似于 FlashAttention 的思想, 在反向传播时重新计算前向传播的中间激活值, 而不是全部存储它们. 这是一种以计算换内存的策略, 可以极大降低激活内存峰值.

#### 第三部分: 组合策略与实践案例

##### 3.1 3D 并行: 组合的力量

在实际的大规模训练中, 单一的并行策略往往不够. 人们通常会组合使用数据并行、流水线并行和张量并行, 形成所谓的**[3D 并行](./Lecture7-3D-Parallelism.md)**.
- **经验法则**:
    1.  **首要目标: 装下模型**.
        -   首先在节点内部署**[张量并行](./Lecture7-Tensor-Parallelism.md)**, 直到 GPU 上限 (通常是 8 路).
        -   如果模型仍然太大, 则在节点间部署**[流水线并行](./Lecture7-Pipeline-Parallelism.md)** (或根据带宽情况选择 **[FSDP](./Lecture7-FSDP.md)**), 直到模型可以完全装入所有 GPU 内存.
    2.  **次要目标: 提升吞吐量**.
        -   利用剩余的所有 GPU, 部署**[数据并行](./Lecture7-Data-Parallelism.md)**来扩展计算吞吐量.
- **批次大小的角色**: 批次大小是一种有限资源. 它可以用于数据并行以提升优化效率, 也可以用于流水线并行以减小气泡. 如何分配这一资源取决于具体的瓶颈.

##### 3.2 前沿模型案例分析

- **Megatron-LM**: 经典论文详细展示了 3D 并行的有效性. 随着模型从 17 亿参数扩展到 1 万亿参数, 策略上先用满 8 路张量并行, 然后根据需要增加流水线并行度, 最后用数据并行扩展到数千个 GPU, 实现了总吞吐量的线性增长.
- **Llama 3 (405B)**: 训练报告明确指出其并行策略顺序为: 张量并行(TP) -> 上下文并行(CP) -> 流水线并行(PP) -> 数据并行(DP, 即 FSDP). 这个顺序正是根据不同策略对网络带宽的需求从高到低排列的.
- **DeepSeek & Yi**: 这些模型普遍采用 ZeRO Stage 1 结合张量并行和流水线并行的策略. 对于**[混合专家模型 (MoE)](./Lecture7-Mixture-of-Experts.md)**, 则使用**[专家并行](./Lecture7-Expert-Parallelism.md)**替代张量并行.
- **Gemini**: 在 TPU 集群上, 采用了 FSDP (ZeRO-3), 模型并行和数据并行的组合. TPU 的环形网格拓扑使其在模型并行方面能扩展到更大规模.

### 课程总结

训练大规模语言模型是一个复杂的系统工程, 不存在单一的“银弹”式并行方案. 我们必须成为一名善于权衡的架构师, 理解数据并行、模型并行和激活并行各自的优劣, 并根据硬件特性、模型大小和批次大小等约束, 巧妙地将它们组合起来. 本讲提供的经验法则和案例分析, 为你在实践中驾驭大规模分布式训练提供了清晰的路线图.

### 拓展阅读

为了更深入地理解本讲内容, 建议采用以下学习路径:

1.  **建立理论基础**: 首先阅读**[多机并行化](./Lecture7-Multi-Machine-Parallelism.md)**笔记, 建立对大规模训练挑战的宏观认识. 接着, 深入学习**[集体通信操作](./Lecture7-Collective-Communication.md)**, 这是理解所有并行算法性能的关键.

2.  **深入数据并行**: 从最基础的**[数据并行](./Lecture7-Data-Parallelism.md)**开始, 重点理解其内存瓶颈. 然后, 依次学习**[ZeRO](./Lecture7-ZeRO.md)**的三个阶段, 特别是 Stage 1 如何利用通信恒等式实现“免费”的内存优化, 以及**[FSDP](./Lecture7-FSDP.md)**如何通过**[通信与计算重叠](./Lecture7-Communication-Computation-Overlap.md)**实现极致的内存节省.

3.  **探索模型并行**: 当理解了数据并行的局限后, 转向**[模型并行](./Lecture7-Model-Parallelism.md)**.
    -   先学习**[流水线并行](./Lecture7-Pipeline-Parallelism.md)**, 并仔细研究**[流水线气泡](./Lecture7-Pipeline-Bubble.md)**的概念, 理解其效率与批次大小的关系.
    -   然后学习**[张量并行](./Lecture7-Tensor-Parallelism.md)**, 对比它与流水线并行在通信开销和实现复杂度上的差异.

4.  **最终优化**: 在掌握了参数和梯度的并行化之后, 关注**[激活并行](./Lecture7-Activation-Parallelism.md)**和**[序列并行](./Lecture7-Sequence-Parallelism.md)**, 了解如何处理激活值这一最后的内存瓶颈.

5.  **综合应用**: 最后, 阅读**[3D 并行](./Lecture7-3D-Parallelism.md)**和**[Megatron-LM](./Lecture7-Megatron-LM.md)**的笔记, 看看工业界是如何将上述所有技术整合起来, 解决实际问题的.