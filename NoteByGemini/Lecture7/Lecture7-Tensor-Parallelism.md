### 概念: 张量并行 (Tensor Parallelism)

#### 1. 核心定义

张量并行是一种**[模型并行](./Lecture7-Model-Parallelism.md)**技术, 它在比层更细的粒度上对模型进行切分. 其核心思想是**并行化模型中的单个大张量运算**, 通常是矩阵乘法 (MatMul). 它沿着模型的**宽度 (width)**维度进行切分, 而非**[流水线并行](./Lecture7-Pipeline-Parallelism.md)**的深度维度.

#### 2. Megatron-LM 中的张量并行

NVIDIA 的 Megatron-LM 框架是张量并行的经典实现, 下面以一个 Transformer 模型中的 MLP (多层感知机) 模块为例解释其工作原理.

一个标准的 MLP 模块包含两次线性变换和一个非线性激活, 可表示为: `Y = GeLU(X*A)*B`.
其中 `X` 是输入, `A` 和 `B` 是权重矩阵.

- **切分策略**:
    - 矩阵 `A` 按**列**切分: `A = [A1, A2]`
    - 矩阵 `B` 按**行**切分: `B = [B1; B2]` (分号表示垂直堆叠)
    - GPU 1 存储 `A1` 和 `B1`, GPU 2 存储 `A2` 和 `B2`.

- **前向传播**:
    1.  **f (Identity)**: 输入 `X` 被复制到两个 GPU 上.
    2.  **并行计算**:
        -   GPU 1 计算: `Y1 = GeLU(X * A1)`
        -   GPU 2 计算: `Y2 = GeLU(X * A2)`
    3.  **并行计算**:
        -   GPU 1 计算: `Z1 = Y1 * B1`
        -   GPU 2 计算: `Z2 = Y2 * B2`
    4.  **g (All-Reduce)**: 对 `Z1` 和 `Z2` 执行一次 All-Reduce 操作, 将它们相加得到最终结果 `Z = Z1 + Z2`. 这个结果在两个 GPU 上都是相同的.

- **反向传播**:
    过程正好相反. 梯度的流动从 `Z` 开始, 经过 `B` 的反向计算时, 梯度被复制 (Identity). 在经过 `A` 的反向计算后, 输入 `X` 的梯度需要通过一次 All-Reduce 进行求和.

#### 3. 优缺点与适用场景

- **优点**:
    - **无流水线气泡**: 由于计算是同步进行的, 不存在**[流水线并行](./Lecture7-Pipeline-Parallelism.md)**中的设备空闲问题, 理论上可以实现更高的 GPU 利用率.
    - **对批次大小不敏感**: 其效率不直接依赖于批次大小, 这使得它成为一种不消耗“批次大小”资源的并行策略.
    - **实现相对直接**: 核心是替换模型中的特定模块 (如 `Linear` 层), 逻辑比复杂的流水线调度要简单.

- **缺点**:
    - **通信开销巨大**: 在每个 Transformer 层的 MLP 和注意力模块中, 都需要执行一次或多次 All-Reduce 操作. 这类操作要求所有参与的 GPU 进行同步和大量数据交换, 对网络带宽和延迟要求极高.
    - **扩展性受限**: 由于其高昂的通信成本, 张量并行的性能会随着 GPU 数量的增加和网络质量的下降而急剧恶化.

- **适用场景**:
    张量并行几乎只用于**单个服务器节点内部 (intra-node)**, 利用 NVLink/NVSwitch 提供的高速、低延迟互连. 在实践中, 张量并行的路数通常被限制在 2 路、4 路或 8 路. 在**[3D 并行](./Lecture7-3D-Parallelism.md)**策略中, 它是在最内层、硬件连接最紧密的地方使用的第一道防线.