### 概念: 专家并行 (Expert Parallelism)

#### 1. 核心定义

专家并行是一种专门用于**[混合专家模型 (Mixture of Experts - MoE)](./Lecture7-Mixture-of-Experts.md)** 的**[模型并行](./Lecture7-Model-Parallelism.md)**策略. 在 MoE 架构中, 一个标准的 MLP (或 FFN) 层被替换为多个并行的、功能相同的“专家”网络 (Experts) 和一个控制流量的“路由器” (Router). 专家并行的核心思想是将这些专家网络**物理地分布**在不同的计算设备 (GPU) 上.

#### 2. 工作流程

1.  **专家分布**: 假设有 `E` 个专家和 `D` 个设备, 每个设备会托管 `E/D` 个专家网络. 例如, 如果有 64 个专家和 8 个 GPU, 那么每个 GPU 会负责 8 个专家.

2.  **路由 (Routing)**:
    - 当一批 token 的隐藏状态输入到 MoE 层时, 路由器 (通常是一个小的线性层) 会为每个 token 计算一组权重, 决定将该 token 发送给哪些专家进行处理 (通常是 top-k, k=1 或 2).
    - 这意味着输入数据需要根据路由器的决策, 被动态地发送到持有相应专家的 GPU 上.

3.  **All-to-All 通信**:
    - 路由决策完成后, 会触发一次 `All-to-All` 的**[集体通信操作](./Lecture7-Collective-Communication.md)**.
    - 在这个操作中, 每个 GPU 将其本地的 token 按照它们的目标专家进行打包, 然后将这些数据包发送到对应的 GPU. 同时, 它也会从其他所有 GPU 接收发往其本地专家的 token.
    - 例如, GPU 0 会把它需要发送给专家 8-15 (位于 GPU 1 上) 的所有 token 收集起来, 发送给 GPU 1; 同时接收来自所有其他 GPU 的、需要由专家 0-7 (位于 GPU 0 上) 处理的 token.

4.  **专家计算**: 通信完成后, 每个 GPU 在其本地的专家网络上, 对接收到的 token 进行计算.

5.  **结果返回**: 计算完成后, 再次通过一次 `All-to-All` 通信, 将处理后的结果发送回它们最初来源的 GPU, 以便恢复原始的序列顺序.

#### 3. 与张量并行的对比

专家并行在概念上与**[张量并行](./Lecture7-Tensor-Parallelism.md)**有相似之处, 因为它们都涉及到切分一个大的计算模块 (MLP) 并通过集体通信来聚合结果. 但存在关键区别:

- **激活模式**: 张量并行是**密集 (dense)** 的, 所有输入都会作用于所有切分的矩阵块. 而专家并行是**稀疏 (sparse)** 的, 每个输入 token 只会激活一小部分专家.
- **通信模式**: 张量并行通常使用 `All-Reduce`, 通信模式是固定的. 专家并行使用 `All-to-All`, 通信的内容和模式是**动态的**, 取决于路由器的实时决策.
- **负载均衡**: 在张量并行中, 负载是天然均衡的. 在专家并行中, 可能会出现**负载不均**的问题, 即某些专家接收到的 token 远多于其他专家, 导致部分 GPU 过载而另一部分空闲. 这通常需要通过引入额外的损失函数 (load balancing loss) 来缓解.

#### 4. 意义

专家并行是实现 MoE 模型高效训练的基础. 它允许模型在参数量上进行极大的扩展 (通过增加专家数量), 而计算量 (FLOPs) 却只随着 `k` (每个 token 激活的专家数) 线性增长. 这种“用稀疏激活换取模型容量”的特性, 使得 MoE 成为当前构建最高效、最强大的大型语言模型的关键架构之一.