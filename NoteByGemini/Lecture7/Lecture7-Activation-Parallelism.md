### 概念: 激活并行 (Activation Parallelism)

#### 1. 核心定义

激活并行是一组旨在减少或分片在模型训练期间产生的**激活值 (activations)**内存占用的并行化技术. 激活值是在前向传播过程中, 模型每一层计算产生的中间结果, 它们需要被存储起来以备反向传播时计算梯度使用. 对于拥有长序列、大隐藏维度或大量层数的模型, 激活值的总大小可能非常庞大, 甚至超过模型参数本身, 成为内存瓶颈.

#### 2. 为何需要激活并行?

传统的**[数据并行](./Lecture7-Data-Parallelism.md)**完全不减少激活内存. 而**[模型并行](./Lecture7-Model-Parallelism.md)** (包括流水线和张量并行) 虽然可以分片参数, 但对激活内存的削减并不彻底.
- **[流水线并行](./Lecture7-Pipeline-Parallelism.md)**: 每个 GPU 只负责一部分层, 因此自然地分片了激活, 但总的激活内存并未减少.
- **[张量并行](./Lecture7-Tensor-Parallelism.md)**: 可以线性减少与矩阵乘法相关的激活内存, 但对于像 LayerNorm, Dropout, 残差连接等逐点 (point-wise) 操作产生的激活无能为力. 这些未被并行的激活内存会随着模型规模继续增长.

因此, 需要专门的技术来处理这些“并行化盲区”, 实现激活内存的完全线性扩展.

#### 3. 主要技术

- **[序列并行 (Sequence Parallelism)](./Lecture7-Sequence-Parallelism.md)**:
    - **核心思想**: 许多未能被张量并行处理的操作 (如 LayerNorm) 在序列维度上是独立的. 即, 对序列中第 `i` 个 token 的计算不依赖于第 `j` 个 token.
    - **实现方式**: 利用这一特性, 将输入序列在序列维度上切分, 分配给不同的 GPU. 每个 GPU 只处理序列的一个片段. 对于那些需要在整个序列上进行归约的层 (如注意力层), 则需要在序列并行和张量并行之间进行**[集体通信操作](./Lecture7-Collective-Communication.md)** (如 All-Gather 和 Reduce-Scatter) 来转换数据布局.
    - **效果**: 通过将最后的并行化盲点——逐点操作——沿序列维度切分, 实现了几乎所有激活内存都可以随张量并行度 `t` 而缩减 `1/t`.

- **[激活重计算 (Activation Recomputation)](./Lecture7-Activation-Recomputation.md)** (也称梯度检查点 Gradient Checkpointing):
    - **核心思想**: 这是一种典型的以计算换内存的策略. 它选择性地在前向传播过程中**不存储**某些计算开销较大或占用内存较多的层的激活值.
    - **实现方式**: 在反向传播需要这些被丢弃的激活值时, 它会从最近的一个检查点 (被存储的激活值) 开始, **重新执行一次部分前向传播**来动态地计算出它们.
    - **效果**: 可以显著降低激活内存的峰值, 允许使用更大的批次大小或更长的序列长度进行训练. 其代价是增加了额外的计算量 (大约是一次额外的前向传播). FlashAttention 就是激活重计算在注意力机制中的一个经典应用.

通过结合使用序列并行和激活重计算, 可以将激活内存的占用控制在非常低的水平, 这对于训练具有极长上下文窗口的大型模型至关重要.