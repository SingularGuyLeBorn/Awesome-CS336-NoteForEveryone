### 概念: 上下文并行 / 环形注意力 (Context Parallel / Ring Attention)

#### 1. 核心定义

上下文并行 (Context Parallelism), 其最著名的实现是**环形注意力 (Ring Attention)**, 是一种专门为处理**超长序列 (very long contexts)** 而设计的并行化策略. 当序列长度 `N` 变得极大时 (例如, 超过单个 GPU 的内存容量), 标准的自注意力机制 `O(N^2)` 的内存和计算开销会变得难以承受. 上下文并行通过在设备间**分割序列**并以一种协同的方式传递键 (Keys) 和值 (Values), 使得在分布式环境中能够计算全局注意力, 而无需在任何单个设备上实例化完整的注意力矩阵.

#### 2. 问题背景: 长序列的挑战

- **内存瓶颈**: 标准注意力机制需要计算一个大小为 `(N, N)` 的注意力分数矩阵. 当 `N` 达到数十万时, 仅这个矩阵的内存占用就会达到数百 GB, 远超单个 GPU 显存.
- **计算瓶颈**: 注意力计算的复杂度是 `O(N^2)`, 计算量随序列长度二次增长.
- **传统并行的局限**:
    - **[张量并行](./Lecture7-Tensor-Parallelism.md)**: 沿隐藏维度 `d` 或头数 `h` 切分, 无法解决 `N` 带来的问题.
    - **[序列并行](./Lecture7-Sequence-Parallelism.md)**: 虽然可以切分序列, 但在计算注意力时, 为了得到全局信息, 仍然需要在某处 (通过 All-Gather) 重建完整的键和值, 导致巨大的通信和临时内存开销.

#### 3. 环形注意力的工作原理

环形注意力的灵感来源于 FlashAttention 的**分块 (tiling/blocking)** 计算思想, 并将其扩展到了分布式环境.

1.  **序列分片**: 将长度为 `N` 的序列切分成 `D` 个块 (D 为设备数), 每个设备 `i` 负责一个查询块 `Q_i`, 以及对应的键 `K_i` 和值 `V_i`.

2.  **环形传递 (Ring-wise Communication)**:
    - **步骤 0**: 每个设备 `i` 使用本地的 `Q_i` 和 `K_i`, `V_i` 计算一个局部的注意力块 `Attention(Q_i, K_i)V_i`.
    - **步骤 1**: 设备 `i` 将自己的 `(K_i, V_i)` 块发送给环上的下一个设备 `(i+1) mod D`. 同时, 从上一个设备 `(i-1) mod D` 接收 `(K_{i-1}, V_{i-1})` 块.
    - **步骤 2**: 设备 `i` 使用本地的 `Q_i` 和新收到的 `K_{i-1}, V_{i-1}` 计算另一个注意力块, 并在线上 (online) 更新其注意力输出.
    - **重复**: 这个传递-计算的过程重复 `D-1` 次, 直到每个设备的查询块 `Q_i` 都与所有其他的键/值块 `K_j, V_j` (j=0...D-1) 作用过.

3.  **在线更新**: 类似于 FlashAttention, 每个设备在计算过程中都维护着当前的注意力输出和归一化统计量 (最大值和指数和), 每次收到新的键/值块时, 都会对结果进行在线更新.

#### 4. 效果与意义

- **打破单设备内存限制**: 环形注意力成功地将注意力的 `O(N^2)` 内存开销**分摊**到了 `D` 个设备上, 每个设备的峰值激活内存仅为 `O(N^2 / D)`. 这使得理论上可以处理任意长度的序列, 只要有足够的设备.
- **计算与通信并行**: 在环形传递的过程中, 计算和通信可以重叠进行, 掩盖了大部分网络延迟.
- **长上下文训练的基石**: 它是 Llama 3 等模型实现超长上下文窗口 (如 131k token) 训练的核心技术之一.

总而言之, 上下文并行/环形注意力是将 FlashAttention 的思想从单 GPU 推广到多 GPU 的杰作, 它通过巧妙的分布式算法, 解决了注意力机制在长序列场景下的核心扩展性难题.