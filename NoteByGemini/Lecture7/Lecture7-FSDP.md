### 概念: FSDP (Fully Sharded Data Parallel)

#### 1. 核心定义

FSDP (Fully Sharded Data Parallel) 是 PyTorch 官方提供的、功能上等价于 **[ZeRO](./Lecture7-ZeRO.md)** Stage 3 的高级**[数据并行](./Lecture7-Data-Parallelism.md)**实现. 它是一种极致的内存优化策略, 旨在通过将模型的所有状态——包括**参数 (Parameters)**、**梯度 (Gradients)** 和**优化器状态 (Optimizer States)**——在数据并行组内的所有 GPU 上进行分片 (sharding), 从而最大限度地减少每个 GPU 上的内存占用.

#### 2. 核心工作机制

与传统数据并行 (DDP) 中每个 GPU 都持有一份完整的模型副本不同, FSDP 的工作机制要精巧得多:

1.  **初始化**: 在训练开始时, 完整的模型参数被分割成 N 份 (N 是数据并行组的大小), 每个 GPU 只拥有并负责其中的 1/N 份. 梯度和优化器状态也同样被分片.

2.  **前向传播 (Forward Pass)**:
    - 当需要计算模型中的某一层 (或一个 FSDP 包装的模块) 时, FSDP 会触发一次**[All-Gather](./Lecture7-Collective-Communication.md)**操作.
    - 所有 GPU 将它们各自拥有的该层参数分片广播给彼此, 使得每个 GPU **临时地**重建出该层的完整参数.
    - 执行该层的前向计算.
    - 计算完成后, 每个 GPU 会**立即丢弃**不属于自己负责的那 (N-1)/N 部分参数, 将内存释放出来, 只保留自己的分片.
    - 这个过程逐层进行, 直至前向传播完成.

3.  **反向传播 (Backward Pass)**:
    - 过程与前向传播类似但方向相反. 在计算某一层的梯度之前, 同样通过 All-Gather 临时重建该层的完整参数.
    - 计算完成后, 得到完整的梯度. 此时, FSDP 会执行一次**[Reduce-Scatter](./Lecture7-Collective-Communication.md)**操作.
    - 这个操作将完整的梯度进行求和, 然后将结果分片, 每个 GPU 只接收并存储自己需要负责的那部分梯度, 并立即丢弃其余部分.

4.  **参数更新 (Optimizer Step)**:
    - 每个 GPU 使用其本地存储的梯度分片和优化器状态分片, 来更新它所负责的那一小部分模型参数.

#### 3. 关键优势: 通信与计算重叠

你可能会认为这种频繁的 All-Gather 和 Reduce-Scatter 会导致巨大的性能开销. FSDP 的高效之处在于它能够巧妙地实现**[通信与计算重叠](./Lecture7-Communication-Computation-Overlap.md)**.
- **预取 (Prefetching)**: FSDP 不会等到计算当前层时才去请求参数. 它会在 GPU 正在计算第 `L` 层的同时, 在后台异步地发起对第 `L+1` 层参数的 All-Gather 请求.
- **掩盖延迟**: 通过这种方式, 当 GPU 完成第 `L` 层的计算时, 第 `L+1` 层的参数很可能已经被传输完毕, GPU 可以无缝衔接地开始下一次计算, 从而将大部分通信延迟隐藏在计算时间之后.

#### 4. 总结

FSDP 是一种强大而复杂的并行策略, 它提供了:
- **极致的内存效率**: 使得在有限的 GPU 内存上训练前所未有的大模型成为可能.
- **保持数据并行的简单性**: 对于用户来说, 通常只需要用 FSDP 包装器来包裹模型或其子模块即可, 无需深入修改模型内部逻辑.
- **高效的性能**: 尽管通信模式更复杂, 但通过通信与计算的重叠, FSDP 仍然能达到很高的训练吞吐量.

FSDP (以及其思想源头 ZeRO-3) 是当前大规模语言模型训练中, 解决内存瓶颈的主流和首选方案.