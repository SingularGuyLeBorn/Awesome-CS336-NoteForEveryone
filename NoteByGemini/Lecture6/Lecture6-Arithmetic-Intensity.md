### 模板A: 核心概念

#### 1. 这是什么？(What is it?)

**算术强度 (Arithmetic Intensity)** 是衡量一个计算任务中计算量与内存通信量比例的指标. 它被精确地定义为:

**算术强度 = 总浮点运算次数 (FLOPs) / 总内存访问字节数 (Bytes)**

这个指标是性能分析中的一个核心概念, 尤其是在 **[GPU 架构](./Lecture6-GPU-Architecture.md)** 这种计算能力远超内存带宽的硬件上. 

#### 2. 为什么重要？(Why is it important?)

算术强度直接决定了一个程序是[计算密集型 (Compute-Bound)](./Lecture6-Memory-vs-Compute-Bound.md)**还是**[内存密集型 (Memory-Bound)](./Lecture6-Memory-vs-Compute-Bound.md), 从而指明了性能优化的方向. 

* 如果一个任务的**算术强度高**, 意味着每从内存中取一个字节的数据, 都能进行大量的计算. 此时, 程序的性能瓶颈在于 GPU 的计算单元速度. 
* 如果一个任务的**算术强度低**, 意味着刚从内存中取出数据, 还没进行多少计算, 就又要写回内存或读取新的数据. 此时, 程序的性能瓶颈在于内存带宽. 

现代 GPU 的计算性能(以 TFLOPS 衡量)增长速度远远快于内存带宽(以 GB/s 衡量)的增长. 因此, 大多数深度学习中的操作, 除了经过高度优化的矩阵乘法外, 本质上都是内存密集型的. 我们进行**[算子融合](./Lecture6-Kernel-Fusion.md)**、**[分块](./Lecture6-Matrix-Multiplication-Tiling.md)**等优化的根本目的, 就是**提高程序的有效算术强度**. 

#### 3. 它是如何工作的？(How does it work?)

我们可以通过一个简单的例子来理解算术强度:

**示例 1:向量加法 `y = x + z`**

* 假设 `x`, `y`, `z` 都是长度为 N 的 FP32 向量. 
* **内存访问**:需要读取 `x` (4N 字节), 读取 `z` (4N 字节), 写入 `y` (4N 字节). 总共 `12N` 字节. 
* **计算量**:进行了 N 次加法运算, 即 `N` FLOPs. 
* **算术强度** = N / 12N = 1/12. 这是一个极低的算术强度, 因此向量加法是典型的**内存密集型**操作. 

**示例 2:矩阵乘法 `C = A * B`**

* 假设 A, B, C 都是 N x N 的 FP32 矩阵. 
* **内存访问 (朴素实现)**:读取 A (4N² 字节), 读取 B (4N² 字节), 写入 C (4N² 字节). 总共 `12N²` 字节. 
* **计算量**:大约是 `2N³` FLOPs. 
* **算术强度** = 2N³ / 12N² = N/6. 算术强度与 N 成正比. 当 N 很大时, 矩阵乘法是典型的**计算密集型**操作. 

通过**[算子融合](./Lecture6-Kernel-Fusion.md)**, 我们可以将多个低算术强度的操作合并成一个高算术强度的操作. 例如, `y = gelu(x)` 如果分步执行, 会涉及多次内存读写; 而融合成一个 Kernel 后, 只需一次读和一次写, 中间结果都保存在高速的寄存器中, 有效算术强度大大提高. 

#### 4. 关键要点 (Key Takeaways)

* 算术强度是**计算量与访存量的比值**, 是判断性能瓶颈的关键指标. 
* **优化目标**:通过算法和编程技巧(如融合、分块)来**提高有效算术强度**, 将内存密集型任务转化为计算密集型任务. 
* 在 GPU 编程中, 应时刻思考:“我如何能让加载到高速内存中的每个数据参与尽可能多的计算？”
  --- END OF file ---

--- FILE: Lecture6-Benchmarking.md ---

### 模板C: 方法/流程

#### 1. 目标 (Objective)

**性能评测 (Benchmarking)** 的核心目标是精确测量一段代码或一个操作端到端的执行时间(通常是墙上时钟时间 Wall-clock time). 它是一个宏观的性能度量工具, 用于回答“这个函数运行需要多长时间？”这类问题. 虽然它不提供内部细节, 但在以下场景中至关重要:

* **比较实现**:判断不同算法或代码实现(例如, 手动 PyTorch vs. **[Triton](./Lecture6-Triton.md)** vs. **[CUDA](./Lecture6-CUDA.md)**)的优劣. 
* **扩展性分析**:了解程序的性能如何随着输入规模(如矩阵维度、批次大小)的变化而变化. 
* **回归测试**:确保代码变更没有引入性能衰退. 

#### 2. 核心步骤与最佳实践 (Steps & Best Practices)

一个可靠的 GPU 性能评测流程应包含以下关键步骤, 这些都在课程的 **[`benchmark` 函数](./Lecture6-Code-benchmark.md)** 中得到了体现:

1. **预热 (Warm-up)**:

   * **原因**:GPU 程序首次运行时, 会产生额外的一次性开销, 包括 CUDA 上下文初始化、JIT 编译、数据首次传输等. 这些开销会严重扭曲计时结果, 使其看起来比实际稳态运行时慢得多. 
   * **做法**:在正式计时前, 将需要评测的函数完整运行数次(例如 1-5 次). 这确保了我们测量的是程序在稳定运行状态下的性能. 
2. **正确同步 (Synchronization)**:

   * **原因**:CPU 和 GPU 是**[异步执行](./Lecture6-CPU-GPU-Synchronization.md)**的. 当 CPU 上的 Python 代码调用一个 GPU 操作时, 它只是将该任务放入一个指令队列中, 然后立即返回并继续执行后续代码, 而不会等待 GPU 完成. 
   * **做法**:必须在计时代码块的**开始和结束**都插入同步点. 在 PyTorch 中, 这是通过 `torch.cuda.synchronize()` 实现的. 
     * `start_time = time.time()` 之前不需要同步(因为预热已经保证了之前的任务已完成). 
     * `end_time = time.time()` **之前必须**调用 `torch.cuda.synchronize()`, 强制 CPU 等待所有已提交到 GPU 的任务全部执行完毕. 否则, 你测量的只是 CPU 提交任务的时间, 而不是 GPU 的实际执行时间, 会导致结果看起来“快得离谱”. 
3. **多次试验与统计 (Multiple Trials & Statistics)**:

   * **原因**:单次测量可能会受到系统抖动(如后台进程、GPU 温度变化导致的动态频率调整)的影响, 结果不稳定. 
   * **做法**:将评测代码运行多次(例如 3-10 次), 然后取其平均值或中位数作为最终结果. 这可以提供一个更稳定和有代表性的性能数据. 

#### 3. 使用的工具 (Tools)

* **Python `time` 模块**:简单直接, 用于获取墙上时钟时间. 
* **`torch.cuda.synchronize()`**:在 GPU 评测中不可或缺的工具, 用于确保 CPU 和 GPU 的执行状态同步. 
* **`torch.utils.benchmark`**:PyTorch 官方提供的更专业的评测工具, 它内部封装了预热、同步和多次试验等逻辑, 并能提供更详细的统计信息. 对于严肃的性能评测, 推荐使用此工具. 
