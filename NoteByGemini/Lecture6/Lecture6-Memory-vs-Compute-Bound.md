### 模板A: 核心概念

#### 1. 这是什么？(What is it?)
在高性能计算中, **内存密集型 (Memory-Bound)** 和 **计算密集型 (Compute-Bound)** 是用来描述程序性能瓶颈来源的两个核心术语. 它们基于程序的**[算术强度](./Lecture6-Arithmetic-Intensity.md)**来划分. 

*   **内存密集型 (Memory-Bound)**:指程序的执行速度主要受限于将数据在内存和计算单元之间传输的速度. 换句话说, GPU 的计算单元大部分时间都在“等待”数据从缓慢的全局内存 (DRAM) 中送达. 这类任务的**[算术强度](./Lecture6-Arithmetic-Intensity.md)**很低. 

*   **计算密集型 (Compute-Bound)**:指程序的执行速度主要受限于计算单元执行数学运算(如浮点乘加)的速度. 在这种情况下, 内存系统能够及时供应数据, 使得计算单元能够保持满负荷工作. 这类任务的**[算术强度](./Lecture6-Arithmetic-Intensity.md)**很高. 

#### 2. 为什么重要？(Why is it important?)
正确地识别一个任务是内存密集型还是计算密集型, 对于选择正确的优化策略至关重要. 

*   **优化内存密集型任务**:
    *   **核心目标**:减少对主内存的访问次数和数据量. 
    *   **常用策略**:
        *   **[算子融合](./Lecture6-Kernel-Fusion.md)**:将多个操作合并, 让中间结果直接在寄存器中流动. 
        *   **数据类型优化**:使用更低精度的数据类型(如 FP16, INT8), 减少内存占用和传输量. 
        *   **改善内存访问模式**:确保内存访问是合并的 (coalesced), 以最大化带宽利用率. 

*   **优化计算密集型任务**:
    *   **核心目标**:最大化计算单元的利用率. 
    *   **常用策略**:
        *   **利用专用硬件**:使用 Tensor Cores 进行混合精度矩阵运算. 
        *   **增加并行度**:确保有足够多的独立计算任务来填满整个 GPU. 
        *   **指令级优化**:通过编译器选项或手写汇编(PTX/SASS)来优化计算指令的执行效率. 

在深度学习中, 一个普遍的规律是:“**矩阵乘法是计算密集型的, 而其他一切(激活、归一化、逐元素操作)都是内存密集型的. **” 因此, 模型训练的总时间实际上是这两种类型瓶颈的混合体. 

#### 3. 它是如何工作的？(How does it work?)
我们可以通过 Roofline 模型来直观地理解这两个概念. Roofline 模型在一个图上绘制了两个性能上限:
1.  **内存带宽上限 (Memory Bandwidth Roof)**:一条斜率为内存带宽的直线. 在此线下方的区域, 性能受限于内存. 
2.  **峰值计算性能上限 (Peak Performance Roof)**:一条水平线, 代表 GPU 的理论计算峰值. 在此线上方的区域, 性能受限于计算. 

一个程序的**[算术强度](./Lecture6-Arithmetic-Intensity.md)**决定了它在 Roofline 图上的横坐标位置. 
*   算术强度低的任务落在斜线区域, 是**内存密集型**. 
*   算术强度高的任务落在水平线区域, 是**计算密集型**. 

我们的优化工作, 例如**[算子融合](./Lecture6-Kernel-Fusion.md)**和**[分块](./Lecture6-Matrix-Multiplication-Tiling.md)**, 本质上是在**提高程序的有效算术强度**, 将其在 Roofline 图上向右移动, 使其摆脱内存墙的限制, 去触及更高的计算性能天花板. 

#### 4. 关键要点 (Key Takeaways)
*   **瓶颈决定优化**:程序的瓶颈(内存或计算)决定了你应该采取哪种优化策略. 
*   **算术强度是钥匙**:**[算术强度](./Lecture6-Arithmetic-Intensity.md)**是判断一个任务属于哪种类型的关键指标. 
*   **深度学习的普遍现象**:大多数操作是内存密集型的. 因此, **[算子融合](./Lecture6-Kernel-Fusion.md)**是提升 PyTorch 模型性能最普遍且有效的技术. 
*   **优化的目标**:理想的优化是将一个内存密集型问题转化为一个计算密集型问题, 从而充分利用 GPU 昂贵的计算资源. 