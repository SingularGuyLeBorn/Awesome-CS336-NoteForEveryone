### 模板B: 特定术语/技术

#### 1. 定义 (Definition)
**Softmax** 函数是一种在机器学习中广泛使用的函数，它能将一个包含任意实数的 K 维向量 “压缩” 成一个 K 维的实数向量，其中每个元素的范围都在 (0, 1) 之间，并且所有元素的总和为 1。因此，Softmax 的输出可以被解释为一个概率分布。

对于输入向量 `z = (z₁, z₂, ..., zₖ)`，其 Softmax 输出向量 `σ(z)` 的第 `i` 个元素定义为：
`σ(z)ᵢ = eᶻᵢ / (Σⱼ eᶻⱼ)` for `i = 1, ..., k`

在深度学习中，Softmax 通常被应用于一个 N x K 的张量的最后一个维度上，即对每一行（每个样本）独立地计算一个 K 维的概率分布。

**数值稳定性**：直接计算 `eᶻᵢ` 可能会导致数值溢出（当 `zᵢ` 是一个较大的正数时）。为了解决这个问题，通常会在计算指数前，从向量 `z` 的所有元素中减去该向量的最大值 `max(z)`。这个操作在数学上不改变最终结果，但能保证指数函数的输入不会过大，从而确保了数值稳定性。
`σ(z)ᵢ = e^(zᵢ - max(z)) / (Σⱼ e^(zⱼ - max(z)))`

#### 2. 关键特性与用途 (Key Features & Usage)
*   **概率化**：将模型的原始输出（logits）转换为概率分布，便于解释和用于分类任务。
*   **可微性**：Softmax 函数是可微的，这使得它能够与交叉熵损失函数等结合，通过反向传播来训练模型。
*   **应用场景**：
    *   **多分类问题**：作为分类模型（如 ResNet, VGG）最后一层的激活函数，输出每个类别的概率。
    *   **注意力机制 (Attention Mechanism)**：在 Transformer 模型中，Softmax 用于将注意力分数（attention scores）转换为注意力权重（attention weights），表示在生成某个词时，应该对输入序列中的其他词赋予多大的“关注度”。

#### 3. 案例分析 (Case Study in this Lecture)
与 **[GeLU](./Lecture6-GeLU.md)** 不同，Softmax 不仅仅是一个逐元素操作，它包含了一个**归约 (Reduction)**步骤（`Σⱼ eᶻⱼ`，即求和）。这使得它的高性能实现更具挑战性，也更能体现 **[Triton](./Lecture6-Triton.md)** 等工具的优势。

1.  **[手动 PyTorch 实现](./Lecture6-Code-manual_softmax.md)**：该实现分步执行了数值稳定版本的 Softmax 计算：求最大值 (`.max()`)、减法、指数 (`torch.exp`)、求和 (`.sum()`)、除法。每一步都可能是一次独立的 Kernel 调用，涉及多次 GPU 全局内存的读写，性能不佳。
2.  **[Triton 实现](./Lecture6-Code-triton_softmax.md)**：这个实现展示了一种高效的**[算子融合](./Lecture6-Kernel-Fusion.md)**策略：
    *   **一行一块 (One row per block)**：每个 Triton Kernel 实例负责处理矩阵的一整行。
    *   **片上归约 (On-chip Reduction)**：将整行数据一次性加载到 SM 的高速寄存器中，然后在片上完成求最大值、减法、指数、求和、除法等所有操作，最后将结果一次性写回全局内存。
    *   这种方法将多次内存往返优化为一次读和一次写，极大地提升了性能，是处理行式归约操作的经典范例。

这个案例说明，对于包含归约的复合操作，通过精心设计的 Kernel（如 Triton 中一行一块的策略），可以实现显著的性能提升。