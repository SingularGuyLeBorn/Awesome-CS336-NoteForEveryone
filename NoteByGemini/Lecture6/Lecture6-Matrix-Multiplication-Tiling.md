### 模板A: 核心概念

#### 1. 这是什么？(What is it?)
**矩阵乘法分块 (Matrix Multiplication Tiling)**，也称为分块矩阵乘法，是一种用于优化大型矩阵乘法性能的核心算法技术。其基本思想是将大的输入矩阵 `A` 和 `B` 分割成许多小的子矩阵（称为“块”或“瓦片”，即 Tile），然后通过对这些小块进行一系列的矩阵乘法和累加来计算最终的结果矩阵 `C`。

#### 2. 为什么重要？(Why is it important?)
朴素的矩阵乘法算法需要 `O(N³)` 次计算和 `O(N²)` 次内存访问。对于无法完全放入 GPU 高速缓存的大型矩阵，每次计算 `C` 中的一个元素都需要反复从缓慢的 DRAM 中读取 `A` 的一整行和 `B` 的一整列。这种极低的**数据复用率**导致算法严重受限于内存带宽，即成为**[内存密集型](./Lecture6-Memory-vs-Compute-Bound.md)**。

分块技术通过充分利用 **[GPU 架构](./Lecture6-GPU-Architecture.md)** 中的内存层次结构，特别是高速但容量有限的**[共享内存 (Shared Memory)](./Lecture6-Shared-Memory.md)**，从根本上解决了这个问题。它将算法的**[算术强度](./Lecture6-Arithmetic-Intensity.md)**提高到了极致，使矩阵乘法成为一个**[计算密集型](./Lecture6-Memory-vs-Compute-Bound.md)**操作，从而能够充分发挥 GPU 强大的计算能力。可以说，没有分块技术，现代深度学习的训练效率是不可想象的。

#### 3. 它是如何工作的？(How does it work?)
分块矩阵乘法的执行流程如下，这通常是在一个 CUDA 或 **[Triton](./Lecture6-Triton.md)** Kernel 中由一个**[线程块 (Thread Block)](./Lecture6-GPU-Execution-Model.md)** 来完成的：

1.  **任务分配**：一个线程块负责计算结果矩阵 `C` 的一个小的子块 `C_sub`。
2.  **主循环**：线程块会沿着输入矩阵 `A` 的行和 `B` 的列进行迭代。在每次迭代中：
    a. **加载数据块**：线程块内的所有线程协同工作，从**全局内存 (DRAM)** 中加载一个 `A` 的子块 `A_sub` 和一个 `B` 的子块 `B_sub` 到**共享内存**中。这是一个关键步骤，因为从 DRAM 的读取是昂贵的。
    b. **同步**：所有线程等待，确保 `A_sub` 和 `B_sub` 已经完整地加载到共享内存中。
    c. **片上计算**：线程块内的每个线程从**共享内存**（而不是全局内存）中读取数据，计算 `A_sub` 和 `B_sub` 的乘积，并将结果累加到自己私有的**寄存器**中。由于共享内存和寄存器的速度极快，这个计算过程非常高效。
    d. **同步**：再次同步，确保本次片上计算完成，准备进入下一次迭代。
3.  **写回结果**：当主循环结束后，每个线程将自己寄存器中保存的最终累加结果写回到全局内存中对应的 `C_sub` 位置。

通过这个流程，`A` 和 `B` 的每个数据块都只从全局内存中读取一次，但却在共享内存中被多次重复使用（被用于计算 `C_sub` 中的所有元素），极大地提高了数据复用率。

#### 4. 关键要点 (Key Takeaways)
*   **核心思想**：用多次对**高速小内存（共享内存）**的访问，来替代多次对**慢速大内存（DRAM）**的访问。
*   **关键资源**：分块技术成功的关键在于有效利用了**[共享内存](./Lecture6-Shared-Memory.md)**这一 SM 内部的高速暂存区。
*   **性能转变**：分块技术将矩阵乘法从**[内存密集型](./Lecture6-Memory-vs-Compute-Bound.md)**转变为**[计算密集型](./Lecture6-Memory-vs-Compute-Bound.md)**，是实现高性能矩阵运算的基石。
*   **Triton 的优势**：在 **[Triton](./Lecture6-Triton.md)** 中实现分块矩阵乘法比在 CUDA 中简单得多，因为 Triton 编译器可以自动处理很多共享内存的管理细节。