### 1. 概念定义

**GPU 架构 (GPU Architecture)** 指的是图形处理单元 (Graphics Processing Unit) 的内部设计和组织方式。与为执行串行任务而优化的中央处理器 (CPU) 不同，GPU 是一种专门设计用于并行处理大规模数据（尤其是图形和矩阵运算）的硬件加速器。其架构的核心思想是通过拥有成百上千个小型计算核心，同时执行相同的操作，从而实现极高的吞吐量。

### 2. 核心原则

GPU 架构与 CPU 架构在设计哲学上存在根本差异：

- **CPU (延迟优化 - Latency-Optimized)**:
    - **目标**: 尽快完成**单个**任务。
    - **设计**: 拥有少量（几个到几十个）强大而复杂的处理核心。每个核心都配备了大型缓存、复杂的控制逻辑（如分支预测）和指令流水线，以最大限度地减少单个指令的执行时间。
    - **类比**: 一个由几位专家组成的团队，每位专家都能独立且迅速地解决一个复杂问题。

- **GPU (吞吐量优化 - Throughput-Optimized)**:
    - **目标**: 在单位时间内完成**尽可能多**的任务。
    - **设计**: 拥有大量（成百上千个）简单且高效的计算核心。这些核心共享控制逻辑和缓存，牺牲了单个任务的执行速度，换取了大规模并行处理的能力。
    - **类比**: 一个由成千上万名工人组成的工厂流水线，每个工人只负责一个简单的步骤，但整个工厂的产出量巨大。

### 3. 关键组件

现代 GPU 架构主要由以下几个关键部分构成：

1.  **流式多处理器 (Streaming Multiprocessor - SM)**:
    - SM 是 GPU 的基本执行单元，可以看作是 GPU 内部的“处理器核心”。一个 GPU 通常包含数十到上百个 SM。
    - 每个 SM 都是一个独立的单元，负责调度和执行分配给它的**[线程块 (Blocks)](./Lecture5-GPU-Execution-Model.md)**。
    - 内部包含执行计算的**流式处理器 (Streaming Processors - SP)**、专门用于矩阵乘法的**[张量核心 (Tensor Cores)](./Lecture5-Tensor-Cores.md)**、调度单元以及高速的**[共享内存 (Shared Memory)](./Lecture5-GPU-Memory-Hierarchy.md)** 和 **L1 缓存**。

2.  **流式处理器 (Streaming Processor - SP)**:
    - 也称为 CUDA 核心 (NVIDIA 术语)，是执行浮点和整数运算的最小计算单元。
    - 一个 SM 内部包含多个 SP。它们遵循 **[SIMT (单指令，多线程)](./Lecture5-SIMT.md)** 模型，即一个 SM 内的多个 SP 同时执行相同的指令，但处理的数据不同。

3.  **内存系统**:
    - GPU 拥有一个复杂的**[内存层级](./Lecture5-GPU-Memory-Hierarchy.md)**，包括极速的片上寄存器、共享内存，以及速度稍慢的 L2 缓存和容量巨大但延迟最高的全局内存 (DRAM/HBM)。正确地管理和利用这个内存层级是实现高性能计算的关键。

### 4. 重要性与应用

理解 GPU 架构对于深度学习和高性能计算至关重要：

- **性能优化**: 只有了解硬件的工作方式，开发者才能编写出能够充分利用其并行能力的代码。例如，通过**[分块技术](./Lecture5-Tiling.md)**最大化共享内存的使用，或通过**[内存合并](./Lecture5-Memory-Coalescing.md)**来优化全局内存访问，这些都直接源于对 GPU 架构的理解。
- **算法设计**: 像 **[FlashAttention](./Lecture5-FlashAttention.md)** 这样的现代算法，其设计的核心就是为了契合 GPU 的架构特点（如内存层级和并行模型），从而突破传统实现的性能瓶颈。
- **模型效率**: 在设计神经网络时，选择对 GPU 友好的操作（如大规模矩阵乘法）可以显著提高训练和推理速度。这解释了为什么**[张量核心](./Lecture5-Tensor-Cores.md)**的出现极大地推动了深度学习的发展。

总之，GPU 架构是现代 AI 和科学计算的基石。它的设计哲学——用极致的并行化换取无与伦比的吞吐量——已经成为驱动整个领域发展的核心动力。