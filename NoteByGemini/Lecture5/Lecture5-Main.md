# 第五讲：详解 GPU 架构与性能优化

### 前言
大家好，希望大家第一份作业做得还顺利。今天我们将深入探讨驱动我们语言模型运行的核心——GPU。对于不熟悉硬件的同学来说，GPU 的工作原理可能相当神秘。本讲的目标就是揭开 **[CUDA](./Lecture5-CUDA.md)** 和 GPU 的神秘面纱。我们将一起理解 GPU 性能瓶颈的成因，比如为什么矩阵乘法的性能会呈现出难以预测的波浪形态。更重要的是，我们将学习如何设计并实现像 **[FlashAttention](./Lecture5-FlashAttention.md)** 这样的高性能算法，从而掌握开启更长上下文窗口的关键。

本讲将分为三个部分：首先，深入剖析 **[GPU 架构](./Lecture5-GPU-Architecture.md)**，包括其执行模型和内存结构；其次，我们将探讨影响 GPU 性能的关键因素和优化技巧；最后，我们将把所有理论知识融会贯通，通过解析 **[FlashAttention](./Lecture5-FlashAttention.md)** 的实现，展示这些优化技术如何在实践中协同工作，创造出卓越的性能。

### 正文

#### 硬件的重要性与计算扩展之路

我们知道，更多的计算资源对于训练大型语言模型至关重要。无论是预训练还是推理，**[伸缩法则](./Lecture5-Scaling-Laws.md)** 都已证明，增加计算、数据和模型规模能带来显著的性能提升。可以说，深度学习的巨大进步，很大程度上是由更快的硬件、更高的利用率和更优的并行化策略驱动的。

早期，CPU 性能的提升遵循 **[登纳德缩放定律](./Lecture5-Dennard-Scaling.md)**，即随着晶体管变小，可以在更低的功耗下以更快的时钟速度运行，从而提升单线程性能。然而，大约从 21 世纪初开始，这种模式走到了尽头。尽管晶体管密度仍在增加，但单线程性能的提升却停滞不前。这意味着我们必须转向**[并行扩展](./Lecture5-Parallel-Scaling.md)**来获取更强的计算能力。从 K20 到 H100，GPU 的整数运算能力呈现出超指数级的增长，这正是我们语言模型发展的关键驱动力。

#### GPU vs. CPU：为不同目标而生

CPU 和 GPU 的设计理念截然不同。CPU 致力于优化**延迟**，目标是尽快完成单个任务。为此，它配备了庞大而复杂的控制单元和分支预测逻辑，以支持复杂的条件控制流。相比之下，GPU 优化的是**吞吐量**，目标是在单位时间内完成尽可能多的任务总和。它的架构由海量的微型计算单元（ALU）和极少的控制逻辑组成。

这种设计差异意味着，虽然单个任务在 GPU 上的完成时间可能比在 CPU 上更长，但 GPU 能够同时处理成千上万个线程，使得任务的**总完成时间**远少于 CPU。

#### GPU 核心架构剖析

GPU 的核心是**[流式多处理器 (SM)](./Lecture5-Streaming-Multiprocessor.md)**。以 A100 GPU 为例，它拥有多达 128 个 SM。每个 SM 都是一个独立的执行单元，可以看作 GPU 的“核心”。在 SM 内部，又包含了大量的**流式处理器 (SP)**，它们负责执行具体的计算任务。这种架构遵循 **[SIMT (单指令，多线程)](./Lecture5-SIMT.md)** 模型：一条指令同时作用于多个不同的数据。此外，现代 GPU 的 SM 内部还集成了专门用于加速矩阵乘法的**[张量核心 (Tensor Cores)](./Lecture5-Tensor-Cores.md)**。

**[GPU 内存层级](./Lecture5-GPU-Memory-Hierarchy.md)** 是理解性能的关键。内存与 SM 的物理距离决定了其访问速度。
- **寄存器 (Registers) 和共享内存 (Shared Memory)**：位于 SM 内部，速度最快（约 20 个时钟周期），是进行高性能计算的核心资源。
- **L2 缓存 (L2 Cache)**：位于 GPU 芯片上，紧邻 SM，速度稍慢（约 200-300 个时钟周期），但仍比全局内存快得多。
- **全局内存 (Global Memory / DRAM)**：通常是 **[高带宽内存 (HBM)](./Lecture5-High-Bandwidth-Memory-HBM.md)**，位于 GPU 芯片之外，通过 HBM 连接器相连，访问延迟最高。

过去几十年的发展中，计算能力的扩展速度远远超过了内存带宽的增长速度，这使得**内存访问**成为现代 GPU 计算的主要瓶颈。

**[GPU 执行模型](./Lecture5-GPU-Execution-Model.md)** 包含三个关键层次：
1.  **线程 (Threads)**：执行计算的基本单位。
2.  **线程块 (Blocks)**：一组线程的集合，每个线程块被调度到一个 SM 上执行，并拥有自己独立的共享内存。
3.  **线程束 (Warps)**：线程块内的线程以 32 个为一组（一个 Warp）的形式被同时执行。这是 GPU 硬件调度的基本单位。

跨线程块的信息交换必须通过缓慢的全局内存进行，因此，理想的 CUDA 程序应尽可能将计算限制在单个线程块内，充分利用高速的共享内存。

顺便提一下 **[TPU 架构](./Lecture5-TPU-Architecture.md)**，它在概念上与 GPU 非常相似，也拥有高速的片上内存和专门的矩阵乘法单元（MXU），但其架构更专注于矩阵运算，设计上更为精简。

#### 提升 GPU 工作负载性能的秘诀

要让机器学习工作负载在 GPU 上飞速运行，我们需要理解并应用一系列性能优化技巧。其核心目标是避免成为**内存密集型 (memory bound)**，并尽可能达到计算单元的饱和，即**计算密集型 (compute bound)**，这可以通过**[屋顶线模型](./Lecture5-Roofline-Model.md)**来直观理解。

以下是六个关键的优化策略：

1.  **避免[控制流发散](./Lecture5-Control-Divergence.md)**：由于一个 Warp 内的所有线程必须执行相同的指令，`if-else` 这样的条件语句会导致部分线程闲置等待，从而降低效率。应尽量避免在并行计算核心中使用复杂的条件分支。

2.  **采用[低精度计算](./Lecture5-Low-Precision-Computation.md)**：使用 FP16 或 BF16 替代 FP32 可以将内存带宽需求减半。更少的位数意味着更少的数据传输。在混合精度训练中，通常将输入和权重存储为 16 位，但在进行矩阵乘法的累加等中间计算时使用 32 位以保持精度。

3.  **[算子融合](./Lecture5-Operator-Fusion.md)**：将多个连续的、简单的操作（如一连串的激活函数）合并成一个单一的 CUDA 核函数。这样可以避免将中间结果反复读写于缓慢的全局内存，而是将所有计算都保留在 SM 的高速寄存器或共享内存中完成。`torch.compile` 等现代编译器可以自动完成这类优化。

4.  **[重计算](./Lecture5-Recomputation.md)**：这是一种以计算换内存的策略。在反向传播中，与其存储所有前向传播的激活值（这会消耗大量内存带宽），不如在需要时即时重新计算它们。当计算资源过剩而内存带宽成为瓶颈时，这是一个极佳的权衡。

5.  **利用[内存合并](./Lecture5-Memory-Coalescing.md)**：DRAM 以“突发模式 (burst mode)”读取数据，即一次读取会返回一个连续的数据块。如果一个 Warp 内的 32 个线程访问的内存地址是连续的且能落入同一个数据块，硬件就能将这些访问合并为一次，从而将内存吞吐量提高数倍。在进行矩阵操作时，错误的内存遍历顺序（如按列遍历行主序矩阵）会导致访问无法合并，性能急剧下降。

6.  **[分块技术](./Lecture5-Tiling.md)**：这是最重要的优化之一。通过将大型矩阵分割成小的“瓦片 (tile)”，并将这些瓦片加载到 SM 的共享内存中，可以最大化地重复使用数据。**[分块矩阵乘法算法](./Lecture5-Tiled-Matrix-Multiplication-Algorithm.md)**首先将输入矩阵的瓦片加载到共享内存，然后所有线程在该瓦片上完成所有可能的计算，最后才写回结果或加载新的瓦片。这极大地减少了对全局内存的访问次数。

#### 矩阵乘法性能之谜的答案

现在我们可以回头解释那张神秘的性能图了。图中的波浪形模式正是由**[分块技术](./Lecture5-Tiling.md)**及其复杂性造成的。
- **对齐问题**：当矩阵的维度不能被分块大小整除时，会导致一些分块的利用率极低。更糟糕的是，如果分块的边界与 DRAM 的突发读取段不对齐，会使**[内存合并](./Lecture5-Memory-Coalescing.md)**失效，导致内存读取次数翻倍。这就是为什么矩阵维度是 32 或 64 的倍数时性能最佳的原因。
- **[波形量化](./Lecture5-Wave-Quantization.md)**：当所需的线程块总数略微超过 SM 的总数时（例如，A100 有 108 个 SM，而计算需要 120 个块），GPU 必须分两“波”来执行。第一波 108 个块并行执行，利用率很高；但第二波只有 12 个块，导致大量 SM 闲置，从而造成性能的急剧下降。

#### 融会贯通：解析 FlashAttention

**[FlashAttention](./Lecture5-FlashAttention.md)** 是将上述优化思想集于一身的典范。它通过两种核心技术，在不牺牲精度的情况下，实现了对注意力计算的亚二次方级别的高带宽内存访问。

- **核心挑战**：标准注意力计算中的 Softmax 操作是一个全局操作，需要计算整行的总和，这与**[分块技术](./Lecture5-Tiling.md)**的局部性原则相悖。如果为每个分块计算后都写回全局内存，会产生巨大的 I/O 开销。

- **解决方案**：
    1.  **[分块技术](./Lecture5-Tiling.md)**：将 Q, K, V 矩阵分块，并将计算限制在 SM 的高速 SRAM（共享内存）中。
    2.  **[在线 Softmax 算法](./Lecture5-Online-Softmax-Algorithm.md)**：FlashAttention 采用了一种巧妙的技巧，可以在逐块处理数据时，以流式方式增量地更新 Softmax 的分子和分母。它通过追踪当前块的最大值，并使用一个伸缩因子来校正之前块的累加和，从而避免了对完整注意力矩阵的实例化。
    3.  **[重计算](./Lecture5-Recomputation.md)**：在前向传播中，它不会存储巨大的 N×N 注意力矩阵。在反向传播时，它会利用保存在 SRAM 中的原始 Q, K, V 块和 Softmax 的归一化统计量，**逐块地重新计算**注意力矩阵，从而在不占用大量全局内存的情况下完成梯度计算。

通过**[FlashAttention 前向传播算法](./Lecture5-FlashAttention-Forward-Pass-Algorithm.md)**，这些技术完美结合，将注意力计算从一个受内存带宽限制的操作，转变为一个能够充分利用 GPU 强大计算能力的操作。

### 拓展阅读

为了系统地掌握本讲内容，我们推荐以下学习路径：

1.  **建立基础模型**：首先，请阅读 **[GPU 架构](./Lecture5-GPU-Architecture.md)**、**[GPU 内存层级](./Lecture5-GPU-Memory-Hierarchy.md)** 和 **[GPU 执行模型](./Lecture5-GPU-Execution-Model.md)** 的笔记。这将帮助你构建一个关于 GPU 如何组织计算和数据的心理模型。

2.  **理解性能原理**：接着，学习 **[屋顶线模型](./Lecture5-Roofline-Model.md)**，它为你提供了一个评估性能瓶颈的理论框架。然后深入了解 **[内存合并](./Lecture5-Memory-Coalescing.md)** 和 **[分块技术](./Lecture5-Tiling.md)**，这是解决内存瓶颈的两个最核心的硬件友好型策略。

3.  **掌握优化工具箱**：现在，你可以学习具体的优化技术，如 **[算子融合](./Lecture5-Operator-Fusion.md)**、**[重计算](./Lecture5-Recomputation.md)** 和 **[低精度计算](./Lecture5-Low-Precision-Computation.md)**。这些是你在编写高性能代码时可以直接使用的“招式”。

4.  **实战案例分析**：最后，将所有知识融会贯通。阅读关于 **[FlashAttention](./Lecture5-FlashAttention.md)** 的 tổng quan 笔记，然后深入分析 **[分块矩阵乘法算法](./Lecture5-Tiled-Matrix-Multiplication-Algorithm.md)** 和 **[在线 Softmax 算法](./Lecture5-Online-Softmax-Algorithm.md)** 的笔记，观察这些基础原理是如何被巧妙地组合起来，创造出解决实际问题的顶尖算法的。