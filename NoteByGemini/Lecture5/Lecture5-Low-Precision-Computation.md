### 1. 概念定义

**低精度计算 (Low Precision Computation)**, 在深度学习领域也常被称为**混合精度训练 (Mixed-Precision Training)**, 是一种利用较低数值精度(如 16 位浮点数)来执行大部分计算和存储, 以加速模型训练和推理并减少内存占用的技术. 

传统的科学计算通常使用 32 位单精度浮点数 (FP32) 或 64 位双精度浮点数 (FP64). 低精度计算则主要使用以下格式:

- **FP16 (半精度浮点数)**: 16 位, 拥有较小的数值范围和较低的精度. 容易出现上溢(overflow)或下溢(underflow)问题. 
- **BF16 (脑浮点数 - BFloat16)**: 16 位, 由 Google 设计. 它拥有与 FP32 相同的指数位数(8 位), 因此数值范围与 FP32 相同, 但小数位数较少(7 位), 因此精度较低. 这种格式在深度学习中更受欢迎, 因为它能有效避免梯度消失或爆炸时的溢出问题. 
- **INT8 (8 位整数)**: 用于量化推理, 将浮点数权重和激活值映射到 256 个整数值, 能极大地提升性能和降低功耗. 

### 2. 核心优势

采用低精度计算能带来三大核心优势, 这些优势都直接解决了 GPU 的主要瓶颈:

1.  **减少内存占用和带宽需求**:
    - **内存占用减半**: 将模型权重、激活值和梯度从 FP32 存储为 FP16 或 BF16, 所需的内存空间直接减少一半. 这使得我们能够训练更大的模型, 或在相同的硬件上使用更大的批量大小 (batch size). 
    - **内存带宽需求减半**: 数据传输量减半意味着**[内存带宽](./Lecture5-GPU-Memory-Hierarchy.md)**的压力也减半. 如**[屋顶线模型](./Lecture5-Roofline-Model.md)**所示, 对于内存密集型操作, 这能直接带来近乎翻倍的性能提升. 

2.  **提升计算速度**:
    - **专用硬件加速**: 现代 GPU 的**[张量核心 (Tensor Cores)](./Lecture5-Tensor-Cores.md)** 专门为 FP16/BF16/INT8 等低精度格式的矩阵乘法设计. 使用这些格式可以充分利用张量核心, 获得数倍于 FP32 的计算吞吐量. 
    - **更高的算术强度**: 如讲座中的 ReLU 示例所示, 对于相同的运算次数, 将数据量减半, 程序的**算术强度 (FLOPs/Byte)** 就翻倍了, 这有助于将程序从内存密集型推向计算密集型. 

### 3. 混合精度 (Mixed Precision) 的实践

单纯使用 FP16 可能会导致数值不稳定, 因为它的范围太小, 在前向传播中可能导致激活值下溢, 在反向传播中梯度可能会小到无法表示(变为 0). 为了解决这个问题, 实际应用中通常采用**混合精度**策略:

- **权重、数据和激活值存储**: 使用 FP16 或 BF16. 
- **高精度累加**: 在执行矩阵乘法等运算时, 尤其是在**[张量核心](./Lecture5-Tensor-Cores.md)**内部, 乘法的结果会累加到 32 位的累加器中, 以保持精度. 
- **FP32 主权重副本**: 维持一个 FP32 精度的模型权重副本. 在每次更新时, 梯度首先被转换成 FP32, 然后应用到这个主权重上, 之后再将更新后的权重转换回 FP16 用于下一次的前向传播. 
- **损失缩放 (Loss Scaling)**: 为了防止梯度下溢(变得太小以至于在 FP16 中为 0), 在反向传播开始前, 将损失值乘以一个大的缩放因子(如 1024). 这样, 所有的梯度都会相应地放大, 从而保留其有效数值. 在权重更新前, 再将梯度除以该缩放因子, 恢复其原始大小. 

总之, 低精度计算是现代 GPU 性能飞跃的关键技术之一. 它通过在计算、存储和传输上全面降低对硬件资源的需求, 极大地提升了深度学习工作负载的效率, 是训练当今大型语言模型的必备技术. 