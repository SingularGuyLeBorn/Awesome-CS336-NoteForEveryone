### 1. 概念定义

**并行扩展 (Parallel Scaling)** 指的是通过增加处理单元（如 CPU 核心、GPU SM 或服务器节点）的数量来提升计算系统整体性能的过程。与“单线程扩展”（即提升单个处理单元的速度，如提高时钟频率）相对，并行扩展关注的是如何有效地利用更多的硬件资源来同时处理一个大型任务的多个部分或同时处理多个独立任务。

在 **[登纳德缩放定律](./Lecture5-Dennard-Scaling.md)** 失效后，单核性能增长停滞，并行扩展成为了延续**[摩尔定律](./Lecture5-Scaling-Laws.md)**所带来的晶体管红利、持续提升计算能力的主要途径。

### 2. 核心思想与类型

并行扩展的核心思想是将一个大的计算问题分解成许多可以同时执行的小块，并将这些小块分配给不同的处理单元。根据任务的性质，并行扩展可以分为两种主要类型：

1.  **任务并行 (Task Parallelism)**:
    - 涉及将多个不同的、独立的任务同时分配给不同的处理器。
    - **示例**: 在一个 Web 服务器中，每个核心可以独立处理一个用户的请求。
    - 在深度学习中，这可以体现在同时进行数据预处理、模型训练和结果评估等不同的流程。

2.  **数据并行 (Data Parallelism)**:
    - 涉及将相同操作应用于一个大型数据集的不同部分。这是深度学习和科学计算中最常见的并行模式。
    - **示例**: 在处理一张大图像时，可以将图像分割成小块，每个核心处理其中一块的滤镜效果。
    - **在深度学习中**: 这是最核心的并行策略。例如，一个大的 batch 数据被分成多个小的 mini-batch，每个 **[GPU](./Lecture5-GPU-Architecture.md)** 负责处理一个 mini-batch 的前向和反向传播，最后同步梯度。

### 3. GPU 与并行扩展

GPU 是并行扩展理念的极致体现。它的架构天生就是为大规模数据并行而设计的：

- **海量核心**: 一个现代 GPU 拥有数千个计算核心，能够同时执行数万个**[线程](./Lecture5-GPU-Execution-Model.md)**。
- **SIMT 模型**: **[单指令，多线程 (SIMT)](./Lecture5-SIMT.md)** 模型允许一条指令同时驱动成百上千个核心对不同数据执行相同操作，这完美契合了矩阵乘法等深度学习核心运算的需求。
- **超指数级增长**: 如讲座中 Bill Dally 的图表所示，GPU 的性能通过并行扩展实现了超指数级的增长。这种增长主要来源于：
    - **增加 SM 数量**: 每个新一代 GPU 都会集成更多的**[流式多处理器 (SM)](./Lecture5-Streaming-Multiprocessor.md)**。
    - **专用硬件**: 引入如**[张量核心 (Tensor Cores)](./Lecture5-Tensor-Cores.md)** 这样的专用电路，它们本身就是高度并行的矩阵乘法引擎。
    - **低精度计算**: 转向**[低精度格式](./Lecture5-Low-Precision-Computation.md)**（如 FP16, INT8），使得在相同的硬件面积和内存带宽下可以并行处理更多的数据。

### 4. 挑战与重要性

尽管并行扩展威力巨大，但也面临挑战，最著名的是**阿姆达尔定律 (Amdahl's Law)**，它指出系统的总加速比受限于程序中无法并行的串行部分的比例。

然而，在深度学习领域，大部分计算（尤其是矩阵乘法）本质上是高度可并行的，这使得 GPU 的并行扩展策略取得了巨大成功。可以说，没有并行扩展，就没有现代 GPU 的辉煌；没有 GPU，就没有今天我们所熟知的大型语言模型。它是连接后摩尔定律时代晶体管增长与 AI 算力需求之间的关键桥梁。