### 1. 概念定义

**重计算 (Recomputation)**，在深度学习中通常被称为**激活检查点 (Activation Checkpointing)** 或**梯度检查点 (Gradient Checkpointing)**，是一种以**增加计算量**为代价来**减少内存占用**的优化技术。

其核心思想是，在前向传播过程中，不再存储所有的中间激活值（这些激活值在反向传播计算梯度时是必需的），而只存储其中少数几个（称为“检查点”）。在反向传播时，当需要某个未被存储的激活值时，系统会从最近的一个检查点开始，重新执行一小部分前向传播计算，以当场生成所需的激活值。

### 2. 工作原理

标准的**反向传播 (Backpropagation)** 算法流程如下：
1.  **前向传播**: 从输入开始，逐层计算并**存储**每一层的激活值。
2.  **反向传播**: 从输出层开始，逐层向后计算梯度。在计算某一层参数的梯度时，需要用到该层在前向传播时产生的激活值。

这个过程的问题在于，存储所有激活值会消耗巨大的内存。对于深度网络或长序列模型（如 Transformer），激活值的内存占用甚至可能超过模型参数本身，成为限制模型规模或批量大小的主要瓶颈。

**重计算**修改了这个流程：

1.  **前向传播 (带检查点)**:
    - 正常进行前向传播计算。
    - 但是，只将网络中某些特定层（检查点）的输出激活值存储在内存中，而**丢弃**其他所有中间层的激活值。

2.  **反向传播 (带重计算)**:
    - 正常进行反向传播。
    - 当需要一个被丢弃的激活值来计算梯度时，算法会找到离它最近的前一个检查点。
    - 然后，从该检查点开始，**重新执行**一小段前向传播，直到计算出所需的那个激活值。
    - 使用这个即时生成的激活值来计算梯度，然后将其丢弃。
    - 继续反向传播过程。

### 3. 优势与权衡

**主要优势**:

- **大幅减少内存占用**: 这是重计算最主要的目的。通过只存储少量检查点，可以极大地减少训练过程中的峰值内存占用，从而能够训练更大的模型或使用更大的批量，这反过来又能提升训练效率和模型性能。

- **提升性能 (在特定场景下)**:
    - 讲座中提到的例子展示了重计算的另一个应用场景：**性能优化**。
    - 当一个操作是**内存密集型 (memory-bound)** 时，意味着 GPU 的计算单元大部分时间都在等待数据从慢速内存中读取。在这种情况下，读取已存储的激活值所花费的时间，可能比重新计算它还要长。
    - 因此，通过重计算，我们用 GPU 闲置的计算资源换取了对慢速全局内存的访问，将瓶颈从内存带宽转移到了计算上，从而可能实现端到端的加速。`5/8th the memory accesses` 的例子正是这个思想的体现。

**主要权衡**:

- **增加计算量**: 重计算的代价是需要额外执行一部分前向传播计算。在最简单的实现中，这大约会增加 33% 的总计算量。然而，在计算资源远比内存带宽丰富的现代 **[GPU](./Lecture5-GPU-Architecture.md)** 上，这种权衡通常是值得的。

### 4. 应用

- **训练大型模型**: 梯度检查点是训练像 GPT-3 这样参数量巨大的模型的关键技术之一。
- **[FlashAttention](./Lecture5-FlashAttention.md)**: 在 FlashAttention 的反向传播中，就采用了重计算的思想。它不会存储 N×N 的注意力矩阵，而是在反向传播时，利用加载到 SRAM 中的 Q、K、V 瓦片**重新计算**所需的注意力矩阵块，从而在计算梯度的同时避免了巨大的内存开销。