### 1. 核心功能与目标

**在线 Softmax (Online Softmax)** 算法的目标是在数据以流式或分块方式到达时，逐步、增量地计算出精确的 Softmax 函数值。它解决了标准 Softmax 算法必须一次性看到所有输入数据的局限性，使其能够被用于**[分块 (Tiling)](./Lecture5-Tiling.md)** 计算流程中，如 **[FlashAttention](./Lecture5-FlashAttention.md)**。

其核心是避免在处理早期数据块时，因无法得知全局最大值和全局归一化分母而导致计算不准确或需要多次重算。

### 2. 参数解析

在一个迭代的、分块的上下文中，算法在处理第 `i` 块时需要以下输入：

- `x_i`: 当前数据块的输入向量。
- `m_old`: 处理完前 `i-1` 块后，累积的（旧的）最大值。
- `l_old`: 处理完前 `i-1` 块后，累积的（旧的）Softmax 指数和（归一化分母的部分和）。

算法将输出更新后的 `m_new` 和 `l_new`，供下一轮迭代使用。

### 3. 核心逻辑 (伪代码与注释)

以下伪代码展示了处理一个新数据块 `x_i` 并更新全局统计量的核心逻辑。

```python
# 初始化 (在处理第一个块之前)
m_old = -infinity  # 旧的全局最大值
l_old = 0.0        # 旧的全局指数和

# 循环处理每个数据块 x_i
for x_i in data_blocks:

    # --- 步骤 1: 计算当前块的局部统计量 ---
    # m_i 是当前块 x_i 内部的最大值
    m_i = max(x_i)

    # l_i 是当前块的指数和，使用当前块的最大值 m_i 进行归一化以保证数值稳定
    l_i = sum(exp(x - m_i) for x in x_i)

    # --- 步骤 2: 更新全局最大值 ---
    # 新的全局最大值是旧的全局最大值和当前块最大值中的较大者
    m_new = max(m_old, m_i)

    # --- 步骤 3: 使用伸缩因子更新全局指数和 ---
    # 这是算法的核心技巧。我们需要将 l_old 和 l_i 统一到新的最大值 m_new 的基准下。

    # 计算伸缩因子
    scale_factor_old = exp(m_old - m_new)
    scale_factor_i = exp(m_i - m_new)

    # 更新全局指数和
    l_new = l_old * scale_factor_old + l_i * scale_factor_i

    # --- 步骤 4: 为下一轮迭代做准备 ---
    # 将新的统计量保存为旧的统计量
    m_old = m_new
    l_old = l_new

# 循环结束后，l_old (即 l_new) 就是最终的全局归一化分母。
# 此时，可以再次遍历所有数据，用最终的 m_old 和 l_old 计算每个元素的精确 softmax 值。
```在 FlashAttention 中，对输出向量的更新也与 `l_new` 的更新同步进行，从而避免了最后的回溯遍历。

### 4. 与理论的连接

- **数值稳定性**: 该算法通过在每个局部块和全局更新中始终减去最大值，继承了标准安全 Softmax 的数值稳定性，有效避免了 `exp` 操作可能导致的浮点数上溢。
- **分块处理**: 该算法的设计完全是为了服务于**[分块技术](./Lecture5-Tiling.md)**。它将一个全局依赖问题（Softmax）分解成了一系列独立的局部计算和轻量级的聚合步骤。
- **数学原理**: 算法的正确性基于指数函数的基本性质 `exp(a+b) = exp(a) * exp(b)`。通过乘以 `exp(m_old - m_new)` 这样的伸缩因子，我们能够无损地改变指数和的基准（即减去的最大值），从而将不同基准下的局部和正确地合并起来。
- **[FlashAttention](./Lecture5-FlashAttention.md)**: 这是 FlashAttention 能够在不物化 N×N 注意力矩阵的情况下计算精确注意力的数学基石。每个 QKᵀ 瓦片计算出的局部 Softmax 统计量，都通过这个在线更新机制，被逐步、正确地融入到最终的全局结果中。