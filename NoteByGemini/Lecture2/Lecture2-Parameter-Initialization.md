# 专题笔记: 参数初始化 (Parameter Initialization)

### 1. 核心概念

**参数初始化**是指在训练开始之前,为神经网络的权重(weights)和偏置(biases)赋予初始值的过程. 这是一个至关重要的步骤,因为一个好的初始化可以: 

*   **加速收敛**: 帮助模型更快地找到好的解. 
*   **避免梯度消失/爆炸**: 防止在深层网络中梯度变得过小或过大,从而保证有效的学习. 
*   **打破对称性**: 如果所有权重都初始化为相同的值(例如0),那么同一层中的所有神经元都会学习到相同的特征,模型将无法学习有用的表示. 

### 2. 问题: 糟糕的初始化

让我们考虑一个简单的线性层 `y = Wx + b`. 

*   **初始化为零**: 如果权重 `W` 全部初始化为0,那么无论输入 `x` 是什么,输出 `y` 在经过激活函数(如ReLU)之前都是0. 梯度也将是0,导致网络无法学习. 
*   **初始化为较大的随机数**: 如果权重 `W` 初始化自一个方差很大的分布(例如,标准正态分布 `N(0, 1)`),那么随着网络层数的加深,激活值的方差会逐层指数级增长. 这会导致: 
    *   对于 Sigmoid/Tanh 等饱和激活函数,输出会迅速落入梯度接近于0的饱和区. 
    *   对于 ReLU,会导致梯度爆炸. 
    *   最终结果是训练非常不稳定. 

**课程中的例子**: 讲座中展示了,如果一个权重矩阵 `W` (d x d) 用标准正态分布初始化,输入 `x` 也是标准正态分布,那么输出 `y = Wx` 的方差大约是 `d`. 当 `d`很大时(例如,`d=4096` 在大模型中很常见),输出值会变得非常大,导致训练不稳定. 

### 3. 解决方案: 方差缩放 (Variance Scaling)

现代参数初始化的核心思想是**方差缩放**: 通过精心设计权重的初始分布,使得每一层输出的方差与输入的方差大致相等,从而维持信号在网络中稳定传播. 

#### a. Xavier / Glorot 初始化

由 Xavier Glorot 和 Yoshua Bengio 在2010年提出,主要针对 Sigmoid 和 Tanh 这类对称的激活函数. 
其核心思想是,权重的方差应该与输入神经元数量 `fan_in` 和输出神经元数量 `fan_out` 的总和成反比. 

*   **均匀分布**: `W ~ U[-limit, limit]`,其中 `limit = sqrt(6 / (fan_in + fan_out))`
*   **正态分布**: `W ~ N(0, std^2)`,其中 `std = sqrt(2 / (fan_in + fan_out))`

#### b. He / Kaiming 初始化

由 Kaiming He 等人在2015年提出,专门为 **ReLU (Rectified Linear Unit)** 及其变体设计. 
ReLU 会将所有负数输入置为0,这相当于“杀死”了一半的神经元. 为了补偿这一信息损失,He 初始化只考虑输入神经元的数量 `fan_in`. 

*   **核心思想**: 权重的方差应该与输入神经元数量 `fan_in` 成反比. 
*   **正态分布**: `W ~ N(0, std^2)`,其中 `std = sqrt(2 / fan_in)`

**课程中的解决方案**: 讲座中提到的将权重除以 `sqrt(input_dim)`(即 `sqrt(fan_in)`)的做法,本质上就是 **He 初始化** 的一种简化形式. 这确保了经过权重矩阵相乘后,输出的方差能保持在1左右,从而避免了数值爆炸. 

### 4. PyTorch 中的实现

PyTorch 在 `torch.nn.init` 模块中提供了所有这些标准的初始化方法. 当你创建一个 `nn.Linear` 或 `nn.Conv2d` 层时,PyTorch 已经默认使用了 He 初始化的一个变种. 

你也可以手动进行初始化: 
```python
import torch
import torch.nn as nn
import math

# 创建一个线性层
layer = nn.Linear(512, 512)

# 手动进行 Kaiming (He) 正态初始化
nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')

# 手动进行 Xavier (Glorot) 均匀初始化
# nn.init.xavier_uniform_(layer.weight)

# 对于偏置,通常初始化为0
if layer.bias is not None:
    nn.init.constant_(layer.bias, 0)```

**课程中的截断正态分布**: 为了进一步增加稳定性,讲座中还提到了一个实践技巧,即使用**截断正态分布(Truncated Normal Distribution)**进行初始化. 这意味着从正态分布中采样后,任何超出某个范围(如-2到2个标准差之外)的值都会被丢弃并重新采样. 这可以防止极端的大值出现在初始权重中,为训练提供一个更安全的起点. 

### 5. 结论

正确的参数初始化是成功训练深度神经网络的第一步. 虽然现代框架已经提供了很好的默认设置,但理解其背后的“方差保持”原理对于诊断训练问题和设计新的网络架构仍然至关重要. **He 初始化**是目前与 ReLU 激活函数配合使用的标准和推荐方法. 

---
**关联知识点**
*   [nn.Module](./Lecture2-nn-Module.md)
*   [梯度 (Gradients)](./Lecture2-Gradients.md)