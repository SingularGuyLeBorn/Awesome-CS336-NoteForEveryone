# 专题笔记: 反向传播 (Backpropagation)

### 1. 核心概念

**反向传播(Backpropagation, BP)**是训练多层人工神经网络的核心算法.  它本质上是一种高效计算**[梯度](./Lecture2-Gradients.md)**(函数对所有变量的偏导数)的方法. 具体来说,反向传播是**应用链式法则(Chain Rule)**在神经网络上的一个巧妙实现. 

它的名字来源于其工作方式: 在计算出最终的损失(误差)后,将这个误差信号从网络的输出层**反向传播**回输入层,逐层计算损失函数对每一层参数(权重和偏置)的梯度. 

### 2. 为何需要反向传播？

训练一个神经网络的目标是调整其参数,以最小化一个**[损失函数](./Lecture2-Optimizers.md)**. 为了使用基于梯度的优化算法(如**[梯度下降](./Lecture2-Stochastic-Gradient-Descent.md)**),我们必须知道损失函数相对于每一个参数的梯度. 

*   **朴素方法(数值梯度)**: 我们可以对每个参数 `w` 分别进行微小的扰动 `h`,然后计算 `(L(w+h) - L(w)) / h`. 这种方法直观但计算成本极高,因为对于一个有数百万参数的模型,需要进行数百万次完整的前向传播. 
*   **反向传播的优势**: 反向传播算法极其高效. **它只需要进行一次前向传播和一次反向传播,就可以计算出损失函数对网络中所有参数的梯度. **

### 3. 反向传播的工作流程

整个过程可以分为两个阶段: 

#### a. 阶段一: 前向传播 (Forward Pass)
1.  输入数据从输入层开始,逐层向前传递. 
2.  在每一层,输入会与该层的权重进行线性组合(如矩阵乘法),加上偏置,然后通过一个非线性激活函数(如 ReLU). 
3.  该层的输出(激活值)成为下一层的输入. 
4.  这个过程一直持续到输出层,生成模型的最终预测. 
5.  将模型预测与真实标签进行比较,计算出总的损失值(一个标量). 

在进行前向传播的同时,系统会构建一个**计算图(Computation Graph)**,记录下所有操作和它们之间的依赖关系. 

#### b. 阶段二: 反向传播 (Backward Pass)
1.  **起点**: 从最终的损失值 `L` 开始. `L` 对自身的梯度 `∂L/∂L` 显然是 1. 
2.  **应用链式法则**: 算法沿着计算图,从后向前(从输出层到输入层)移动. 对于图中的每一个节点(代表一个操作),它会根据其后节点的梯度,计算出其前节点的梯度. 
    *   例如,如果 `z = f(y)` 且 `y = g(x)`,那么根据链式法则,损失 `L` 对 `x` 的梯度是 `∂L/∂x = ∂L/∂z * ∂z/∂y * ∂y/∂x`. 
    *   反向传播算法优雅地、系统地应用这个法则,将梯度“信息”一层层地传递回去. 
3.  **计算参数梯度**: 当梯度信号传播到某个包含可学习参数(如权重 `W`)的层时,就可以计算出损失 `L` 对该参数 `W` 的梯度 `∂L/∂W`. 
4.  **终点**: 当梯度一直传播到网络的输入层(或任何不再需要梯度的点),过程结束. 此时,所有可训练参数的 `.grad` 属性都已经被填充了正确的梯度值. 

### 4. 课程中的意义: 计算成本

在讲座中,我们了解到反向传播的计算成本大约是前向传播的两倍. 
*   **前向传播 ≈ 2 * P * D** (P=参数量, D=数据量)
*   **反向传播 ≈ 4 * P * D**

这是因为在反向传播过程中,为了计算关于权重 `W` 的梯度,我们通常需要用到在前向传播时计算出的激活值. 同时,为了将梯度进一步传播到前一层,我们还需要计算损失关于该层输入的梯度. 这两部分的计算量加起来,大致是前向传播计算量的两倍. 

**[PyTorch](./Lecture2-PyTorch.md)** 的 **[Autograd](./Lecture2-Autograd.md)** 引擎为我们自动处理了所有这一切复杂的计算,让我们可以只关注模型架构的设计. 

---
**关联知识点**
*   [梯度 (Gradients)](./Lecture2-Gradients.md)
*   [Autograd](./Lecture2-Autograd.md)
*   [优化器 (Optimizers)](./Lecture2-Optimizers.md)
*   [FLOPS (浮点运算)](./Lecture2-FLOPS.md)