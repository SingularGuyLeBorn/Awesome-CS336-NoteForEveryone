# 专题笔记: FLOPS (浮点运算)

### 1. 核心概念

**FLOPS** 是 "Floating-point Operations Per Second"(每秒浮点运算次数)的缩写. 它是一个衡量计算硬件性能速度的指标,尤其是在科学计算和深度学习领域. 

然而,在深度学习的语境中,这个术语经常被以两种 slightly different 的方式使用,区分它们至关重要: 

1.  **FLOPS (大写 "S")**: 指的是**计算速率**,单位是“次/秒”. 例如,"NVIDIA H100 的 BF16 理论性能约为 1000 TFLOPS",意味着它每秒能执行 1000 万亿次浮点运算. 
2.  **FLOPs (小写 "s")**: 指的是**计算量**,是一个无单位的计数值,表示完成某个任务(如一次模型前向传播)所需的总浮点运算次数.  例如,"训练 GPT-3 一次大约需要 3.14e23 FLOPs". 

**在本课程和通常的讨论中,为了避免混淆,我们倾向于: **
*   使用 **FLOPs** 或“**总FLOPs**”来表示计算量. 
*   使用 **FLOPS/s** 或直接写出单位(如 TFLOPS/s)来表示计算速度. 

### 2. 为何 FLOPS 在深度学习中如此重要？

*   **硬件无关的模型复杂度衡量标准**: FLOPs 提供了一种评估模型计算复杂度的标准化方法.  一个模型的 FLOPs 越高,通常意味着它在单次前向或反向传播中需要的计算就越多,也就越“昂贵”. 
*   **训练成本估算**: 总训练 FLOPs 是估算大模型训练项目所需时间和金钱的核心指标. 通过 `总训练FLOPs = 单次迭代FLOPs * 总迭代次数`,可以得出一个与具体硬件实现无关的总计算量. 
*   **性能瓶颈分析**: 通过比较模型的理论 FLOPs 和实际运行时间,我们可以计算出**[模型FLOPS利用率(MFU)](./Lecture2-MFU.md)**,从而判断我们的训练流程是否高效,硬件资源是否被充分利用. 

### 3. 如何计算 FLOPs？

在深度学习模型中,绝大多数的计算量来自于**矩阵乘法(Matrix Multiplication, MatMul)**和**卷积(Convolution)**. 

#### **矩阵乘法的FLOPs**

对于一个形状为 `(M, K)` 的矩阵 A 与一个形状为 `(K, N)` 的矩阵 B 相乘,得到一个形状为 `(M, N)` 的矩阵 C,即 `C = A @ B`. 

计算 C 中的每一个元素 `C_ij`,需要进行 K 次乘法和 K-1 次加法. 因此,近似为 `2 * K` 次浮点运算. 
由于矩阵 C 共有 `M * N` 个元素,所以总的计算量为: 

**FLOPs (MatMul) ≈ 2 * M * K * N**

这个 `2 * M * K * N` 的公式是进行所有“餐巾纸数学”的基石. 

### 4. 著名的 "6 * P * D" 规则

在课程中提到的估算训练总计算量的经验法则是: 

**总训练 FLOPs ≈ 6 * P * D**

其中: 
*   **P** 是模型的参数量 (Parameters). 
*   **D** 是训练数据集的总 Token 数 (Dataset size in tokens). 

这个公式的来源可以这样理解: 

1.  **前向传播 (Forward Pass)**: 对于一个典型的 Transformer 模型,其前向传播的计算量大致与参数量成正比. 粗略地,我们可以认为每个 Token 的前向传播需要 `2 * P` 的 FLOPs. (这里的“2”来源于矩阵乘法中的乘加运算). 
    *   **FLOPs_forward ≈ 2 * P * D**

2.  **反向传播 (Backward Pass)**: 根据经验和理论分析,**[反向传播](./Lecture2-Backpropagation.md)**计算梯度所需的计算量大约是前向传播的两倍. 
    *   **FLOPs_backward ≈ 2 * FLOPs_forward ≈ 4 * P * D**

3.  **总和**: 将前向和反向传播的计算量相加. 
    *   **FLOPs_total = FLOPs_forward + FLOPs_backward ≈ 2*P*D + 4*P*D = 6 * P * D**

这是一个非常实用的粗略估算,它忽略了像激活函数、层归一化等其他操作的计算量,因为在大模型中,这些操作的计算量相比于巨大的矩阵乘法可以忽略不计. 这个规则能帮助我们快速地对任意大模型的训练成本有一个量级上的概念. 

---
**关联知识点**
*   [MFU (模型FLOPS利用率)](./Lecture2-MFU.md)
*   [反向传播 (Backpropagation)](./Lecture2-Backpropagation.md)
*   [NVIDIA H100](./Lecture2-NVIDIA-H100.md)