# 专题笔记: 模型并行 (Model Parallelism)

### 1. 核心概念

**模型并行 (Model Parallelism)** 是一种分布式训练技术,用于训练那些**因为体积过大而无法装入单个 GPU 显存**的巨型模型. 与更常见的数据并行(Data Parallelism)不同,模型并行的核心思想是**将单个模型本身拆分到多个计算设备(GPU)上**. 

*   **数据并行 (Data Parallelism)**: 每个 GPU 都持有一份完整的模型副本. 训练数据被切分成多个批次,每个 GPU 独立处理一个批次的数据,最后通过梯度同步来更新所有模型副本. 这是最常见的并行策略,但它要求单个模型能放入单卡显存. 
*   **模型并行 (Model Parallelism)**: 当模型参数量和激活值大到单卡无法承受时,我们将模型的不同部分(如不同的层或层内的不同组件)放置在不同的 GPU 上. 数据在这些 GPU 之间以流水线的方式进行前向和反向传播. 

### 2. 为何需要模型并行？

随着**[Transformer](./Lecture2-Transformer.md)** 架构的成功,模型规模急剧膨胀,从数亿参数增长到数千亿甚至万亿参数. 让我们回顾一下单卡内存的构成: 

1.  **模型参数 (Parameters)**: 权重和偏置. 
2.  **梯度 (Gradients)**: 与参数大小相同. 
3.  **优化器状态 (Optimizer States)**: 使用 **[AdamW](./Lecture2-Adam-AdamW.md)** 时,大约是参数大小的两倍. 
4.  **激活值 (Activations)**: 前向传播过程中需要保存的中间结果,用于反向传播计算梯度. 其大小与 `批处理大小 * 序列长度 * 隐藏层维度 * 层数` 成正比,是主要的动态内存开销. 

当模型变得巨大时(例如,一个175B参数的GPT-3模型),即使使用 **[BF16](./Lecture2-FP32-FP16-BF16-FP8.md)** 精度,仅参数、梯度和优化器状态就需要 `175B * (2 + 2 + 4) = 1.4TB` 的显存,这远远超过了任何单张 GPU(如 **[H100](./Lecture2-NVIDIA-H100.md)** 的 80GB)的容量. 因此,模型并行成为训练这类模型的唯一选择. 

### 3. 主要的模型并行技术

#### a. 张量并行 (Tensor Parallelism)

*   **核心思想**: 将模型中的单个大张量(通常是权重矩阵)沿特定维度切分到多个 GPU 上. 计算时,每个 GPU 只处理自己持有的那部分切片,然后通过高效的通信操作(如 `all-gather` 或 `all-reduce`)来组合结果. 
*   **应用**: 在 Transformer 模型中,张量并行通常被应用于 `nn.Linear` 层和多头注意力模块. 例如,一个大的权重矩阵 `W` 可以按列切分到两个 GPU 上,`Y = XA` 就变成了 `Y = [Y1, Y2] = [XA1, XA2]`,其中 `A1` 和 `A2` 分别在 GPU 0 和 GPU 1 上. 
*   **优点**: 通信开销相对较低,可以很好地集成在模型层内部. 
*   **代表作**: NVIDIA 的 Megatron-LM 框架是张量并行的杰出实现. 

#### b. 流水线并行 (Pipeline Parallelism)

*   **核心思想**: 将模型的不同**层 (layers)** 分配到不同的 GPU 上,形成一个“流水线”. 例如,在一个4卡的设置中,GPU 0 可能负责第1-10层,GPU 1 负责11-20层,以此类推. 
*   **工作流程**: 
    1.  输入数据首先在 GPU 0 上完成前向传播. 
    2.  GPU 0 的输出(激活值)被发送到 GPU 1. 
    3.  当 GPU 1 正在处理这些数据时,GPU 0 可以开始处理下一个微批次(micro-batch)的数据. 
    4.  数据就这样像在工厂流水线一样依次流经所有 GPU. 反向传播也以类似但相反的顺序进行. 
*   **挑战**: “流水线气泡 (pipeline bubble)”. 在流水线的开始和结束阶段,很多 GPU 处于空闲等待状态,这降低了硬件的利用率(**[MFU](./Lecture2-MFU.md)**). 
*   **解决方案**: 通过将一个批次(batch)的数据再切分为多个**微批次(micro-batches)**并交错执行,可以显著减小气泡大小,提高效率. 
*   **代表作**: Google 的 Gpipe 和 NVIDIA 的 Megatron-LM 都实现了高效的流水线并行. 

#### c. 序列并行 (Sequence Parallelism)

*   **核心思想**: 当序列长度非常长时,激活值的内存占用会成为新的瓶颈. 序列并行旨在将输入序列本身沿序列长度维度进行切分. 
*   **挑战**: 自注意力机制要求每个 token 都能看到序列中的所有其他 token,这与序列切分是矛盾的. 
*   **解决方案**: 通过在自注意力计算之前使用 `all-gather` 操作来重构完整的序列,计算后再丢弃不需要的部分. 这需要精巧的通信和计算重排. 
*   **优点**: 可以有效降低因长序列导致的激活值内存压力. 

### 4. 3D 并行

在实践中,为了训练最大规模的模型,通常会将这三种并行策略结合起来使用,形成所谓的 **3D 并行**: 
*   **数据并行**: 在节点(node)之间复制模型. 
*   **流水线并行**: 在节点内的 GPU 之间划分模型层. 
*   **张量并行**: 在单个流水线阶段内的 GPU 之间切分张量. 

这种复杂的组合需要像 DeepSpeed、Megatron-LM 或 FSDP (Fully Sharded Data Parallel) 这样的高级分布式训练框架来管理. 

---
**关联知识点**
*   [Transformer](./Lecture2-Transformer.md)
*   [优化器 (Optimizers)](./Lecture2-Optimizers.md)
*   [NVIDIA H100](./Lecture2-NVIDIA-H100.md)
*   [MFU (模型FLOPS利用率)](./Lecture2-MFU.md)