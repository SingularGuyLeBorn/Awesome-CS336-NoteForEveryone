# 专题笔记: 模型量化 (Quantization)

### 1. 核心概念

**模型量化 (Model Quantization)** 是一种模型压缩和优化技术,其核心思想是**降低模型参数和/或激活值的数值精度**. 通常,这意味着将标准的32位浮点数(**[FP32](./Lecture2-FP32-FP16-BF16-FP8.md)**)或16位浮点数(**[FP16/BF16](./Lecture2-FP32-FP16-BF16-FP8.md)**)转换为位数更低的整数格式,最常见的是8位整数(INT8),但也有更激进的INT4甚至二值/三值网络. 

量化主要用于**模型推理(Inference)**阶段,而不是训练阶段,尽管也存在量化感知训练(Quantization-Aware Training)这样的技术. 

### 2. 为什么需要量化？

在模型训练完成后,我们通常希望将其部署到各种环境中,如云服务器、边缘设备(如手机、摄像头)或自动驾驶汽车. 在这些场景下,我们追求的是**低延迟、高吞吐量和低功耗**. 量化能够带来以下显著好处: 

1.  **减小模型体积**: 将FP32模型量化为INT8,模型在磁盘上和内存中的大小能直接减少约 **75%**. 这对于存储空间有限的边缘设备至关重要. 
2.  **降低内存带宽需求**: 更小的数据类型意味着从内存加载到计算单元所需的时间更短,这可以缓解推理过程中的内存带宽瓶颈. 
3.  **加速计算**: 现代CPU和GPU(包括专门的AI加速器)为低位宽整数运算提供了专门的、速度更快的硬件指令. INT8运算通常比FP32运算快得多. 
4.  **降低功耗**: 整数运算比浮点运算消耗的能量更少,这对于电池供电的移动设备尤其重要. 

### 3. 量化是如何工作的？

将浮点数转换为整数,需要一个**映射(mapping)**过程. 最简单的线性量化方案包括两个关键参数: 

*   **零点 (Zero-point)**: 将浮点数中的 `0.0` 映射到哪个整数值. 
*   **缩放因子 (Scale)**: 浮点数范围和整数范围之间的比例尺. 它定义了整数值增加1对应于浮点值增加多少. 

**映射公式**: 
`float_value = scale * (integer_value - zero_point)`

**量化过程 (Float -> Int)**: 
1.  确定待量化张量(如一层权重)的浮点数范围 `(min_float, max_float)`. 
2.  根据这个范围计算出 `scale` 和 `zero_point`. 
3.  使用公式将每个浮点数转换为整数,并进行裁剪和舍入,使其落在目标整数范围内(例如,INT8的范围是 [-128, 127]). 

**反量化过程 (Int -> Float)**: 
在需要进行浮点运算时(例如,与偏置相加后),再使用上述公式将整数转换回浮点数. 

### 4. 主要的量化技术

*   **训练后量化 (Post-Training Quantization, PTQ)**: 
    *   **过程**: 在模型已经训练好之后,直接对其进行量化. 
    *   **优点**: 实现简单、快速,不需要重新训练或原始训练数据. 
    *   **缺点**: 可能会有明显的精度损失,因为量化过程中的误差没有在训练中得到补偿. 
    *   **适用场景**: 对精度下降不敏感,或追求最快部署的应用. 

*   **量化感知训练 (Quantization-Aware Training, QAT)**: 
    *   **过程**: 在训练或微调(fine-tuning)过程中**模拟量化**操作. 具体来说,在前向传播时,权重和激活值会被“伪量化”(即: 量化后再反量化回浮点数),但反向传播的梯度计算仍然在高精度下进行. 
    *   **优点**: 模型在训练时就能“感知”到量化会带来的误差,并学着去适应它. 因此,QAT通常能达到比PTQ高得多的精度,有时甚至能与原始FP32模型持平. 
    *   **缺点**: 过程更复杂,需要额外的训练/微调时间和计算资源. 
    *   **适用场景**: 对精度要求非常高的应用. 

### 5. 训练 vs. 推理的精度差异

如课程中所述,在低精度下进行**训练**比在低精度下进行**推理**要困难得多. 

*   **训练的挑战**: 训练过程涉及到微小的梯度更新和累积. 低精度(如INT8)的离散性和有限范围使得精确表示和累积这些微小变化变得极其困难,很容易导致训练不稳定或完全失败. 
*   **推理的优势**: 一旦模型训练完成,其参数和激活值的分布通常会稳定在一个相对较小的范围内. 此时,我们可以找到一个很好的映射方案,用低精度整数来近似这些值,而不会丢失太多信息. 模型对这种轻微的扰动通常具有较好的鲁棒性. 

因此,常见的流程是: **使用相对较高且稳定的精度(如 [BF16](./Lecture2-FP32-FP16-BF16-FP8.md))进行训练,然后在推理部署时,采用更激进的量化策略(如INT8)来追求极致的效率. **

---
**关联知识点**
*   [FP32 / FP16 / BF16 / FP8](./Lecture2-FP32-FP16-BF16-FP8.md)
*   [混合精度训练 (Mixed Precision Training)](./Lecture2-Mixed-Precision-Training.md)