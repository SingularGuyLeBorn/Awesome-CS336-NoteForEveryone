# 专题：AdamW 优化器
## 1. 核心思想
**AdamW** (Adam with Weight Decay) 是一种对广泛使用的 Adam 优化器进行修正的算法，由 Ilya Loshchilov 和 Frank Hutter 在 2017 年的论文《Decoupled Weight Decay Regularization》中提出。
  
AdamW 的核心思想是：**将权重衰减（Weight Decay）与梯度更新解耦，以修正标准 Adam 优化器中 L2 正则化实现方式的缺陷。**
  
这个看似微小的改动，在实践中被证明能够显著改善模型的泛化能力，尤其是在训练像 [Transformer](./Lecture1-Transformer.md) 这样的大型模型时。如今，AdamW 已经取代了传统的 Adam，成为训练[语言模型](./Lecture1-Language-Models.md)的默认标准优化器。
## 2. 问题背景：Adam 中的权重衰减
在随机梯度下降（SGD）中，L2 正则化和权重衰减是等价的。L2 正则化是在损失函数中加入一项 `λ/2 * ||w||^2`，这在计算梯度时，会额外产生一项 `-λw`。而权重衰减则是直接在权重更新规则中减去一项 `λw`。对于 SGD，这两种做法效果完全相同。
  
然而，对于像 Adam 这样的**自适应学习率优化器**，情况就不同了。Adam 会为每个参数计算一个自适应的学习率，这个学习率会受到历史梯度大小的影响。
- **标准 Adam 的做法:** 将 L2 正则化项的梯度 `λw` 与原始的损失梯度 `g_t` 相加，然后一起送入 Adam 的自适应学习率计算机制中。
  - `g_t_eff = g_t + λw_t`
  - `w_{t+1} = w_t - α_t * m_t / (sqrt(v_t) + ε)`  (其中 `m_t` 和 `v_t` 是基于 `g_t_eff` 计算的)
- **问题:** 这种做法导致权重衰减的效果会受到历史梯度（`v_t`）的缩放影响。对于那些历史梯度较小（即更新不频繁）的权重，其权重衰减的效果会被放大；而对于历史梯度较大（更新频繁）的权重，其权重衰减的效果会被减弱。这与权重衰减的初衷（对所有权重施加同等比例的惩罚）相悖。
## 3. AdamW 的解决方案：解耦权重衰减
AdamW 采取了一种更直接、更符合直觉的方式：
1. **梯度计算:** 只使用原始损失函数的梯度 `g_t` 来更新 Adam 的一阶和二阶矩估计（`m_t` 和 `v_t`）。L2 正则化项被完全排除在自适应学习率的计算之外。
   - `m_t` 和 `v_t` 只由 `g_t` 决定。
2. **权重更新:** 在使用 Adam 计算出梯度更新量之后，**直接从当前权重中减去一个与学习率无关的衰减项**。
   - `w_{t+1} = w_t - α * (m_t / (sqrt(v_t) + ε) + λ * w_t)`
   - （更常见的实现方式是）`w_{t+1} = w_t * (1 - λ) - α_t * ...`
  
     这种解耦的方式确保了权重衰减的效果就像在 SGD 中一样，对所有权重都是一个固定的比例，与它们的历史梯度无关。这使得权重衰减成为一个更有效、更可预测的正则化器。
## 4. 伪代码对比
**标准 Adam (带 L2 正则化):**
````
g = grad(Loss + 0.5 * lambda * w^2)  # 梯度包含正则化项
m, v = update_moments(g)
w = w - lr * m / (sqrt(v) + eps)
````
**AdamW:**
````
g = grad(Loss)  # 梯度不包含正则化项
m, v = update_moments(g)
w = w - lr * m / (sqrt(v) + eps)  # Adam 自身的更新
w = w - lr * lambda * w            # 解耦的权重衰减
````
## 5. 实践意义
- **成为默认标准:** AdamW 已经成为 Hugging Face Transformers、PyTorch 等主流深度学习库中的默认优化器选项。
- **更好的泛化能力:** 在大量 NLP 和 CV 任务中，AdamW 被证明能够比标准 Adam 取得更低的测试错误率和更好的泛化性能。
- **超参数解耦:** 它使得学习率 `α` 和权重衰减率 `λ` 这两个超参数更加独立，更容易进行调整。
***
**关键论文:** [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101)