# 专题: N-gram 模型
## 1. 核心思想
**N-gram 模型**是一种简单而强大的统计**[语言模型](./Lecture1-Language-Models.md)**,在神经网络兴起之前,它是自然语言处理(NLP)领域的主导技术. 
其核心思想基于一个简化的假设,即**马尔可夫假设(Markov Assumption)**: **一个词出现的概率,只与它前面紧邻的 N-1 个词有关,而与更早的词无关. **
通过这个假设,预测一个长序列概率的复杂问题,被简化为一系列计算局部概率的简单问题. 
*   **Unigram (1-gram):** 词的出现是相互独立的,`P(w_i)`. 
*   **Bigram (2-gram):** 词的出现只依赖于前一个词,`P(w_i | w_{i-1})`. 
*   **Trigram (3-gram):** 词的出现依赖于前两个词,`P(w_i | w_{i-2}, w_{i-1})`. 
## 2. 工作原理
### 2.1 概率计算
以 Trigram 为例,一个句子 `S = (w_1, w_2, ..., w_k)` 的概率可以被近似计算为: 
`P(S) ≈ P(w_1) * P(w_2 | w_1) * Π_{i=3 to k} P(w_i | w_{i-2}, w_{i-1})`
而其中每一项的条件概率 `P(w_i | w_{i-2}, w_{i-1})` 都是通过在大型语料库中进行**最大似然估计(Maximum Likelihood Estimation, MLE)**得到的,即通过“数数”: 
`P(w_i | w_{i-2}, w_{i-1}) = Count(w_{i-2}, w_{i-1}, w_i) / Count(w_{i-2}, w_{i-1})`
例如,要计算 "the cat sat" 之后出现 "on" 的概率,我们就在语料库中统计 "the cat sat on" 出现了多少次,然后除以 "the cat sat" 出现了多少次. 
### 2.2 数据稀疏性与平滑技术
N-gram 模型面临一个核心的、致命的问题: **数据稀疏性(Data Sparsity)**. 
*   **问题:** 对于一个稍微大一点的 N(如 4 或 5),很多 N-gram 组合在训练语料库中可能从未出现过. 例如,`Count("Unicorns eat sparkly rainbows")` 很可能为 0. 根据上面的公式,这将导致整个句子的概率为 0,这是非常不合理的. 
*   **解决方案: 平滑(Smoothing)**. 平滑技术旨在将一些概率从见过的 N-gram 中“平分”一些给从未见过的 N-gram,确保没有任何组合的概率为绝对的 0. 
    *   **拉普拉斯平滑(Laplace/Add-one Smoothing):** 最简单的方法,给每个 N-gram 的计数都加 1. 
    *   **古德-图灵平滑(Good-Turing Smoothing):** 一种更复杂的统计方法,根据“出现过 r 次的 N-gram 的数量”来估计“未见过的 N-gram 的总概率”. 
    *   **Kneser-Ney 平滑:** 在当时最先进、最复杂的平滑技术,它考虑了词语在不同上下文中的出现情况. 
## 3. 优缺点分析
*   **优点:**
    *   **简单直观:** 算法基于计数,易于理解和实现. 
    *   **计算高效:** 模型的训练和使用都相对快速. 
    *   **可解释性强:** 模型的概率是基于明确的语料库统计数据. 
*   **缺点:**
    *   **数据稀疏性:** 即使有平滑技术,这个问题也无法根除,尤其当 N 增大时. 
    *   **无法捕捉长距离依赖:** 马尔可夫假设的本质决定了它只能看到一个很小的上下文窗口,无法理解句子中相距较远的词之间的语法或语义关系. 
    *   **无泛化能力:** 模型无法理解词语之间的语义相似性. 对于模型来说,"cat" 和 "kitty" 是两个完全不同的、毫无关联的符号. 它无法将在 "the cat sat" 上学到的知识泛化到 "the kitty sat". 
    *   **存储问题:** 随着 N 和词汇表的增大,存储所有 N-gram 计数的空间需求会爆炸式增长. 课程中提到,谷歌在 2007 年就训练过基于 2 万亿 token 的 5-gram 模型,其规模在当时是惊人的. 
## 4. 历史地位
尽管存在这些根本性的缺陷,N-gram 模型在 NLP 的历史中扮演了至关重要的角色,并在很长一段时间内是机器翻译、语音识别等应用的核心组件. 它的局限性也直接催生了后续**神经语言模型**的发展,这些模型通过**词嵌入(Word Embeddings)**来解决泛化问题,并通过 **RNN** 或 **[Transformer](./Lecture1-Transformer.md)** 来解决长距离依赖问题. 