# 专题: 语言模型 (Language Models)
## 1. 核心思想
语言模型 (Language Model, LM) 是一个对 token 序列的概率分布进行建模的数学模型. 其最核心的任务是: **给定一个 token 序列,预测下一个最可能出现的 token 是什么**. 
  
这个看似简单的任务,即 P(w_t | w_1, w_2, ..., w_{t-1}),是自然语言处理(NLP)领域几乎所有复杂任务的基础. 一个能够精确预测下一个词的模型,必然在某种程度上“理解”了语言的语法、语义、事实知识甚至推理能力. 
## 2. 发展历程
语言模型的发展可以大致分为几个阶段: 
1. **统计语言模型 (Statistical LMs):**
   - [N-gram 模型](./Lecture1-N-gram-%E6%A8%A1%E5%9E%8B.md)**:** 这是早期的主流方法. 它基于马尔可夫假设,认为一个词的出现只与它前面的 N-1 个词有关. 例如,一个 3-gram 模型会使用 P(w_t | w_{t-2}, w_{t-1}) 来近似真实概率. 
   - **优点:** 简单、可解释、计算效率高. 
   - **缺点:** 存在严重的数据稀疏问题(很多 N-gram 组合在训练集中从未出现),并且无法捕捉长距离依赖. 
2. **神经语言模型 (Neural LMs):**
   - **前馈神经网络 (Feed-Forward Neural Networks):** 由 Bengio 等人在 2003 年提出,首次使用神经网络来构建语言模型. 它将词语映射到连续的向量空间(词嵌入),从而缓解了数据稀疏问题,并能捕捉词语间的语义相似性. 
   - **循环神经网络 (RNNs):** 包括 LSTMs 和 GRUs,它们通过一个循环的隐藏状态来处理序列,理论上可以捕捉任意长度的依赖关系. 这使得 [Seq2Seq 模型](./Lecture1-Seq2Seq-%E6%A8%A1%E5%9E%8B.md) 在机器翻译等任务上取得了巨大成功. 
   - **缺点:** RNN 的顺序计算方式使其难以并行化,且存在梯度消失/爆炸问题,导致长距离依赖的捕捉能力在实践中仍然有限. 
3. **预训练语言模型 (Pre-trained LMs):**
   - [BERT](./Lecture1-BERT.md)**:** 标志着一个新时代的开始. 它使用 [Transformer](./Lecture1-Transformer.md) 的编码器部分,并通过“掩码语言模型”(Masked Language Model)任务在海量无标签文本上进行预训练. 预训练好的模型可以针对下游任务进行微调,极大地提升了各种 NLP 任务的性能. 
   - **GPT 系列:** 由 OpenAI 开发,使用 [Transformer](./Lecture1-Transformer.md) 的解码器部分,专注于经典的“下一个词预测”任务. 从 [GPT-2](./Lecture1-GPT-4.md) 开始,人们发现大规模的 GPT 模型展现出了惊人的少样本(few-shot)甚至零样本(zero-shot)学习能力. 
4. **大语言模型 (Large Language Models, LLMs):**
   - [GPT-3](./Lecture1-GPT-4.md)/[GPT-4](./Lecture1-GPT-4.md)**:** 等模型的出现,展示了当模型规模(参数量、数据量、计算量)达到某个阈值后,会产生**[涌现能力](./Lecture1-Emergent-Behavior.md)**,如上下文学习、复杂推理等. 
   - LLMs 不再仅仅是 NLP 的一个组件,而是成为了一个通用的、可交互的智能体. 它们通过[监督式微调](./Lecture1-Supervised-Fine-Tuning.md)**和**[从人类反馈中强化学习 (RLHF)](./Lecture1-RLHF.md)等对齐技术,变得更加有用和安全. 
## 3. 核心分类
根据其架构和训练方式,现代语言模型主要分为三类: 
- **仅编码器模型 (Encoder-only):** 如 [BERT](./Lecture1-BERT.md). 它们能够看到整个输入序列(双向上下文),非常适合做自然语言理解(NLU)任务,如文本分类、情感分析、命名实体识别. 
- **仅解码器模型 (Decoder-only):** 如 GPT 系列. 它们是自回归的,只能看到当前位置之前的内容(单向上下文),天生适合做文本生成(NLG)任务. 这是当前大语言模型的主流架构. 
- **编码器-解码器模型 (Encoder-Decoder):** 如 T5、BART 和原始的 [Transformer](./Lecture1-Transformer.md). 它们结合了前两者的特点,非常适合处理输入和输出都是序列的任务,如机器翻译、文本摘要. 
## 4. 意义与影响
语言模型,特别是大语言模型,已经成为人工智能领域最具变革性的技术之一. 它们不仅在自然语言处理领域取得了突破,还在代码生成、科学发现、多模态交互等众多领域展现出巨大潜力,正在重塑人机交互的范式. 