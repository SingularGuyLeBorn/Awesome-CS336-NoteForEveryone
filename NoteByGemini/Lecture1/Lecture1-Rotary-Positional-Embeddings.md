# 专题：旋转位置编码 (Rotary Positional Embeddings, RoPE)
## 1. 核心思想
**旋转位置编码 (Rotary Positional Embeddings, RoPE)**，由 Su 等人在 2021 年的论文《RoFormer: Enhanced Transformer with Rotary Position Embedding》中提出，是目前最先进、最主流的**[位置编码](./Lecture1-Positional-Encoding.md)**方法之一。
RoPE 的核心思想是：**利用数学上的复数乘法性质，将绝对位置信息以一种相对的方式、优雅地编码进自注意力机制中。**
具体来说，它通过在**[注意力机制](./Lecture1-Self-Attention.md)**计算相似度的点积操作之前，对查询（Query）向量和键（Key）向量进行旋转，从而将位置信息注入模型。旋转的角度取决于 token 在序列中的绝对位置。
## 2. 工作原理
想象一下，查询向量 `q` 和键向量 `k` 不再是实数向量，而是二维复平面上的向量。
1.  **位置编码的定义:** 对于序列中的第 `m` 个位置，RoPE 定义了一个旋转矩阵（或等效的复数）：`R_m = e^(i * mθ)`，其中 `θ` 是一个预设的常数。
2.  **对 Q 和 K 进行旋转:**
    *   在计算注意力得分之前，将位于第 `m` 个位置的查询向量 `q_m` 乘以旋转矩阵 `R_m`。
    *   将位于第 `n` 个位置的键向量 `k_n` 乘以旋转矩阵 `R_n`。
3.  **计算点积:**
    *   计算旋转后的查询和键的点积：`<q_m * R_m, k_n * R_n>`。
    *   根据复数乘法的性质，这个点积的结果只依赖于 `q_m` 和 `k_n` 的原始点积以及它们之间的**相对位置 `m-n`**。
    *   `<q_m * R_m, k_n * R_n> = Re[ (q_m * e^(i*mθ)) * (k_n * e^(i*nθ))^* ] = Re[ (q_m * k_n^*) * e^(i*(m-n)θ) ]`
    *   其中 `Re[...]` 表示取实部，`*` 表示共轭。
这个公式的绝妙之处在于，**绝对位置 `m` 和 `n` 被消去了，只留下了相对位置 `m-n`**。这意味着，模型在计算两个 token 之间的注意力得分时，能够天然地、隐式地考虑到它们的相对距离。
### 在高维空间中的实现
在实践中，**[Transformer](./Lecture1-Transformer.md)** 的 Q 和 K 向量是高维的（例如 128 维）。RoPE 的做法是将高维向量两两配对，看作是一系列二维向量，然后对每个二维子空间应用上述的旋转操作。不同的二维子空间会使用不同的旋转频率 `θ_i`，这些频率构成一个几何级数，类似于原始 Transformer 中的正弦位置编码。
## 3. RoPE 的优势
相比于其他位置编码方法，RoPE 具有几个显著的优势：
*   **天然的相对位置编码:** 它将相对位置信息直接融入了注意力计算的核心，这是自注意力机制最需要的信息类型。这比那些将绝对位置编码简单相加的方法更为优雅和有效。
*   **良好的外推性 (Extrapolation):** 由于其编码的是相对关系，RoPE 在处理比训练时更长的序列时，表现出比其他方法（特别是绝对位置编码）更好的泛化能力。即使模型没见过位置 2048，它也能理解位置 2048 和 2047 之间的关系，就像它理解 1024 和 1023 之间的关系一样。
*   **不增加模型参数:** RoPE 是一种“在飞行中”（on-the-fly）计算并应用的位置编码，它本身不包含任何可训练的参数。
*   **与线性注意力兼容:** 它可以无缝地与各种线性注意力变体结合，提升其性能。
## 4. 实践与影响
由于其卓越的性能和优雅的数学性质，RoPE 已经迅速成为最先进的**[语言模型](./Lecture1-Language-Models.md)**的标配，被 LLaMA、PaLM、Mistral、Qwen 等几乎所有主流的开源和闭源模型所采用。它被认为是 **[Transformer](./Lecture1-Transformer.md)** 架构近代最重要的改进之一。
---
**关键论文:** [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)