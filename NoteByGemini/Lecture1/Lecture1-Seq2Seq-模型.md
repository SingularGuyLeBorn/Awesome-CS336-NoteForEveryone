# 专题：Seq2Seq 模型
## 1. 核心思想
**Seq2Seq(Sequence-to-Sequence,序列到序列)模型**是一种深度学习模型架构,专门用于处理输入和输出都是可变长度序列的任务. 它由 Google 的 Sutskever 等人和 Yoshua Bengio 的团队在 2014 年几乎同时独立提出,并首先在机器翻译领域取得了巨大成功. 
Seq2Seq 模型的核心思想是：**使用一个编码器(Encoder)将整个输入序列压缩成一个固定长度的上下文向量(Context Vector),然后用另一个解码器(Decoder)从这个上下文向量中“解码”出整个输出序列. **
这个“编码-解码”的框架非常通用,可以应用于任何输入输出为序列的任务,如：
*   **机器翻译:** 输入一种语言的句子,输出另一种语言的句子. 
*   **文本摘要:** 输入一篇长文章,输出一个短摘要. 
*   **对话系统:** 输入一个问题,输出一个回答. 
*   **语音识别:** 输入一段音频的声学特征序列,输出对应的文字序列. 
## 2. 架构：编码器-解码器
Seq2Seq 模型主要由两个基于循环神经网络(Recurrent Neural Network, RNN),通常是 LSTM 或 GRU 的组件构成. 
### 2.1 编码器 (Encoder)
*   **作用:** 读取并理解输入序列. 
*   **过程:** 编码器是一个 RNN. 它逐个地读取输入序列中的每一个 token. 在每一步,它都会更新自己的隐藏状态(hidden state). 当读取完整个输入序列后,编码器最后的那个隐藏状态,就被认为是整个输入序列的语义概括,即**上下文向量(Context Vector)**. 
*   **这个上下文向量,在理论上,应该包含了输入序列的所有关键信息. **
### 2.2 解码器 (Decoder)
*   **作用:** 根据上下文向量生成输出序列. 
*   **过程:** 解码器是另一个 RNN. 它的初始隐藏状态被设置为编码器生成的上下文向量. 
    1.  解码器首先接收一个特殊的起始符 `[GO]` 作为输入. 
    2.  根据其当前的隐藏状态(初始为上下文向量)和输入(`[GO]`),它会预测输出序列的第一个 token. 
    3.  然后,在下一步,它将上一步生成的 token 作为新的输入,并更新自己的隐藏状态,接着预测第二个 token. 
    4.  这个过程不断重复,直到解码器生成一个特殊的终止符 `[EOS]`,表示序列生成结束. 
## 3. 引入注意力机制 (Attention Mechanism)
基础的 Seq2Seq 模型存在一个严重的瓶颈：**编码器必须将输入序列的所有信息都压缩到一个固定长度的上下文向量中. ** 对于长句子来说,这会导致严重的信息丢失. 
为了解决这个问题,Bahdanau 等人在 2014 年提出了**注意力机制(Attention Mechanism)**,这是对 Seq2Seq 模型的革命性改进. 
*   **核心思想:** 解码器在生成每个 token 时,不应该只依赖于那个单一的、固定的上下文向量. 相反,它应该被允许“回头看”并“关注”输入序列中所有位置的隐藏状态. 
*   **过程:**
    1.  编码器不再只是输出最后一个隐藏状态,而是输出**每一个时间步的隐藏状态**. 
    2.  在解码的每一步,解码器都会根据自己当前的隐藏状态,去和编码器的所有隐藏状态进行一次匹配计算,得出一组**注意力权重**. 
    3.  这组权重表示了在生成当前这个输出 token 时,输入序列的哪些部分是最重要的. 
    4.  解码器使用这组权重,对编码器的所有隐藏状态进行加权求和,得到一个**动态的、针对当前解码步骤的上下文向量**. 
    5.  最后,解码器利用这个动态的上下文向量来预测下一个 token. 
## 4. 历史地位与影响
*   **奠定了现代 NLP 架构的基础:** 引入了注意力机制的 Seq2Seq 模型,是 **[Transformer](./Lecture1-Transformer.md)** 架构的直接前身. 《Attention Is All You Need》的论文正是将 Seq2Seq 框架中的 RNN 完全替换为自注意力层,从而创造了 Transformer. 
*   **在多任务上取得突破:** Seq2Seq 模型首次证明了,一个单一的、端到端的深度学习模型可以成功地处理各种复杂的序列转换任务,极大地推动了 NLP 领域的发展. 
尽管现在大部分任务已被 Transformer 所取代,但 Seq2Seq 的“编码器-解码器”思想,以及它与注意力机制的结合,仍然是理解现代 NLP 模型演进历史中不可或`缺的一环. 