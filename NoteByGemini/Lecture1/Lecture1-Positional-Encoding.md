# 专题：位置编码 (Positional Encoding)
## 1. 问题背景
**[自注意力机制](./Lecture1-Self-Attention.md)**的核心是一个集合操作（set operation）。它在计算一个 token 的表示时，平等地看待序列中的所有其他 token，而忽略了它们的顺序。换句话
说，对于自注意力层而言，句子 "The cat sat on the mat" 和 "The mat sat on the cat" 的输入是等价的，因为它无法区分词语的位置。
然而，在自然语言中，词语的顺序是至关重要的。为了解决这个问题，**[Transformer](./Lecture1-Transformer.md)** 必须引入一种机制来将 token 的位置信息注入到模型中。**位置编码（Positional Encoding）**正是为此而生。
其核心思想是：**创建一个与 token 的位置相关的、特定模式的向量，并将其添加到输入 token 的词嵌入（word embedding）中。**
## 2. 原始 Transformer 的正弦/余弦位置编码
在经典的《Attention Is All You Need》论文中，作者提出了一种巧妙的、无需学习的位置编码方法，它使用正弦（sine）和余弦（cosine）函数来生成位置向量。
对于一个在序列中位置为 `pos`、在嵌入向量中维度为 `i` 的元素，其位置编码 `PE` 的计算公式如下：
*   **当 `i` 是偶数时:** `PE(pos, 2i) = sin(pos / 10000^(2i / d_model))`
*   **当 `i` 是奇数时:** `PE(pos, 2i+1) = cos(pos / 10000^(2i / d_model))`
其中 `d_model` 是词嵌入的维度。
### 为什么选择这个函数？
这个看似复杂的函数具有一些非常优秀的性质：
1.  **唯一性:** 每个位置 `pos` 都有一个独一无二的位置编码向量。
2.  **确定性:** 它不是随机生成的，也不需要学习，可以被提前计算好。
3.  **蕴含相对位置信息:** 最关键的性质是，对于任意的固定偏移量 `k`，`PE(pos + k)` 可以表示为 `PE(pos)` 的一个线性变换。这意味着模型可以很容易地学习到 token 之间的相对位置关系。例如，`pos+k` 位置的编码是 `pos` 位置编码和 `k` 位置编码的某种组合，这使得注意力机制可以关注到相对距离。
4.  **良好的外推性:** 即使模型在训练时只见过长度为 `L` 的序列，理论上它也能生成比 `L` 更长的序列的位置编码。
## 3. 其他位置编码方法
尽管正弦/余弦位置编码非常经典，但后续的研究者们也提出了多种替代方案，试图更有效地编码位置信息。
### 3.1 可学习的绝对位置编码 (Learned Absolute Positional Embeddings)
*   **方法:** 这是最简单的一种方法，被 **[BERT](./Lecture1-BERT.md)** 和 **[GPT-2](./Lecture1-GPT-4.md)** 等早期模型采用。它创建一个位置编码矩阵 `P`，大小为 `(max_sequence_length, d_model)`。矩阵的第 `i` 行就是第 `i` 个位置的编码向量。这个矩阵像词嵌入一样，是可学习的参数，在训练过程中通过梯度下降进行优化。
*   **优点:** 简单，模型可以自己学习出最优的位置表示。
*   **缺点:** **外推性差**。模型无法处理比训练时 `max_sequence_length` 更长的序列，因为没有对应的位置编码。
### 3.2 相对位置编码 (Relative Positional Embeddings)
*   **思想:** 这类方法认为，绝对位置本身不重要，重要的是两个 token 之间的相对距离。
*   **方法:** 它们直接在**[注意力机制](./Lecture1-Self-Attention.md)**的计算过程中修改注意力得分。例如，在计算 `query_i` 和 `key_j` 的注意力得分时，额外加上一个只与相对距离 `i-j` 有关的可学习偏置项。
*   **代表:** T5 模型。
### 3.3 旋转位置编码 (Rotary Positional Embeddings, RoPE)
*   **方法:** 这是目前最先进和主流的方法，被 LLaMA、PaLM 等模型采用。详情请参考 **[RoPE 专题笔记](./Lecture1-Rotary-Positional-Embeddings.md)**。
*   **核心思想:** RoPE 通过将查询（Query）和键（Key）向量在复数空间中进行“旋转”来注入位置信息，旋转的角度由 token 的绝对位置决定。这种方法巧妙地使得最终的注意力得分只依赖于 token 间的相对位置。
*   **优点:** 结合了绝对位置编码的形式和相对位置编码的思想，具有极佳的性能和外推性。
**结论:** 位置编码是使 **[Transformer](./Lecture1-Transformer.md)** 能够理解序列顺序的关键技术。从最初的绝对位置编码，到后来的相对位置编码，再到如今主流的 RoPE，其演进过程反映了研究者们对如何将位置信息最有效地融入自注意力机制的不断深化理解。