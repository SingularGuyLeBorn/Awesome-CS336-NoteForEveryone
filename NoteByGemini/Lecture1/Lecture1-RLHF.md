# 专题：从人类反馈中强化学习 (RLHF)
## 1. 核心思想
**从人类反馈中强化学习 (Reinforcement Learning from Human Feedback, RLHF)** 是继**[监督式微调 (SFT)](./Lecture1-Supervised-Fine-Tuning.md)**之后,对**[语言模型](./Lecture1-Language-Models.md)**进行对齐(Alignment)的第二个关键阶段. 
其核心思想是：**不再依赖于昂贵的、由人类撰写的“标准答案”,而是利用更轻量级、更易于获取的人类偏好数据(例如,在两个回答中,哪个更好？),来训练一个“奖励模型”(Reward Model),然后用这个奖励模型作为信号,通过强化学习算法来进一步优化语言模型. **
RLHF 的目标是让模型的输出更符合人类复杂的、难以用规则精确描述的价值观,例如“有用性”、“无害性”和“真实性”. 
## 2. RLHF 的三阶段流程
RLHF 的经典流程(由 InstructGPT 和 ChatGPT 推广)通常包含三个步骤：
### 阶段 1: 监督式微调 (SFT)
*   **目标:** 教会模型基本的指令遵循能力. 
*   **过程:** 与**[SFT 专题笔记](./Lecture1-Supervised-Fine-Tuning.md)**中描述的完全相同. 使用一个高质量的“指令-回答”数据集来微调预训练的基础模型. 这个 SFT 模型是后续所有步骤的起点. 
### 阶段 2: 训练奖励模型 (Reward Model, RM)
*   **目标:** 学习一个能够模仿人类偏好的打分函数. 
*   **过程:**
    a. **收集偏好数据:** 针对同一个用户指令(prompt),使用 SFT 模型生成多个不同的回答(例如 2 到 4 个). 
    b. **人类排序:** 让人类标注员对这些回答进行排序,指出哪个最好,哪个次之,等等. 
    c. **构建偏好对:** 将排序数据转换成成对的比较数据. 例如,如果排序是 `C > A > B`,那么可以得到 `(prompt, chosen=C, rejected=A)` 和 `(prompt, chosen=A, rejected=B)` 等数据对. 
    d. **训练奖励模型:** 奖励模型本身通常是一个与 SFT 模型架构类似但规模较小的语言模型. 它的任务是接收一个 `(prompt, response)` 对,输出一个标量分数,代表这个回答有多“好”. 训练的目标是,对于每一个偏好对 `(chosen, rejected)`,RM 给 `chosen` 的打分要高于给 `rejected` 的打分. 这通常通过一个特定的损失函数(如 Bradley-Terry 模型)来实现. 
### 阶段 3: 使用强化学习进行优化
*   **目标:** 利用奖励模型作为指导,优化 SFT 模型的策略,使其能生成获得更高奖励的回答. 
*   **过程:**
    a. **强化学习设置:**
        *   **策略 (Policy):** 当前的语言模型. 
        *   **动作空间 (Action Space):** 整个词汇表,即在每一步生成一个 token. 
        *   **状态 (State):** 到目前为止已经生成的 token 序列. 
        *   **奖励 (Reward):** 对于一个最终生成的完整回答,由阶段 2 的奖励模型给出的分数. 
    b. **优化算法:** 使用强化学习算法(如**[近端策略优化, PPO](./Lecture1-PPO.md)**)来更新语言模型的参数. 
    c. **KL 散度惩罚:** 在优化过程中,除了要最大化奖励模型的打分,还需要加入一个惩罚项,确保优化后的模型不会与原始的 SFT 模型偏离太远. 这通常通过计算两个模型输出分布之间的 KL 散度来实现. 这个惩罚项至关重要,它可以防止模型为了追求高分而生成一些无意义或偏离原始数据分布的文本(所谓的“模式崩溃”),并保留模型在预训练和 SFT 阶段学到的语言能力. 
## 3. 意义与影响
*   **超越 SFT:** RLHF 能够捕捉比 SFT 更细微、更复杂的人类偏好,使得模型在“帮助性”和“安全性”上达到新的高度. ChatGPT 的巨大成功很大程度上归功于其高质量的 RLHF 流程. 
*   **可扩展的对齐方法:** 相比于撰写标准答案,提供偏好排序的成本更低,更容易规模化,这使得 RLHF 成为一种更具可扩展性的对齐方法. 
*   **成为行业标准:** RLHF 已成为训练顶尖聊天机器人的行业标准流程. 
后续的研究也提出了如**[直接偏好优化 (DPO)](./Lecture1-DPO.md)** 等更简单、更稳定的替代方案,它们试图在不显式训练奖励模型和使用复杂强化学习的情况下,直接从偏好数据中优化语言模型. 