# 专题：注意力机制 (Self-Attention)
## 1. 核心思想
**自注意力(Self-Attention)**,也称为内部注意力(intra-attention),是 **[Transformer](./Lecture1-Transformer.md)** 模型的核心驱动力,也是其能够捕捉长距离依赖并进行并行计算的关键. 
其核心思想是：**一个序列中的每个 token,都可以“关注”到序列中所有其他的 token(包括它自己),并根据相关性计算出一个加权的上下文表示. **
换句话说,为了计算一个词的新表示,自注意力机制会动态地、带权重地聚合序列中所有词的信息. 权重的大小,代表了其他词对于理解当前这个词的重要性. 
## 2. 查询-键-值(Query-Key-Value, QKV)模型
自注意力的计算过程通常通过查询(Query)、键(Key)和值(Value)这三个概念来描述. 这三个向量都是由同一个输入 token 的嵌入向量,通过乘以不同的可学习权重矩阵(`W_Q`, `W_K`, `W_V`)得到的. 
1.  **查询 (Query, Q):** 代表了当前 token 为了理解自己,主动发出的“提问”. 
2.  **键 (Key, K):** 代表了序列中每个 token 准备被查询的“标签”或“索引”,用于和查询进行匹配. 
3.  **值 (Value, V):** 代表了序列中每个 token 实际携带的“内容”或信息. 
可以将这个过程类比于在图书馆(数据库)中检索信息：
*   你手里的**查询(Query)**是一张写着问题的纸条. 
*   图书馆里每一本书的**键(Key)**是书的标题或摘要. 
*   每一本书的**值(Value)**是书的实际内容. 
*   你用你的**查询**去和所有书的**键**做匹配,匹配度越高的书,你就会花越多的“注意力”去阅读它的**值**(内容). 最后,你把所有阅读到的内容根据重要性(注意力权重)汇总起来,形成你对问题的最终理解. 
## 3. 计算步骤
自注意力的计算可以分解为以下步骤：
1.  **生成 Q, K, V 向量:**
    *   对于输入序列中的每一个 token 嵌入 `x_i`,计算其对应的 `q_i, k_i, v_i`：
        *   `q_i = W_Q * x_i`
        *   `k_i = W_K * x_i`
        *   `v_i = W_V * x_i`
2.  **计算注意力得分 (Attention Scores):**
    *   对于要计算其新表示的 token `i`,用它的查询 `q_i` 与序列中所有其他 token `j` 的键 `k_j` 进行点积运算. 
    *   `score(i, j) = q_i · k_j`
    *   这个得分衡量了 token `i` 应该对 token `j` 投入多少关注. 
3.  **缩放 (Scaling):**
    *   将上一步得到的得分除以一个缩放因子,通常是 `sqrt(d_k)`,其中 `d_k` 是键向量的维度. 
    *   **作用:** 这个缩放操作是为了防止点积结果过大,导致后续 Softmax 函数进入梯度非常小的区域,从而使得训练过程不稳定. 
4.  **计算注意力权重 (Attention Weights):**
    *   将缩放后的得分通过一个 Softmax 函数,将其归一化为和为 1 的概率分布. 
    *   `weights_ij = softmax(score(i, j) / sqrt(d_k))`
5.  **加权求和:**
    *   用得到的注意力权重 `weights_ij` 去加权求和序列中所有 token 的值向量 `v_j`,得到 token `i` 的最终输出表示 `z_i`. 
    *   `z_i = Σ_j (weights_ij * v_j)`
这个过程对序列中的每一个 token 都并行地进行,最终得到整个序列的新表示. 
## 4. 多头注意力 (Multi-Head Attention)
为了让模型能够同时关注来自不同表示子空间的信息(例如,一个头关注语法关系,另一个头关注语义关系),Transformer 引入了**多头注意力**. 
其做法是：
*   将 Q, K, V 向量的维度切分成 `h` 个“头”(heads). 
*   在每个头内部,独立地、并行地执行上述的自注意力计算过程. 
*   将 `h` 个头得到的输出向量拼接(concatenate)起来. 
*   通过一个最终的线性变换,将拼接后的向量投影回原始的维度. 
多头机制极大地增强了注意力层的表达能力. 
## 5. 优势
*   **并行计算:** 注意力计算不依赖于前一时刻的输出,可以对整个序列并行处理. 
*   **长距离依赖:** 序列中任意两个 token 之间的交互路径长度都是 O(1),能有效捕捉长距离依赖. 
*   **可解释性:** 注意力权重矩阵在一定程度上提供了模型决策的可解释性. 
**缺点:**
*   **计算复杂度:** 注意力矩阵的计算和存储复杂度是序列长度 `N` 的平方(O(N^2)),这使得它在处理超长序列时面临挑战. 像 **[FlashAttention](./Lecture1-FlashAttention.md)** 这样的技术正是为了缓解这个问题而设计的. 