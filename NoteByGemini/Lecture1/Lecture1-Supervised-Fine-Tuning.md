# 专题：监督式微调 (Supervised Fine-Tuning, SFT)
## 1. 核心思想
**监督式微调 (Supervised Fine-Tuning, SFT)**,也常被称为指令微调(Instruction Tuning),是**[语言模型](./Lecture1-Language-Models.md)**对齐(Alignment)过程中的第一个关键阶段. 
其核心思想是：**使用一个由高质量的“指令-回答”(Instruction-Response)对组成的数据集,以标准的监督学习方式来微调一个已经经过预训练的基础模型(Base Model). **
这个过程的目的是教会模型**如何遵循人类的指令**. 一个未经 SFT 的基础模型,尽管知识渊博,但它只会进行“文本补全”(text completion),而不是“指令遵循”(instruction following). 例如,如果你给基础模型输入“写一首关于月亮的诗”,它可能会续写成“……是谁在去年提出的问题？”,而不是直接写诗. 
SFT 的目标就是弥合这种行为上的差距,让模型学会以一种有用的、对话式的、遵循指令的方式与用户互动. 
## 2. SFT 的工作流程
1.  **获取一个预训练的基础模型:**
    *   这通常是一个已经通过自监督学习在海量文本(如 **[Common Crawl](./Lecture1-Common-Crawl.md)**、书籍、代码)上训练过的模型,如 LLaMA、Mistral 等. 这个模型已经具备了丰富的世界知识和语言能力. 
2.  **构建高质量的指令数据集:**
    *   这是 SFT 中最关键、成本最高的一步. 数据集由成千上万个样本组成,每个样本都是一个 `(prompt, response)` 对. 
    *   **Prompt (指令):** 模拟用户可能提出的各种问题或要求,例如“解释一下什么是黑洞？”、“用三句话总结一下哈姆雷特的剧情”、“帮我写一封请假邮件”. 
    *   **Response (回答):** 由人类标注员精心撰写的、理想的、高质量的回答. 
    *   **数据来源:** 可以是人工撰写,也可以是利用强大的模型(如 **[GPT-4](./Lecture1-GPT-4.md)**)生成初稿,再由人类进行修改和完善. 数据集的多样性(涵盖不同领域、不同任务类型)和高质量至关重要. 
3.  **进行监督式学习:**
    *   将指令和回答拼接成一个序列,然后使用与预训练时相同的“下一个词预测”损失函数来微调模型. 
    *   **关键点:** 通常只计算回答部分的损失(Loss Masking). 也就是说,模型在预测指令部分的 token 时,其产生的损失不计入梯度更新. 模型被强制要求在看到完整的指令后,以最大的概率生成出标准的、高质量的回答. 
    *   这个过程通常不需要像预训练那样耗费巨大的计算资源,因为指令数据集的规模相对较小(数万到数十万级别),并且模型只需微调少量轮次(epochs). 
## 3. 效果与意义
*   **解锁指令遵循能力:** SFT 是让模型从一个“知识库”转变为一个“智能助手”的关键步骤. 一篇名为《The Flan Collection》的著名论文表明,通过在大量不同任务的指令上进行微调,模型可以展现出惊人的泛化能力,能够解决它在 SFT 阶段从未见过的全新任务. 
*   **注入特定风格和知识:** SFT 也是向模型注入特定对话风格、角色扮演能力或特定领域知识的有效手段. 
*   **为 RLHF 做准备:** SFT 后的模型是进行后续**[从人类反馈中强化学习 (RLHF)](./Lecture1-RLHF.md)** 的起点. 一个好的 SFT 模型能够产生足够多样和高质量的回答,为后续的偏好学习提供良好的初始策略. 
## 4. 挑战
*   **数据质量决定上限:** SFT 的效果高度依赖于指令数据集的质量. 如果数据中存在事实错误、偏见或不好的风格,这些都会被模型“学会”. 
*   **对齐税 (Alignment Tax):** 在某些情况下,经过 SFT 后,模型在一些基准学术测试上的性能可能会略有下降. 这被认为是模型将一部分“能力”用于“学习如何与人对话”而付出的代价. 
*   **灾难性遗忘 (Catastrophic Forgetting):** 如果微调不当(如学习率过高),模型可能会忘记在预训练阶段学到的知识. 
尽管存在挑战,SFT 仍然是目前将大语言模型变得实用、可控和有用的最有效、最直接的方法之一. 