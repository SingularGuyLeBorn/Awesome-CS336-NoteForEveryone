# 专题: 推理 (Inference)
## 1. 核心定义
**推理 (Inference)** 是指使用一个已经训练好的**[语言模型](./Lecture1-Language-Models.md)**,根据给定的输入(称为 prompt 或提示),来生成输出(通常是文本)的过程. 
与训练(Training)相比,推理有几个显著不同的特点: 
*   **目标不同:** 训练的目标是学习模型参数,而推理的目标是利用固定的参数来执行任务. 
*   **成本模式不同:** 训练是一次性的、巨大的资本支出(CapEx),而推理是持续的、与使用量成正比的运营支出(OpEx). 在全球范围内,花在推理上的总计算成本正在迅速超过训练成本. 
*   **性能瓶颈不同:** 推理过程,特别是自回归解码阶段,往往是**内存带宽受限 (Memory-bound)**,而不是计算受限 (Compute-bound). 
## 2. 推理的两个阶段
对于自回归的解码器模型(如 GPT),一次典型的推理过程可以分为两个阶段: 
### 2.1 预填充阶段 (Prefill Phase)
*   **任务:** 处理输入的 prompt. 
*   **过程:** 模型接收整个 prompt 序列(例如,一个包含 100 个 token 的问题),并进行一次并行的前向传播计算. 这会计算出 prompt 中所有 token 的注意力关系,并生成一个 KV 缓存(Key-Value Cache). 
*   **特点:**
    *   **并行度高:** 因为可以一次性处理所有输入 token,GPU 的计算单元可以被充分利用. 
    *   **计算受限 (Compute-bound):** 在这个阶段,性能瓶颈主要是矩阵乘法等计算操作. 
    *   **与训练时类似:** 这个阶段的计算模式与训练时的一个前向传播步骤非常相似. 
### 2.2 解码阶段 (Decoding Phase)
*   **任务:** 自回归地、逐个生成输出 token. 
*   **过程:**
    1.  模型根据预填充阶段的结果,生成第一个输出 token. 
    2.  将这个新生成的 token 添加到输入序列中. 
    3.  模型进行一次前向传播,计算这个新 token 的表示,并生成下一个 token. 
    4.  重复此过程,直到生成一个特殊的终止符 `[EOS]` 或达到最大长度. 
*   **特点:**
    *   **串行依赖:** 必须生成了第 `t` 个 token,才能开始生成第 `t+1` 个 token. 
    *   **内存带宽受限 (Memory-bound):** 在每一步,模型只需要为一个 token 进行计算,计算量很小,但却需要从 GPU 的全局内存中加载整个模型的权重以及巨大的 KV 缓存. 数据加载时间远远超过了实际计算时间,导致 GPU 核心大量空闲. 这是推理优化的核心挑战. 
## 3. 推理优化技术
为了提高推理的吞吐量(Throughput,每秒处理的 token 数)和降低延迟(Latency,生成每个 token 的时间),研究人员开发了多种优化技术: 
### 3.1 KV 缓存 (Key-Value Cache)
*   **核心思想:** 在解码阶段的每一步,**[注意力机制](./Lecture1-Self-Attention.md)**都需要看到之前所有 token 的键(Key)和值(Value). 如果不做优化,每生成一个新 token 都需要重新计算前面所有 token 的 K 和 V,这是巨大的浪费. 
*   **解决方案:** KV 缓存将已经计算过的 token 的 K 和 V 向量存储在 GPU 内存中. 在生成新 token 时,只需计算当前 token 的 K 和 V,然后与缓存中的历史 K 和 V 拼接起来即可. 这是最基础也是最重要的推理优化. 
### 3.2 批处理 (Batching)
*   **思想:** 将多个用户的请求组合成一个批次(batch),一起进行处理,以提高 GPU 的利用率. 
*   **持续批处理 (Continuous Batching):** 像 vLLM 等先进的推理服务器采用的策略. 它允许在批次处理过程中动态地添加新的请求和移除已完成的请求,从而实现比静态批处理高得多的吞吐量. 
### 3.3 量化 (Quantization)
*   **思想:** 使用更低精度的数据类型(如 INT8, INT4)来存储模型权重和/或激活值,而不是标准的 FP16/BF16. 
*   **好处:**
    *   **减少内存占用:** 模型更小,KV 缓存也更小. 
    *   **减少内存带宽压力:** 每次加载的数据量更少. 
    *   **更快的计算:** 一些硬件支持更快的低精度计算. 
### 3.4 模型级优化
*   **[推测解码 (Speculative Decoding)](./Lecture1-Speculative-Decoding.md):** 使用一个小模型来“推测”地生成一小段 token 序列,然后用大模型一次性地、并行地验证这些推测. 如果验证通过,就可以一次性接受多个 token,从而加速解码. 
*   **模型剪枝 (Pruning) 和蒸馏 (Distillation):** 创建一个更小、更快的模型来模仿大模型的行为. 
### 3.5 核函数优化
*   **算子融合 (Operator Fusion):** 使用 **[Triton](./Lecture1-Triton.md)** 等工具编写自定义**[GPU 核函数](./Lecture1-GPU-Kernels.md)**,将多个操作合并成一个,减少内存读写. 
*   **[FlashAttention](./Lecture1-FlashAttention.md):** 针对注意力计算的 I/O 优化,能显著提升长序列场景下的推理速度. 