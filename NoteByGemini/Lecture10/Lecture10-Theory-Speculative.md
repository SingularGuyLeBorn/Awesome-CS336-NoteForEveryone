# 理论专题：投机采样 (Speculative Sampling)

### 1. 核心直觉
在推理的生成阶段，大模型受限于内存带宽，生成一个token很慢。但是，检查（Verify）一组token很快，因为检查过程是并行的（类似于Prefill阶段）。
**投机采样**利用了这种非对称性：
1.  用一个小模型（Draft Model，生成速度快但较笨）“猜测”接下来可能的 $K$ 个token。
2.  用大模型（Target Model）并行地“打分”这 $K$ 个token。
3.  根据打分结果，决定接受或拒绝这些猜测。

### 2. 数学保证
如果不加控制地接受小模型的输出，最终的生成质量会下降。投机采样的核心贡献在于其**拒绝采样 (Rejection Sampling)** 机制，它在数学上保证了：
$$ P(\text{输出}) \equiv P_{\text{大模型}}(\text{输出}) $$
即最终生成的token分布与单独运行大模型完全一致，是**无损**的加速。

### 3. 算法流程
假设小模型概率为 $p(x)$，大模型概率为 $q(x)$。
对于小模型生成的某个 token $x$：
*   **情况 1：$q(x) \ge p(x)$** (大模型觉得小模型低估了这个词)
    *   **接受率**：100% 接受。
*   **情况 2：$q(x) < p(x)$** (大模型觉得小模型高估了这个词，比如小模型生成了幻觉)
    *   **接受率**：以概率 $\frac{q(x)}{p(x)}$ 接受。
    *   如果拒绝，则从修正后的分布中重新采样一个新的 token，并丢弃该 token 之后的所有猜测。

### 4. 收益
如果小模型猜得准，一次大模型的前向传播（Parallel）可以生成多个 token。如果猜不准，至少也能生成一个 token（保底）。平均而言，这种方法能显著减少内存读取的次数。