### 专题笔记:T5 模型

#### 1. 核心理念:“万物皆可文本到文本”

T5(Text-to-Text Transfer Transformer)是 Google 于 2019 年推出的一个极具影响力的模型. 其核心理念是**将所有 NLP 任务统一为一种“文本到文本”(Text-to-Text)的格式**. 无论是翻译、摘要、问答还是分类任务,T5 都通过在输入文本前添加一个任务特定的前缀(如 `translate English to German:`),然后生成相应的输出文本来完成. 

这种统一的框架极大地简化了模型的使用和微调,并证明了一个足够大的模型在海量无标签数据上进行预训练后,可以快速适应各种下游任务. 

#### 2. 架构特点与“大胆”的超参数选择

T5 遵循了经典的 **[Transformer 架构](./Lecture3-Transformer-Architecture.md)**,包含编码器和解码器. 然而,它在**[超参数](./Lecture3-Model-Hyperparameters.md)**的选择上做出了一些非常“大胆”和非传统的决策,使其成为研究模型架构时的一个重要且有趣的反例. 

*   **极端的 FFN 扩展比例**:
    *   在标准的 T5-11B 模型中,`d_model`(模型宽度)为 1024,而 `d_ff`(FFN 隐藏层维度)却高达 65,536. 这意味着 `d_ff / d_model` 的比例达到了惊人的 **64 倍**. 
    *   这与业界普遍遵循的 4 倍(对于 ReLU)或 2.67 倍(对于 GLU)的“黄金法则”形成鲜明对比. 
    *   Google 研究者最初的理由是,极宽的 FFN 层可以更好地利用其 TPU 加速器上的大规模密集矩阵乘法,从而提升硬件效率. 

*   **后续版本的回归**:
    *   有趣的是,T5 的一个改进版本 **T5 v1.1** 在架构上做了调整,它采用了 **[GeGLU](./Lecture3-Activation-Functions.md)** 激活函数,并将 FFN 的扩展比例降回到了一个更标准的 **2.5 倍**. T5 v1.1 的性能普遍优于原始 T5. 
    *   这一“回归”暗示,尽管极端的超参数设置可以工作,但可能并非最优选择. 业界普遍采纳的“黄金法则”背后,可能确实有其性能上的考量,而不仅仅是历史惯例. 

#### 3. 对业界的启示

T5 模型提供了几个宝贵的教训:
1.  **超参数并非铁律**:T5 的成功表明,没有一成不变的超参数选择. 在特定的硬件和任务背景下,打破常规可能会带来意想不到的效果. 
2.  **简单统一框架的威力**:其“文本到文本”的框架启发了后续许多研究,推动了 prompt engineering 和 in-context learning 的发展. 
3.  **实践是检验真理的唯一标准**:T5 v1.1 的改进也提醒我们,即使是大型研究机构的决策也需要不断地通过实验来验证和优化. 行业共识的形成往往是大量实践和迭代的结果. 