### 专题笔记:位置编码 (Positional Embedding)

#### 1. 核心问题

**[Transformer 架构](./Lecture3-Transformer-Architecture.md)**的核心——自注意力机制,在处理输入序列时,本质上是“无序”的(order-agnostic). 它将输入视为一个集合(set)而非序列(sequence),无法区分 "the cat sat on the mat" 和 "the mat sat on the cat" 这两个句子. 因此,必须通过一种外部机制向模型明确地注入词元(token)的位置信息. **位置编码**就是解决这个问题的关键技术. 

#### 2. 关键变体与演进

##### **A. 绝对位置编码 (Absolute Positional Embedding)**

-   **思想**:为序列中的每一个绝对位置(如第 1、2、3...个位置)学习一个独一无二的嵌入向量. 然后,将这个位置嵌入向量与对应的词嵌入向量相加,作为模型的最终输入. 
-   **变体1:学习式位置编码 (Learned Positional Embedding)**
    -   **做法**:创建一个位置嵌入矩阵 `P`,其大小为 `(max_sequence_length, d_model)`. 第 `i` 个位置的编码就是 `P` 的第 `i` 行. 这个矩阵与模型其他参数一起通过梯度下降进行学习. 
    -   **应用**:GPT、BERT 等早期模型采用此方法. 
    -   **缺点**:模型的泛化能力受限于 `max_sequence_length`. 如果遇到比训练时更长的序列,模型将无法处理. 
-   **变体2:正弦/余弦位置编码 (Sinusoidal Positional Embedding)**
    -   **做法**:使用不同频率的正弦和余弦函数来为每个位置生成一个固定的、非学习的编码向量. 
    -   **应用**:原始 **[Transformer](./Lecture3-Attention-Is-All-You-Need.md)** 论文中提出. 
    -   **优点**:理论上可以外推到任意长度的序列. 其周期性也可能帮助模型学习相对位置关系. 

##### **B. 相对位置编码 (Relative Positional Embedding)**

-   **思想**:认为词元之间的相对位置(如“相距 3 个 token”)比其绝对位置更重要. 因此,相对位置编码直接在注意力计算过程中注入相对位置信息. 
-   **做法**:在计算注意力得分 `q·k` 时,额外增加一个与查询(query)和键(key)之间相对距离相关的可学习偏置项. 
-   **应用**:**[T5](./Lecture3-T5-Model.md)**、Gopher 等模型采用了这种方法. 

##### **C. 旋转位置编码 (Rotary Position Embedding, RoPE)**

RoPE 是近年来最成功、应用最广泛的位置编码方案,它巧妙地结合了绝对位置和相对位置编码的优点. 

-   **思想**:通过**旋转**来编码绝对位置,同时通过旋转的性质来隐式地表达相对位置. 
-   **核心机制**:
    1.  它不会像绝对位置编码那样将位置向量“加”到词嵌入上. 
    2.  而是在计算注意力之前,对 Query (Q) 和 Key (K) 向量进行旋转操作. 
    3.  一个在位置 `m` 的向量 `q_m` 和一个在位置 `n` 的向量 `k_n`,它们旋转后的内积 `<Rq_m, Rk_n>` 只与它们的原始向量 `q`、`k` 以及**相对位置差 `m-n`** 有关,而与绝对位置 `m` 和 `n` 无关. 
    4.  具体实现上,它将 `d_model` 维度的向量两两配对,在形成的二维子空间中进行旋转,旋转角度 `θ` 由位置索引决定,并且不同配对使用不同的旋转频率. 
-   **优势**:
    -   **良好的外推性**:在处理超长序列方面表现出色. 
    -   **性能优越**:在多种任务上都取得了顶尖的性能. 
-   **应用**:**已成为现代 LLM 的标配**. **[LLaMA](./Lecture3-LLaMA-Architecture.md)**、PaLM、Mistral、Gemma 等几乎所有先进模型都采用了 RoPE. 