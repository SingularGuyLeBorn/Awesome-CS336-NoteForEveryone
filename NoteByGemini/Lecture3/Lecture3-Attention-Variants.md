### 专题笔记:注意力机制变体 (Attention Variants)

#### 1. 背景:标准自注意力的局限

标准的自注意力机制虽然强大,但存在两个主要局限:
1.  **计算复杂度**:其计算和内存复杂度都与输入序列长度 `N` 的平方成正比,即 `O(N²)`. 这使得它难以处理非常长的序列(如整本书或长对话). 
2.  **推理效率**:在自回归生成(Inference)过程中,需要维护一个不断增长的**[KV 缓存](./Lecture3-Attention-Variants.md)**,这会消耗大量内存带宽,成为推理速度的主要瓶颈. 

为了克服这些局限,研究者们提出了一系列注意力机制的变体. 

#### 2. 提升推理效率:MQA & GQA

##### **A. KV 缓存 (KV Cache)**

在逐个 token 生成文本时,为了避免重复计算,模型会将过去所有 token 的 Key (K) 和 Value (V) 向量存储起来,这个存储区就是 KV 缓存. 每生成一个新 token,只需要计算新的 Query 向量,并让它与缓存中所有的 K, V 进行交互. 然而,随着序列变长,KV 缓存会变得非常大,每次从 GPU 内存中读取它都非常耗时. 

##### **B. 多查询注意力 (Multi-Query Attention, MQA)**

-   **思想**:推理瓶颈在于巨大的 KV 缓存. 既然如此,我们可以减小 K 和 V 的尺寸. 
-   **做法**:MQA 仍然保留多个 Query 头(`num_q_heads`),但让所有这些 Query 头**共享同一份 Key 和 Value 头**(即 `num_kv_heads = 1`). 
-   **效果**:极大地减小了 KV 缓存的大小,从而显著降低了内存带宽需求,提升了推理吞吐量. 

##### **C. 分组查询注意力 (Grouped-Query Attention, GQA)**

-   **思想**:MQA 可能是一个过于激进的简化,可能会损失一定的模型性能. GQA 提供了一个折中方案. 
-   **做法**:将多个 Query 头分成若干组,**每组内的 Query 头共享一份 Key 和 Value 头**. 例如,可以有 8 个 Query 头,但只有 2 个 KV 头,其中前 4 个 Query 头共享第一个 KV 头,后 4 个共享第二个. 
-   **效果**:在模型性能损失极小(甚至没有损失)的情况下,仍然能获得接近 MQA 的推理加速效果. GQA 已成为现代 LLM(如 **[LLaMA 2](./Lecture3-LLaMA-Architecture.md)** 和 Mistral)的标配. 

#### 3. 处理长上下文:稀疏与滑动窗口注意力

##### **A. 稀疏注意力 (Sparse Attention)**

-   **思想**:完全的 `O(N²)` 注意力是多余的,一个 token 可能只需要关注序列中的少数几个关键 token. 
-   **做法**:通过预定义的稀疏模式(如局部窗口、全局节点、跨步等)来限制每个 Query 只与一部分 Key 进行交互,从而将计算复杂度降低到 `O(N * logN)` 或 `O(N * sqrt(N))`. GPT-3 在其原始版本中就使用了这类技术来支持更长的上下文. 

##### **B. 滑动窗口注意力 (Sliding Window Attention, SWA)**

-   **思想**:这是稀疏注意力的一种简单而有效的实现. 
-   **做法**:每个 token 只允许关注其左侧一个固定大小的“窗口”(Window)内的 token. 例如,窗口大小为 4096,则第 8000 个 token 只能看到第 3904 到 7999 个 token. 
-   **效果**:计算复杂度降低到 `O(N * W)`,其中 `W` 是窗口大小,是线性的. 通过堆叠多层 SWA,模型的有效感受野(Effective Context Length)可以超出单个窗口的限制. Mistral 7B 模型就因有效使用 SWA 而闻名. 

##### **C. 混合注意力策略 (Hybrid Attention)**

-   **思想**:结合不同注意力的优点,处理不同范围的依赖关系. 
-   **做法**:这是最新的发展趋势. 模型(如 Cohere Command A, LLaMA 4)会**交替使用不同类型的注意力层**:
    -   大部分层使用**带 [RoPE](./Lecture3-Positional-Embedding.md) 的滑动窗口注意力**,高效地处理**局部和短距离**的依赖关系. 
    -   每隔 N 层(如每 4 层),插入一个**不带任何位置编码的全注意力层**,用于捕捉**全局和长距离**的依赖关系. 
-   **效果**:这种精巧的设计,使得模型既能高效处理数据,又能理解和整合超长上下文中的信息,是实现百万级 token 上下文窗口的关键技术之一. 