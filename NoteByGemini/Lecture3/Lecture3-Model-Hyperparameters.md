### 专题笔记:模型超参数 (Model Hyperparameters)

#### 1. 核心概念

超参数是在模型训练开始之前就需要设定的参数,它们决定了模型的架构和训练过程的特性. 在 **[Transformer 架构](./Lecture3-Transformer-Architecture.md)**中,一些关键的超参数选择已经形成了业界广泛遵循的“经验法则”或“黄金法则”. 这些共识的形成,是基于大量实验(如 **[Scaling Laws](./Lecture3-Scaling-Laws.md)** 论文中的研究)和实践经验的沉淀. 

#### 2. 关键超参数的“黄金法则”

##### **A. FFN 隐藏层维度比例 (`d_ff / d_model`)**

-   **定义**:前馈网络(FFN)内部的隐藏层维度 `d_ff` 与模型主维度 `d_model` 之间的比例. 这个比例决定了 FFN 的“宽度”. 
-   **黄金法则**:
    -   对于使用 **ReLU/GELU** 等非门控激活函数的 FFN,`d_ff` 通常设置为 **`4 * d_model`**. 
    -   对于使用 **[SwiGLU](./Lecture3-Activation-Functions.md)** 等门控激活函数的 FFN,为了在增加门控参数后保持总参数量大致不变,`d_ff` 通常设置为 **`(8/3) * d_model ≈ 2.67 * d_model`**. 
-   **例外**:**[T5 模型](./Lecture3-T5-Model.md)** 曾使用高达 64 倍的比例,但其后续版本也回归了标准设置,这反过来验证了该法则的有效性. 研究表明,该比例在 1-10 之间是一个性能接近最优的“甜点区”. 

##### **B. 模型纵横比 (`d_model / n_layer`)**

-   **定义**:模型的宽度(`d_model`)与深度(层数 `n_layer`)之间的比例. 这个比例决定了模型的“体型”. 
-   **黄金法则**:
    -   经验表明,`d_model / n_layer` 的比值在 **100-200** 之间是一个性能和效率的“甜点区”. 
    -   例如,GPT-3(`d_model=12288, n_layer=96`)、**[LLaMA](./Lecture3-LLaMA-Architecture.md)**(如 LLaMA-65B, `d_model=8192, n_layer=80`)等模型的比值都在 **128** 左右. 
-   **权衡**:
    -   **更深(比值小)**:模型难以通过流水线并行(Pipeline Parallelism)进行扩展,推理延迟更高. 
    -   **更宽(比值大)**:参数主要集中在矩阵乘法上,有利于张量并行(Tensor Parallelism),但可能在表达能力上不如深度模型高效. 

##### **C. 多头注意力维度**

-   **定义**:在多头注意力机制中,每个头的维度 `head_dim` 与总头数 `num_heads` 的关系. 
-   **黄金法则**:
    -   标准做法是保持 `num_heads * head_dim = d_model`. 这意味着增加头数时,会相应地减小每个头的维度,以保持总计算量和参数量不变. 
    -   尽管有研究指出过小的 `head_dim` 可能会导致“低秩瓶颈”,但在实践中,这一设计被证明非常有效,几乎所有模型都遵循此规则. 

##### **D. 词汇表大小 (Vocabulary Size)**

-   **定义**:模型能够识别和生成的独立词元(token)的总数. 
-   **经验法则**:
    -   对于**单语(主要是英语)模型**,词汇表大小通常在 **30,000 到 50,000** 之间. 
    -   对于**多语言模型或生产级系统**,为了更好地覆盖多种语言、特殊字符和代码,词汇表会显著增大,通常在 **100,000 到 250,000** 之间. 例如,LLaMA 3 的词汇表大小为 128,000. 

**结论**:在设计或训练自己的语言模型时,遵循这些经过验证的超参数“黄金法则”,是一个非常明智的起点. 它们代表了社区在平衡模型性能、训练效率和稳定性方面的集体智慧. 