# CS336: 从零开始构建语言模型 - 第一讲

## 1. 课程与教职员介绍

### 欢迎致辞与课程理念

- **Percy Liang (讲师)**: 欢迎大家来到 CS336！我对此课程充满期待,因为它将带领大家端到端地体验构建语言模型的完整流程,涵盖从数据、系统到建模的方方面面. 
- **Pansy (联合讲师)**: 我和 Percy 花了很长时间思考,希望把真正有深度的技术教给学生. 我们坚信一个理念: “**唯有亲手从零构建,方能真正理解 (you gotta build it from scratch to understand it)**”. 这便是本课程的核心精神. 
- **我(小卡拉米学生):** 我不能同意更多

### 教学团队 (Teaching Assistants, TAs)

- **Robert**: 他开玩笑说自己曾经挂掉这门课,但现在成为了你们的助教. 
- **Neil**: 一位计算机系的博士三年级学生,研究兴趣在于模型、推理和合成数据. 
- **Marcel**: 曾是去年课程中多个排行榜的冠军,是大家今年要挑战的目标. 

### 课程更新

- 这是本课程第二次开设,规模扩大了50%,助教从两位增加到了三位. 
- 一个重大的变化是: **所有课程视频都将上传至YouTube**,让全世界的学习者都能从零开始学习构建语言模型. 

## 2. 为什么要开设这门课？

### 现状: 研究与技术的脱节

如果我们问 GPT-4 为什么要开设这样一门课,它会给出一些“促进基础理解、激发创新”之类的标准答案. 

但真实的原因是: 我们正处在一场“危机”之中. 

1. **研究人员与底层技术的脱节**: 八年前,AI研究者会亲手实现和训练自己的模型. 六年前,至少还会下载并微调 BERT 这样的模型. 而现在,许多人仅仅通过调用专有模型的API(Prompting)就能完成工作. 
2. **抽象层的泄露 (Leaky Abstractions)**: 虽然更高层次的抽象能让我们完成更多工作,但这并非坏事. 然而,与编程语言或操作系统不同,语言模型的抽象层是“泄露”的,我们并不真正理解它的内部机制. 
3. **基础研究的需求**: 许多根本性的研究突破,需要我们深入技术栈的底层,协同设计数据、系统和模型. **对技术的全面理解是进行基础研究的必要条件**. 

因此,本课程的目标就是**为了推动基础研究的持续发展**,而我们的哲学是: **欲懂之,必造之 (to understand it, you have to build it)**. 

### 挑战: 语言模型的工业化与规模限制

从零构建语言模型面临一个巨大挑战: **模型的工业化**. 

- **恐怖的规模**: 据传 GPT-4 拥有 1.8 万亿参数,训练成本高达1亿美元. XAI 正在构建拥有20万个 H100 的计算集群. 这些数字对学术界来说是遥不可及的. 
- **技术保密**: 出于竞争和安全考虑,像 GPT-4 这样的前沿模型 (Frontier Models) 不会披露任何技术细节. 

这意味着: **我们无法在课程中训练出自己的 GPT-4**. 我们将构建的是**小型语言模型 (small language models)**. 

但这引出了一个新问题: **小型模型的结论不一定能代表大型模型**. 

- **例子1: 计算量分布 (FLOPs Distribution)**

  - 在小型模型中,注意力层 (Attention) 和前馈网络层 (MLP) 的计算量大致相当. 
  - 但在1750亿参数的大型模型中,MLP 的计算量占据了绝对主导. 
  - 这意味着,如果在小模型上花费大量精力优化 Attention,可能是在优化一个在大规模下无关紧要的部分. 

- **例子2: 涌现能力 (Emergent Behavior)**

  - 研究表明,许多能力(如上下文学习 In-context learning)只有在模型的训练计算量达到某个阈值后才会“涌现”. 
  - 如果只在小规模下实验,你可能会得出“语言模型没用”的错误结论. 


## 3. 我们能从这门课学到什么？

尽管存在上述挑战,我们依然能学到宝贵的知识. 知识可以分为三类: 

1. **机制 (Mechanics)**: **这部分可以完全教会**. 

   - 事物如何工作的原理. 例如,什么是 Transformer,模型并行化 (model parallelism) 如何高效利用 GPU. 这些是构建模型的“原材料”. 

2. **思维模式 (Mindset)**: **这部分至关重要,也能够教会**. 

   - 我们将培养一种核心思维: **尽可能压榨硬件性能,并严肃对待规模化 (scaling)**. 
   - 许多技术(如 Transformer)的组件早已存在,但正是 OpenAI 所引领的“规模化思维”催生了新一代AI模型. 

3. **直觉 (Intuitions)**: **这部分只能部分教会**. 

   - 关于“哪些数据和建模决策 (modeling decisions) 能带来好模型 (good models)”的直觉. 
   - 这是因为在小规模上有效的架构和数据集,在大规模上可能并非如此. 
   - 有时候,最佳实践来自于实验而非理论. 例如,SwiGLU 这个非线性激活函数效果很好,但论文作者在结论中坦言: “除了神恩浩荡 (divine benevolence),我们无法提供任何解释. ”


## 4. 效率与“惨痛的教训” (The Bitter Lesson)

许多人对 Rich Sutton 的“惨痛的教训”有一个误解,认为“规模就是一切,算法不重要”. 

- **正确的解读**: **在规模化场景下的算法 (algorithms at scale) 才是最重要的**. 
- **核心公式**: `模型准确率 (Accuracy) = 算法效率 (Efficiency) × 投入资源 (Resources)`. 
- **效率的重要性**: 在投入数亿美元训练模型时,效率至关重要,任何浪费都是不可承受的. 
- **算法的进步**: 一篇2020年 OpenAI 的论文指出,从2012到2019年,将 ImageNet 训练到同等准确率所需的算力,由于算法效率的提升,降低了44倍. **这个速度甚至超过了摩尔定律**. 

因此,本课程的核心框架问题是: 

> **在给定的计算和数据预算下,你能构建出的最佳模型是什么？**

作为研究者,我们的目标就是**最大化算法效率**. 

## 5. 语言模型简史与现状

- **早期**: 语言模型的概念可追溯到香农 (Shannon) 估算英语熵的工作. 在AI领域,它曾是机器翻译、语音识别等大系统的一个组件. 值得注意的是,早在2007年,Google 就在2万亿个 token 上训练了 5-gram 模型,这个 token 数量比 GPT-3 还多. 
- **深度学习革命的组件**:

  - 2003年: 第一个神经网络语言模型 (Bengio 团队). 
  - 后续发展: Seq2Seq 模型、Adam 优化器、注意力机制 (Attention). 
  - 2017年: **Transformer** 诞生 (论文: *Attention Is All You Need*). 
  - 系统层面: 混合专家模型 (Mixture of Experts) 和模型并行化 (Model Parallelism) 的研究也已成熟. 

- **预训练与微调时代**: ELMO, BERT, T5 等模型引领了“基础模型 (foundation models)”的潮流. 
- **规模化时代**: OpenAI 整合了上述技术,并凭借卓越的工程能力和对规模化定律的信仰,推出了 GPT-2 和 GPT-3,开启了新纪元. 
- **开放模型的兴起**: 随着闭源模型 (closed models) 的成功,社区也涌现出大量开源/开放模型 (open models),如 EleutherAI、Meta (LLaMA)、Bloom、DeepSeek 等. 
- **“开放”的层次**:

  1. **闭源模型 (Closed Models)**: 如 GPT-4,不提供任何细节. 
  2. **开放权重模型 (Open-weight Models)**: 提供模型权重,但可能不提供数据细节. 
  3. **开源模型 (Open-source Models)**: 提供权重、数据和详尽的论文. 


## 6. 课程安排与后勤 (Logistics)

- **课程网站**: 所有资料都在线上. 
- **工作量**: 这是一门5个学分的课程,但工作量巨大. 有学生评价: “**第一次作业的工作量约等于 CS224n 的五次作业加上期末项目**”. 
- **适合人群**:

  - 对理解事物底层原理有执念的人. 
  - 希望在研究工程能力和大规模机器学习系统构建方面实现飞跃的人. 

- **不适合人群**:

  - 想在本学期完成其他研究项目的人. 
  - 只想学习最新最热技术(而不是花时间调试BPE)的人. 
  - 将此课作为学习构建LLM应用的入门课的人. (建议先从 Prompting 和微调开始)

- **作业 (Assignments)**:

  - 共5次作业,**不提供脚手架代码 (scaffolding code)**,你将从一个空文件开始. 
  - 我们提供单元测试 (unit tests) 来检查正确性. 
  - 你需要自己做软件设计决策. 
  - **策略**: 先在本地用小数据集实现和调试,再到集群上进行基准测试. 

- **计算集群**:

  - 感谢 Together AI 提供的 H100 GPU 集群. 
  - 请尽早开始作业,因为截止日期前集群会非常拥堵. 

- **AI 编程工具**:

  - Copilot 等工具可能会剥夺你的学习体验. 
  - 你可以审慎使用,但后果自负,你需要为自己的学习负责. 


## 7. 课程内容概览: 五大支柱

本课程围绕**效率**这一核心原则,分为五大支柱,探讨在给定的硬件和数据预算下如何做出最优的设计决策. 

### 支柱一: 基础 (Basics)

- **目标**: 搭建一个可以工作的完整训练pipeline. 
- **内容**:

  - **分词器 (Tokenizer)**: 实现字节对编码 (Byte-Pair Encoding, BPE). 
  - **模型架构 (Model Architecture)**: 实现 Transformer 及其现代改进版(如 SwiGLU 激活函数、RoPE 位置编码、RMSNorm 归一化). 
  - **训练 (Training)**: 实现 AdamW 优化器、学习率调度和完整的训练循环. 

- **作业一**:

  - 实现上述所有组件. 
  - 在给定的 H100 算力预算(90分钟)下,参加 OpenWebText 数据集上的困惑度 (perplexity) 排行榜. 


### 支柱二: 系统 (Systems)

- **目标**: 压榨硬件的极致性能. 
- **内容**:

  - **核函数 (Kernels)**: 深入理解 GPU 架构,学习如何通过算子融合 (fusion) 和分块 (tiling) 等技术减少数据搬运,并使用 Triton 编写高效的核函数. 
  - **并行化 (Parallelism)**: 学习数据并行 (data parallelism)、张量并行 (tensor parallelism) 等技术,在多GPU上高效训练. 
  - **推理 (Inference)**: 学习推理过程中的 Prefill 和 Decode 阶段,以及如何通过推测解码 (speculative decoding) 等技术进行加速. 

- **作业二**:

  - 实现自定义的 Triton kernel. 
  - 实现数据并行化. 
  - 学会使用性能分析工具 (profiler) 来定位瓶颈. 


### 支柱三: 缩放定律 (Scaling Laws)

- **目标**: 通过小规模实验,预测大规模训练时的超参数和性能. 
- **内容**:

  - **Chinchilla 最优**: 学习在给定的计算预算下,如何平衡模型大小 (model size) 和训练数据量,以达到最优性能. 
  - **经验法则**: `模型参数量 × 20 ≈ 最佳训练 Token 数`. 

- **作业三**:

  - 在一个模拟的“训练API”上,用有限的 FLOPs 预算进行实验. 
  - 根据实验数据拟合自己的缩放定律. 
  - 提交你对更大规模训练的最佳超参数的预测,参加排行榜. 


### 支柱四: 数据 (Data)

- **目标**: 理解数据如何决定模型的能力. 
- **内容**:

  - **评估 (Evaluation)**: 学习如何评估模型,包括困惑度 (Perplexity)、标准化测试 (如 MMLU) 和对指令遵循能力的评估. 
  - **数据策管 (Data Curation)**: 数据并非凭空而来. 我们将学习如何处理原始网络数据 (如 Common Crawl),包括: 

    - 从 HTML 中提取高质量文本. 
    - 数据过滤 (filtering) 和去重 (deduplication). 
    - 处理有害内容. 


- **作业四**:

  - 处理原始的 Common Crawl 数据. 
  - 训练分类器进行数据筛选. 
  - 在给定的 Token 预算下,优化模型在测试集上的困惑度,参加排行榜. 


### 支柱五: 对齐 (Alignment)

- **目标**: 让预训练好的基础模型 (base model) 变得有用、听话且安全. 
- **内容**:

  - **对齐的三个方面**: ①遵循指令；②控制生成风格；③安全(拒绝有害回答). 
  - **监督式微调 (Supervised Fine-Tuning, SFT)**: 使用高质量的“指令-回答”对来微调模型. 
  - **从反馈中学习 (Learning from Feedback)**:

    - **数据**: 使用偏好数据 (preference data),即标注者判断模型生成的两个回答中哪个更好. 
    - **算法**: 学习 DPO (Direct Preference Optimization) 和 GRPO (Group-Relative Preference Optimization) 等比传统强化学习更简单高效的算法. 


- **作业五**:

  - 实现 SFT、DPO 和 GRPO. 


## 8. 第一单元深入探讨: 分词 (Tokenization)

分词是将原始文本字符串 (string) 转换为整数序列 (sequence of integers) 的过程,也是模型处理文本的第一步. 

- [Andrew Karpathy - Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)

### 尝试一: 基于字符的分词 (Character-based)

- **方法**: 将每个 Unicode 字符映射到一个整数. 
- **优点**: 非常简单. 
- **缺点**:

  1. **词汇表过大**: Unicode 字符非常多. 
  2. **效率低下**: 为罕见字符分配了独立的 token,浪费了词汇表空间. 
  3. **压缩率低**: 序列长度很长. 


### 尝试二: 基于字节的分词 (Byte-based)

- **方法**: 将字符串编码为 UTF-8 字节序列,每个字节 (0-255) 是一个 token. 
- **优点**: 词汇表大小固定为256,非常小巧,没有未知 token. 
- **缺点**:

  1. **压缩率极差 (1.0)**: 每个字节都变成一个 token,导致序列非常长. 
  2. **计算效率低**: Transformer 的注意力机制是序列长度的二次方复杂度,长序列会带来巨大的计算开销. 


### 尝试三: 基于单词的分词 (Word-based)

- **方法**: 用正则表达式等方式将文本切分成单词. 
- **优点**: 压缩率高,符合直觉. 
- **缺点**:

  1. **词汇表无限大**: 总会遇到词汇表中没有的新词(Out-of-Vocabulary, OOV). 
  2. **处理 "unk" 标记**: 需要用一个特殊的 `[UNK]` 标记来表示未知词,处理起来很麻烦. 


### 最终选择: 字节对编码 (Byte Pair Encoding, BPE)

BPE 是一种非常古老(1994年)的数据压缩算法,后来被引入 NLP 领域,并由 GPT-2 发扬光大. 

- **核心思想**: **不再预设分词规则,而是从数据中学习**. 它有机地将语料中频繁出现的字符序列合并成一个 token. 
- **BPE 训练算法**:

  1. **初始化**: 将文本转换为字节序列,初始词汇表就是所有单个字节(0-255). 
  2. **迭代合并**:

     a.  统计当前序列中所有相邻 token 对的出现频率. 

     b.  找到频率最高的 token 对(例如 `('t', 'h')`). 

     c.  将这个 token 对合并成一个新的 token(例如 `'th'`),并将其加入词汇表. 

     d.  在整个文本序列中,将所有出现的 `('t', 'h')` 替换为新的 `'th'` token. 

  3. **重复**: 重复上一步骤,直到达到预设的词汇表大小. 

- **优点**:

  - **自适应**: 能根据语料库自动学习词汇. 
  - **压缩率和序列长度的平衡**: 常见词(如 "language")成为一个 token,不常见词(如 "tokenization")被拆分成几个子词(如 "token" + "ization"),既避免了 OOV 问题,又控制了序列长度. 
  - **可逆**: 没有信息损失. 

- **总结**: BPE 是一个非常有效的启发式算法. 尽管我们都希望未来能有直接处理原始字节的高效模型架构,但在那一天到来之前,我们仍需与分词技术打交道. 

## 9. 课程预告

今天的课程到此结束. 下次课我们将深入 **PyTorch 的细节**,并关注**资源核算 (resource accounting)**,教会大家如何精确地分析模型训练中每一份计算资源(FLOPs)的去向. 我们下次见！